{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56e497f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hippolytepilchen/micromamba/envs/pooled_embed/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from embed_llm import DATA_PATH\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b05e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQUAD\n",
    "new_data = []\n",
    "data = load_dataset(\"squad\", split=\"validation\")\n",
    "for sample in data:\n",
    "    new_data.append({\n",
    "        'question': sample['question'],\n",
    "        'answer': sample['answers']['text'][0],\n",
    "        'passages':sample['context']\n",
    "    })\n",
    "with open(DATA_PATH + \"raw/squad_validation.jsonl\", \"w\") as f:\n",
    "    for item in new_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae15878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN/Dailymail\n",
    "new_data = []\n",
    "data = load_dataset(\"abisee/cnn_dailymail\",\"1.0.0\", split=\"test\")\n",
    "for sample in data:\n",
    "    new_data.append({\n",
    "        'question': \"What is a very short summary of the above text?\",\n",
    "        'answer': sample['highlights'],\n",
    "        'passages':sample['article']\n",
    "    })\n",
    "with open(DATA_PATH + \"raw/cnn_validation.jsonl\", \"w\") as f:\n",
    "    for item in new_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f44ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NQ \n",
    "new_data = []\n",
    "data = load_dataset(\"nq_open\",split='validation')\n",
    "for sample in data:\n",
    "    new_data.append({\n",
    "        'question': sample['question'],\n",
    "        'answer': sample['answer'][0],\n",
    "    })\n",
    "with open(DATA_PATH + \"raw/nq_validation.jsonl\", \"w\") as f:\n",
    "    for item in new_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ca1aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRIVIAQA\n",
    "new_data = []\n",
    "data = load_dataset(\"mandarjoshi/trivia_qa\", 'unfiltered.nocontext',split='validation')\n",
    "for sample in data:\n",
    "    new_data.append({\n",
    "        'question': sample['question'],\n",
    "        'answer': sample['answer']['aliases'],\n",
    "    })\n",
    "with open(DATA_PATH + \"raw/triviaqa_validation.jsonl\", \"w\") as f:\n",
    "    for item in new_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899d9b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HotpotQA\n",
    "new_data = []\n",
    "data = load_dataset(\"hotpotqa/hotpot_qa\", 'distractor',split='validation')\n",
    "for sample in data:\n",
    "    new_data.append({\n",
    "        'question': sample['question'],\n",
    "        'answer': sample['answer'],\n",
    "        'passages': [' '.join(sentences) for sentences in sample['context']['sentences']]\n",
    "    })\n",
    "with open(DATA_PATH + \"raw/hotpotqa_validation.jsonl\", \"w\") as f:\n",
    "    for item in new_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529786b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atlas chunks for eval \n",
    "# Follow instructions from https://github.com/facebookresearch/atlas?tab=readme-ov-file#available-data-and-Models-for-download\n",
    "# Load it at DATA_PATH + 'raw/Atlas_passages_validation.jsonl'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d26f9c8",
   "metadata": {},
   "source": [
    "### Fine-tuning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8902d7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freebase QA\n",
    "new_data = []\n",
    "data = load_dataset(\"freebase_qa\", split=\"train\")\n",
    "for sample in data:\n",
    "    new_data.append({\n",
    "        'question': sample['RawQuestion'],\n",
    "        'answer': sample['Parses']['Answers'][0]['AnswersName'][0][0]\n",
    "    })\n",
    "with open(DATA_PATH + \"raw/freebase_qa_train.jsonl\", \"w\") as f:\n",
    "    for item in new_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4834877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSMARCO\n",
    "new_data = []\n",
    "data = load_dataset(\"ms_marco\",\"v2.1\", split=\"train\")\n",
    "for sample in data:\n",
    "    if 'No Answer' in sample['answers'][0]:\n",
    "        continue\n",
    "    new_data.append({\n",
    "        'question': sample['query'],\n",
    "        'answer': sample['answers'][0]\n",
    "    })\n",
    "with open(DATA_PATH + \"raw/msmarco_train.jsonl\", \"w\") as f:\n",
    "    for item in new_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd72b1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROP\n",
    "new_data = []\n",
    "data = load_dataset(\"drop\", split=\"train\")\n",
    "for sample in data:\n",
    "    new_data.append({\n",
    "        'question': sample['question'],\n",
    "        'answer': sample['answers_spans']['spans'][0],\n",
    "        'passages':sample['passage']\n",
    "    })\n",
    "with open(DATA_PATH + \"raw/drop_train.jsonl\", \"w\") as f:\n",
    "    for item in new_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5fb9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIKISUM\n",
    "\n",
    "ds = load_dataset(\"d0rj/wikisum\")\n",
    "data = ds['train']\n",
    "new_data = []\n",
    "for sample in data:\n",
    "    if len(sample['article'])<8000:\n",
    "        new_data.append({\n",
    "            'passage':sample['article'],\n",
    "            'question':'What is a summary of the previous text?',\n",
    "            'answer':sample['summary']\n",
    "        })\n",
    "with open(DATA_PATH + 'WikiSum_train.jsonl','w') as f:\n",
    "    for item in new_data:\n",
    "        f.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229da8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIALOGSUM\n",
    "\n",
    "ds = load_dataset(\"knkarthick/dialogsum\")\n",
    "data = ds['train']\n",
    "new_data = []\n",
    "for sample in data:\n",
    "    if len(sample['dialogue'])<8000:\n",
    "        new_data.append({\n",
    "            'passage':sample['dialogue'],\n",
    "            'question':'Write a short summary of the previous dialogue.',\n",
    "            'answer':sample['summary']\n",
    "        })\n",
    "with open(DATA_PATH + 'DialogSum_train.jsonl','w') as f:\n",
    "    for item in new_data:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f214fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMSUM\n",
    "\n",
    "ds = load_dataset(\"knkarthick/samsum\")\n",
    "data = ds['train']\n",
    "new_data = []\n",
    "for sample in data:\n",
    "    if sample['dialogue'] is None:\n",
    "        continue\n",
    "    if len(sample['dialogue'])<8000:\n",
    "        new_data.append({\n",
    "            'passage':sample['dialogue'],\n",
    "            'question':'Write a very short summary of the previous dialogue.',\n",
    "            'answer':sample['summary']\n",
    "        })\n",
    "with open(DATA_PATH + 'SamSum_train.jsonl','w') as f:\n",
    "    for item in new_data:\n",
    "        f.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ad674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ParaSCI\n",
    "ds = load_dataset(\"HHousen/ParaSCI\")\n",
    "\n",
    "data = ds['train']\n",
    "new_data = []\n",
    "for sample in data:\n",
    "    if len(sample['sentence1'])>100 and len(sample['sentence2'])>100:\n",
    "        new_data.append({\n",
    "            'passage':sample['sentence1'],\n",
    "            'question':'Paraphrase the previous text.',\n",
    "            'answer':sample['sentence2'],\n",
    "        })\n",
    "with open(DATA_PATH + 'ParaSCI_train.jsonl','w') as f:\n",
    "    for item in new_data:\n",
    "        f.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd97649e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 777/777 [00:00<00:00, 3.57kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adversarial_qa 29966\n",
      "msmarco 59699\n",
      "freebase_qa 20356\n",
      "sciq 11679\n",
      "asqa 4353\n"
     ]
    }
   ],
   "source": [
    "# Training QA\n",
    "train_qa_data = {}\n",
    "ds = load_dataset(\"dmrau/multi_qa\")\n",
    "\n",
    "for sample in ds['train']:\n",
    "    data_id = re.sub(r\"\\d+\", \"\", sample[\"id\"])\n",
    "    if data_id not in ['adversarial_qa', 'freebase_qa', 'sciq', 'msmarco', 'asqa']:\n",
    "        continue\n",
    "    \n",
    "    if data_id == \"msmarco\" and \"No Answer\" in  sample['label']:\n",
    "        continue\n",
    "    \n",
    "    if data_id not in train_qa_data:\n",
    "        train_qa_data[data_id] = []\n",
    "        \n",
    "    train_qa_data[data_id].append({\n",
    "        'question': sample['content'],\n",
    "        'answer': sample['label']\n",
    "    })\n",
    "\n",
    "for k, v in train_qa_data.items():\n",
    "    with open(DATA_PATH + f\"raw/{k}_train.jsonl\", \"w\") as f:\n",
    "        for item in v:\n",
    "            f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc973119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KILT passages for training retrieval\n",
    "ds = load_dataset(\"dmrau/kilt-128\")\n",
    "new_data = []\n",
    "for sample in ds['train']:\n",
    "\n",
    "    new_data.append({\n",
    "            'content':sample['text']\n",
    "    })\n",
    "with open(DATA_PATH + 'KILT_passages_train.jsonl','w') as f:\n",
    "    for item in new_data:\n",
    "        f.write(json.dumps(item) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pooled_embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
