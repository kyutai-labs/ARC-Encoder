{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56e497f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hippolytepilchen/micromamba/envs/pooled_embed/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from embed_llm import DATA_PATH\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d06c95",
   "metadata": {},
   "source": [
    "### Evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b05e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQUAD\n",
    "new_data = []\n",
    "data = load_dataset(\"squad\", split=\"validation\")\n",
    "for sample in data:\n",
    "    new_data.append({\n",
    "        'question': sample['question'],\n",
    "        'answer': sample['answers']['text'][0],\n",
    "        'passages':sample['context']\n",
    "    })\n",
    "with open(DATA_PATH + \"raw/squad_validation.jsonl\", \"w\") as f:\n",
    "    for item in new_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae15878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN/Dailymail\n",
    "new_data = []\n",
    "data = load_dataset(\"abisee/cnn_dailymail\",\"1.0.0\", split=\"test\")\n",
    "for sample in data:\n",
    "    new_data.append({\n",
    "        'question': \"What is a very short summary of the above text?\",\n",
    "        'answer': sample['highlights'],\n",
    "        'passages':sample['article']\n",
    "    })\n",
    "with open(DATA_PATH + \"raw/cnn_validation.jsonl\", \"w\") as f:\n",
    "    for item in new_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f44ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NQ \n",
    "new_data = []\n",
    "data = load_dataset(\"nq_open\",split='validation')\n",
    "for sample in data:\n",
    "    new_data.append({\n",
    "        'question': sample['question'],\n",
    "        'answer': sample['answer'][0],\n",
    "    })\n",
    "with open(DATA_PATH + \"raw/nq_validation.jsonl\", \"w\") as f:\n",
    "    for item in new_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ca1aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRIVIAQA\n",
    "new_data = []\n",
    "data = load_dataset(\"mandarjoshi/trivia_qa\", 'unfiltered.nocontext',split='validation')\n",
    "for sample in data:\n",
    "    new_data.append({\n",
    "        'question': sample['question'],\n",
    "        'answer': sample['answer']['aliases'],\n",
    "    })\n",
    "with open(DATA_PATH + \"raw/triviaqa_validation.jsonl\", \"w\") as f:\n",
    "    for item in new_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899d9b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HotpotQA\n",
    "new_data = []\n",
    "data = load_dataset(\"hotpotqa/hotpot_qa\", 'distractor',split='validation')\n",
    "for sample in data:\n",
    "    new_data.append({\n",
    "        'question': sample['question'],\n",
    "        'answer': sample['answer'],\n",
    "        'passages': [' '.join(sentences) for sentences in sample['context']['sentences']]\n",
    "    })\n",
    "with open(DATA_PATH + \"raw/hotpotqa_validation.jsonl\", \"w\") as f:\n",
    "    for item in new_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529786b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atlas chunks for eval \n",
    "# Follow instructions from https://github.com/facebookresearch/atlas?tab=readme-ov-file#available-data-and-Models-for-download\n",
    "# Load it at DATA_PATH + 'raw/Atlas_passages_validation.jsonl' with each line having a 'text' key\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518a5704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Flores evaluation datasets\n",
    "fleurs = datasets.load_dataset(\"facebook/flores\", \"all\", split=\"dev\")\n",
    "\n",
    "with open(DATA_PATH + \"flores/eng_Latn.jsonl\", \"w\") as f:\n",
    "    for item in fleurs['sentence_eng_Latn']:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "        \n",
    "with open(DATA_PATH + \"flores/spa_Latn.jsonl\", \"w\") as f:\n",
    "    for item in fleurs['sentence_spa_Latn']:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "with open(DATA_PATH + \"flores/fra_Latn.jsonl\", \"w\") as f:\n",
    "    for item in fleurs['sentence_fra_Latn']:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "        \n",
    "with open(DATA_PATH + \"flores/deu_Latn.jsonl\", \"w\") as f:\n",
    "    for item in fleurs['sentence_deu_Latn']:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "        \n",
    "with open(DATA_PATH + \"flores/dan_Latn.jsonl\", \"w\") as f:\n",
    "    for item in fleurs['sentence_dan_Latn']:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce62d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our fine-tuning dataset from huggingface\n",
    "!wget https://huggingface.co/datasets/kyutai/ARC_finetuning/resolve/main/ARC-Encoder_ft.zip? -P DATA_PATH + 'w_retrieved/' \n",
    "!unzip DATA_PATH + 'w_retrieved/ARC-Encoder_ft.zip' -d . DATA_PATH + 'w_retrieved/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e82702d",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a30b02a",
   "metadata": {},
   "source": [
    "### Long-Context Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edb2dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PG19 \n",
    "# Follow instructions from https://github.com/google-deepmind/pg19\n",
    "# Load training set at DATA_PATH + 'raw/pg19.jsonl' with each line having a 'text' key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595b1de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ArXiv\n",
    "\n",
    "ds = load_dataset(\"togethercomputer/RedPajama-Data-1T\", 'arxiv', split='train')\n",
    "new_data = []\n",
    "for sample in data:\n",
    "    new_data.append({\n",
    "        'text': sample['text'],\n",
    "    })\n",
    "with open(DATA_PATH + 'raw/arxiv.jsonl', \"w\") as f:\n",
    "    for item in new_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3043d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NQA\n",
    "data = load_dataset(\"narrativeqa\", split = 'validation')\n",
    "new_data = []\n",
    "instruction = \"You are given a story, which can be either a novel or a movie script, and a question. Answer the question as concisely as you can, using a single phrase if possible.\\n\\n\"\n",
    "for sample in data:\n",
    "    new_data.append(\n",
    "                {\n",
    "                    \"passage\": sample[\"document\"][\"text\"],\n",
    "                    \"answer\": [answer[\"text\"] for answer in sample[\"answers\"]],\n",
    "                    \"question\": sample[\"question\"][\"text\"],\n",
    "                    \"instruction\": instruction,\n",
    "                }\n",
    "            )\n",
    "\n",
    "with open(DATA_PATH + \"long_context/narrativeqa_validation.jsonl\", \"w\") as f:\n",
    "    for item in new_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "        \n",
    "\n",
    "# QASPER\n",
    "data = load_dataset(\"tau/scrolls\", \"qasper\", split = 'validation')\n",
    "new_data = []\n",
    "instruction = 'You are given a scientific article and a question. Answer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\".\\n\\n'\n",
    "for sample in data:\n",
    "    new_data.append(\n",
    "                {\n",
    "            \"passage\": sample[\"input\"][\n",
    "                sample[\"input\"].index(\"\\n\\n\") + 2 :\n",
    "            ].strip(),\n",
    "            \"answer\": sample[\"output\"],\n",
    "            \"question\": sample[\"input\"][\n",
    "                : sample[\"input\"].index(\"\\n\\n\")\n",
    "            ].strip(),\n",
    "            \"instruction\": instruction,\n",
    "        }\n",
    "    )\n",
    "with open(DATA_PATH + \"long_context/qasper_validation.jsonl\",\"w\") as f:\n",
    "    for item in new_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "        \n",
    "        \n",
    "# GovReport\n",
    "valid_dataset = load_dataset(\"ccdv/govreport-summarization\", split = 'validation')\n",
    "new_data = []\n",
    "instruction = \"Instruction: You are given a report by a government agency. Write a one-page summary of the report.\\n\"\n",
    "\n",
    "for sample in valid_dataset:\n",
    "    new_data.append(\n",
    "        {\n",
    "            \"passage\": sample[\"report\"],\n",
    "            \"answer\": sample[\"summary\"],\n",
    "            \"instruction\": instruction,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "with open(DATA_PATH + \"long_context/govreport_validation.jsonl\",\"w\") as f:\n",
    "    for item in new_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "        \n",
    "        \n",
    "# QMSUM\n",
    "valid_dataset = load_dataset(\"tau/scrolls\", \"qmsum\", split = 'validation')\n",
    "new_data = []\n",
    "instruction = \"You are given a meeting transcript and a query containing a question or instruction. Answer the query in one or more sentences.\\n\\n\"\n",
    "\n",
    "for sample in valid_dataset:\n",
    "    new_data.append(\n",
    "                {\n",
    "                    \"passage\": sample[\"input\"][\n",
    "                        sample[\"input\"].index(\"\\n\\n\") + 2 :\n",
    "                    ].strip(),\n",
    "                    \"answer\": sample[\"output\"],\n",
    "                    \"question\": sample[\"input\"].split(\"\\n\")[0].strip(),\n",
    "                    \"instruction\": instruction,\n",
    "                }\n",
    "            )\n",
    "\n",
    "\n",
    "with open(DATA_PATH + \"long_context/qmsum_validation.jsonl\",\"w\") as f:\n",
    "    for item in new_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d26f9c8",
   "metadata": {},
   "source": [
    "### Fine-tuning sub-datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8902d7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freebase QA\n",
    "new_data = []\n",
    "data = load_dataset(\"freebase_qa\", split=\"train\")\n",
    "for sample in data:\n",
    "    new_data.append({\n",
    "        'question': sample['RawQuestion'],\n",
    "        'answer': sample['Parses']['Answers'][0]['AnswersName'][0][0]\n",
    "    })\n",
    "with open(DATA_PATH + \"raw/freebase_qa_train.jsonl\", \"w\") as f:\n",
    "    for item in new_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4834877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSMARCO\n",
    "new_data = []\n",
    "data = load_dataset(\"ms_marco\",\"v2.1\", split=\"train\")\n",
    "for sample in data:\n",
    "    if 'No Answer' in sample['answers'][0]:\n",
    "        continue\n",
    "    new_data.append({\n",
    "        'question': sample['query'],\n",
    "        'answer': sample['answers'][0]\n",
    "    })\n",
    "with open(DATA_PATH + \"raw/msmarco_train.jsonl\", \"w\") as f:\n",
    "    for item in new_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd72b1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROP\n",
    "new_data = []\n",
    "data = load_dataset(\"drop\", split=\"train\")\n",
    "for sample in data:\n",
    "    new_data.append({\n",
    "        'question': sample['question'],\n",
    "        'answer': sample['answers_spans']['spans'][0],\n",
    "        'passages':sample['passage']\n",
    "    })\n",
    "with open(DATA_PATH + \"raw/drop_train.jsonl\", \"w\") as f:\n",
    "    for item in new_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5fb9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIKISUM\n",
    "\n",
    "ds = load_dataset(\"d0rj/wikisum\")\n",
    "data = ds['train']\n",
    "new_data = []\n",
    "for sample in data:\n",
    "    if len(sample['article'])<8000:\n",
    "        new_data.append({\n",
    "            'passage':sample['article'],\n",
    "            'question':'What is a summary of the previous text?',\n",
    "            'answer':sample['summary']\n",
    "        })\n",
    "with open(DATA_PATH + 'WikiSum_train.jsonl','w') as f:\n",
    "    for item in new_data:\n",
    "        f.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229da8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIALOGSUM\n",
    "\n",
    "ds = load_dataset(\"knkarthick/dialogsum\")\n",
    "data = ds['train']\n",
    "new_data = []\n",
    "for sample in data:\n",
    "    if len(sample['dialogue'])<8000:\n",
    "        new_data.append({\n",
    "            'passage':sample['dialogue'],\n",
    "            'question':'Write a short summary of the previous dialogue.',\n",
    "            'answer':sample['summary']\n",
    "        })\n",
    "with open(DATA_PATH + 'DialogSum_train.jsonl','w') as f:\n",
    "    for item in new_data:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f214fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMSUM\n",
    "\n",
    "ds = load_dataset(\"knkarthick/samsum\")\n",
    "data = ds['train']\n",
    "new_data = []\n",
    "for sample in data:\n",
    "    if sample['dialogue'] is None:\n",
    "        continue\n",
    "    if len(sample['dialogue'])<8000:\n",
    "        new_data.append({\n",
    "            'passage':sample['dialogue'],\n",
    "            'question':'Write a very short summary of the previous dialogue.',\n",
    "            'answer':sample['summary']\n",
    "        })\n",
    "with open(DATA_PATH + 'SamSum_train.jsonl','w') as f:\n",
    "    for item in new_data:\n",
    "        f.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ad674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ParaSCI\n",
    "ds = load_dataset(\"HHousen/ParaSCI\")\n",
    "\n",
    "data = ds['train']\n",
    "new_data = []\n",
    "for sample in data:\n",
    "    if len(sample['sentence1'])>100 and len(sample['sentence2'])>100:\n",
    "        new_data.append({\n",
    "            'passage':sample['sentence1'],\n",
    "            'question':'Paraphrase the previous text.',\n",
    "            'answer':sample['sentence2'],\n",
    "        })\n",
    "with open(DATA_PATH + 'ParaSCI_train.jsonl','w') as f:\n",
    "    for item in new_data:\n",
    "        f.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd97649e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 777/777 [00:00<00:00, 3.57kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adversarial_qa 29966\n",
      "msmarco 59699\n",
      "freebase_qa 20356\n",
      "sciq 11679\n",
      "asqa 4353\n"
     ]
    }
   ],
   "source": [
    "# Training QA\n",
    "train_qa_data = {}\n",
    "ds = load_dataset(\"dmrau/multi_qa\")\n",
    "\n",
    "for sample in ds['train']:\n",
    "    data_id = re.sub(r\"\\d+\", \"\", sample[\"id\"])\n",
    "    if data_id not in ['adversarial_qa', 'freebase_qa', 'sciq', 'msmarco', 'asqa']:\n",
    "        continue\n",
    "    \n",
    "    if data_id == \"msmarco\" and \"No Answer\" in  sample['label']:\n",
    "        continue\n",
    "    \n",
    "    if data_id not in train_qa_data:\n",
    "        train_qa_data[data_id] = []\n",
    "        \n",
    "    train_qa_data[data_id].append({\n",
    "        'question': sample['content'],\n",
    "        'answer': sample['label']\n",
    "    })\n",
    "\n",
    "for k, v in train_qa_data.items():\n",
    "    with open(DATA_PATH + f\"raw/{k}_train.jsonl\", \"w\") as f:\n",
    "        for item in v:\n",
    "            f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc973119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KILT passages for training retrieval\n",
    "ds = load_dataset(\"dmrau/kilt-128\")\n",
    "new_data = []\n",
    "for sample in ds['train']:\n",
    "\n",
    "    new_data.append({\n",
    "            'content':sample['text']\n",
    "    })\n",
    "with open(DATA_PATH + 'KILT_passages_train.jsonl','w') as f:\n",
    "    for item in new_data:\n",
    "        f.write(json.dumps(item) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
