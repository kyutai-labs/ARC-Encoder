{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hippolytepilchen/micromamba/envs/llm_embed/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/hippolytepilchen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from embed_llm.models.augmented_model import EmbedAugPipeline\n",
    "from embed_llm.generation.evaluation import  ensure_reproducibility\n",
    "from bertviz import head_view, model_view # type: ignore\n",
    "from embed_llm.models.mistral.generate import get_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_reproducibility(29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 for loading\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Must have a params json for pipeline\n",
    "llm_path = '/lustre/scwpod02/client/kyutai-interns/hippop/models/mistral_7B'\n",
    "\n",
    "run_name = 'pretrain_llm_trained_rec_singpassage_054f63f8'\n",
    "last_ckpt = '040000' # '008500' #'010000' \n",
    "\n",
    "run_name = '128_SL_FN_False_1_MLP_True_CA_16_CAL_False_SKV_True_DB'\n",
    "last_ckpt = '010000'\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    device = torch.device('cuda:0')\n",
    "    print(f'Using {device} for loading')\n",
    "\n",
    "w_embeds = True\n",
    "max_batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:04<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cross att state dict\n",
      "Not only LoRA weights found in the checkpoint. Skipping other weights.\n",
      "Loading LoRA weights...\n",
      "If n_layers is 1, hidden_dim must be equal to out_dim, \n",
      " but hidden_dim is not equal to out_dim so hidden_dim is set to out_dim\n",
      "Loading MLP projector\n",
      "Pipeline w prefix or not ?  False\n"
     ]
    }
   ],
   "source": [
    "pipeline: EmbedAugPipeline = EmbedAugPipeline.load_inference_model(\n",
    "    llm_path=llm_path,\n",
    "    ckpt_path=\"/lustre/scwpod02/client/kyutai-interns/hippop/tmp/\"\n",
    "    + run_name\n",
    "    + \"/checkpoints/checkpoint_\"\n",
    "    + last_ckpt,\n",
    "    device=device,\n",
    "    llm_name=\"Mistral7B\",\n",
    "    embed_model_name=\"NVEmbed\",  # Not used if pretrainde ckpt available\n",
    "    max_batch_size=max_batch_size,\n",
    ")\n",
    "print('Pipeline w prefix or not ? ', pipeline.pipeline_args.w_prefix_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hippolytepilchen/.cache/huggingface/modules/transformers_modules/nvidia/NV-Embed-v2/5130cf1daf847c1bacee854a6ef1ca939e747fb2/modeling_nvembed.py:349: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(batch_dict.get('input_ids').to(batch_dict.get('input_ids')).long()),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Pre Embed: [8, 0]\n",
      "Prompt Post Embed: [10, 1]\n",
      "['\\n\\nMario Alberto Bortolazzi (born 12 January 1964) is a former professional footballer who played as a midfielder. He played for 12 seasons at A.C. Milan, winning the Serie A championship in 1991–92, and the Coppa Italia in 1993–94. He was transferred to Genoa C.F.C. for 500,000 lire, and made his debut on 25 September 1995, in a match against Fiorentina at', 'Mario Bortolazzi\\n\\nMario Bortolazzi (born 12 January 1965 in Genoa) is an Italian former professional football player and coach. He played 12 seasons in Serie A, for A.C. Milan, Atalanta, Fiorentina, and Hellas Verona. He played 246 games in Serie A, scoring 14 goals. 1990–91 season, he played 12 games in Serie B, for A.C. Milan. 1991–92 season,']\n"
     ]
    }
   ],
   "source": [
    "# Flipping attempts\n",
    "w_embeds = True\n",
    "temp = 0\n",
    "max_tokens = 128\n",
    "i_token_to_flip = -1\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "\n",
    "temp = [temp] * max_tokens   \n",
    "if max_tokens > i_token_to_flip >= 0:\n",
    "    temp[i_token_to_flip] = 1000\n",
    "    \n",
    "prompt = ''\n",
    "text_conditioning ='Mario Bortolazzi (born 10 January 1965, in Verona) is an Italian professional football coach and a former player, who played as a midfielder. \\\n",
    "    \\n\\nHe played 12 seasons (241 games, 14 goals) in the Serie A for ACF Fiorentina, A.C. Milan, Hellas Verona F.C., Atalanta B.C. and Genoa C.F.C.'\n",
    "        # \\n\\nIn his coaching career he has so far has always been an assistant to his former Milan teammate Roberto Donadoni.\\\n",
    "        #     \\n\\nHonours\\n\\n - Milan\\n - Serie A champion: 1987–88.\\n\\n - Genoa\\n - Anglo-Italian Cup winner: 1995–96.'\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "generated_sequence, attn_or_logprobs, embeddings = pipeline.generate(prompt_pre_embed = ['In other words, background: ',''], \n",
    "                                    prompt_post_embed = [' is just another way of saying: ',''],\n",
    "                                    text_conditioning = [text_conditioning]*2, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False,\n",
    "                                    random_flip = i_token_to_flip,\n",
    "                                    device = device,\n",
    "                                    device_generation = device if torch.cuda.device_count() <= 1 else torch.device('cuda:1'),\n",
    "                                    return_embeddings = True)\n",
    "print(generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_passages = 20\n",
    "\n",
    "lim_toks = 128\n",
    "eval_data = '/lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_valid.jsonl'\n",
    "train_data = '/lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_train.jsonl'\n",
    "train_passage = []\n",
    "valid_passage = []\n",
    "\n",
    "with open(train_data, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == n_passages:\n",
    "            break\n",
    "        train_passage.append(pipeline.tokenizer.decode(pipeline.tokenizer.encode(json.loads(line)['text'].split('\\n\\n')[1], eos = True, bos = True)[:lim_toks]))\n",
    "  \n",
    "with open(eval_data, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == n_passages:\n",
    "            break\n",
    "        valid_passage.append(pipeline.tokenizer.decode(pipeline.tokenizer.encode(json.loads(line)['text'].split('\\n\\n')[1], eos = True, bos = True)[:lim_toks]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_embeds = True\n",
    "temp = 0\n",
    "max_tokens = 128\n",
    "i_token_to_flip = -1\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "\n",
    "temp = [temp] * max_tokens   \n",
    "if max_tokens > i_token_to_flip >= 0:\n",
    "    temp[i_token_to_flip] = 1000\n",
    "# 1 information in the doc which enables to answer the question but not good response often in-context\n",
    "# 2 information in the doc which enables to answer the question and good response often in-context\n",
    "# 3 Hard negative passage\n",
    "# 4 Same\n",
    "\n",
    "prompt_prefix = \"Query: \"\n",
    "prompts = ['who has most followers on instagram in world','who did the united states win its independence from', 'locations for the film an englishman who went up a hill', 'who is the valley of the dolls based on']\n",
    "prompts = [prompt_prefix + prompt + '\\nAnswer:' for prompt in prompts]\n",
    "\n",
    "conditioning = [\"This list contains the top 50 accounts with the most followers on the photo and video-sharing social platform Instagram. As of July 2019, the most followed user is Instagram's own account, with over 308 million followers. Cristiano Ronaldo is the most followed individual, with over 177 million followers. Fifteen accounts have exceeded 100 million followers on the site.\",\n",
    "                     \"During the American Revolution, the legal separation of the thirteen colonies from Great Britain in 1776 actually occurred on July 2, when the Second Continental Congress voted to approve a resolution of independence that had been proposed in June by Richard Henry Lee of Virginia declaring the United States independent from Great Britain's rule. After voting for independence, Congress turned its attention to the Declaration of Independence, a statement explaining this decision, which had been prepared by a Committee of Five, with Thomas Jefferson as its principal author. Congress debated and revised the wording of the Declaration, finally approving it two days later on July 4. A day earlier, John Adams had written to his wife Abigail\",\n",
    "                     'The village was a primary location for the making of the film \\\"The Englishman Who Went Up a Hill But Came Down a Mountain\\\", which starred Hugh Grant. The hilltop scenes were filmed on the Gyrn, the long hill that overlooks the village. It was also featured in \\\"Monk\\'s Hood\\\", an episode of \\\"The Cadfael Chronicles\\\"',\n",
    "                     'Valley of the Dolls is the first novel by American writer Jacqueline Susann. Published in 1966, the book was the biggest selling novel of its year. To date, it has sold more than 31 million copies, making it one of the best-selling works in publishing history.']\n",
    "\n",
    "# answers = ['Instagram','Great Britain',\"Llansilin in Powys\",[\"Judy Garland\", \"Carole Landis\", \"Dean Martin\", \"Ethel Merman\"]]\n",
    "\n",
    "# conditioning = 'Kyutai is a non-profit laboratory dedicated to open research in AI, founded in November 2023 by the iliad Group, CMA CGM and Schmidt Sciences.'\n",
    "# prompts =  prompt_prefix +'Query: when was founded Kyutai?\\nAnswer: '\n",
    "\n",
    "\n",
    "\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "pipeline.model.llm.pos_to_keep = []\n",
    "generated_sequence, logprobs = pipeline.generate(prompts = prompts, \n",
    "                                    text_conditioning = conditioning, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False,\n",
    "                                    random_flip = i_token_to_flip,\n",
    "                                    device = device,\n",
    "                                    device_generation = device if torch.cuda.device_count() <= 1 else torch.device('cuda:1'),\n",
    "                                    return_embeddings = False)\n",
    "print(generated_sequence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vizualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_att_weights, tokens = get_attention(generated_sequence[0], embeddings, pipeline.tokenizer, pipeline.model.llm, n_tokens = 20)\n",
    "# self_att_weights, tokens = get_attention('He played 12 seasons (243 games, 14 goals) in the Serie A for A.C. Milan, A.F.C. Fiorentina, Hellas Verona, Atalanta B.C. and Genoa C.F.C.', embeddings, pipeline.tokenizer, pipeline.model.llm, n_tokens = 20)\n",
    "\n",
    "head_view(self_att_weights, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_att_weights, tokens = get_attention(generated_sequence[0], embeddings, pipeline.tokenizer, pipeline.model.llm, n_tokens = 20)\n",
    "model_view([att[:,:,1:,1:] for att in self_att_weights], [tokens[0]] + tokens[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_per_head_att = [torch.mean(att, dim = 1, keepdim = True) for att in self_att_weights]\n",
    "overall_mean = torch.mean(torch.stack(self_att_weights), dim = 0)\n",
    "\n",
    "head_view(mean_per_head_att, tokens)\n",
    "# head_view([overall_mean], tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop)\n",
    "temperatures = [0, 0.5, 0.7, 1, 1.5]\n",
    "max_tokens = 150\n",
    "\n",
    "results_generation = {'0':{'train': {'word_prompt':{}, 'empty_prompt':{}}, 'valid': {'word_prompt':{}, 'empty_prompt':{}}}, \n",
    "                        '0.5':{'train': {'word_prompt':{}, 'empty_prompt':{}}, 'valid': {'word_prompt':{}, 'empty_prompt':{}}},\n",
    "                        '0.7':{'train': {'word_prompt':{}, 'empty_prompt':{}}, 'valid': {'word_prompt':{}, 'empty_prompt':{}}},\n",
    "                        '1':{'train': {'word_prompt':{}, 'empty_prompt':{}}, 'valid': {'word_prompt':{}, 'empty_prompt':{}}},\n",
    "                        '1.5':{'train': {'word_prompt':{}, 'empty_prompt':{}}, 'valid': {'word_prompt':{}, 'empty_prompt':{}}}}\n",
    "\n",
    "\n",
    "n_passages = len(train_passage)\n",
    "assert n_passages == len(valid_passage)\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f'Temperature: {temp}')    \n",
    "    generated_sequences = []\n",
    "    \n",
    "    for i in range(0, n_passages, max_batch_size):\n",
    "        passage = train_passage[i:i+max_batch_size]\n",
    "        generated_sequence, logprobs = pipeline.generate(prompts = [text.split(' ')[0] for text in passage], \n",
    "                                    text_conditioning = passage, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False,\n",
    "                                     device = device,\n",
    "                                    device_generation = device if torch.cuda.device_count() <= 1 else torch.device('cuda:1'))\n",
    "           \n",
    "        generated_sequences.extend(generated_sequence)\n",
    "    results_generation[str(temp)]['train']['word_prompt'] = {'seq':generated_sequences}\n",
    "    print('Train Passage:', passage)\n",
    "    print('Train Generated:', generated_sequence)\n",
    "    generated_sequences = []\n",
    "    for i in range(0, n_passages, max_batch_size):\n",
    "        passage = train_passage[i:i+max_batch_size]\n",
    "        generated_sequence, logprobs = pipeline.generate(prompts = [''] * len(passage), \n",
    "                                    text_conditioning = passage, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False,\n",
    "                                     device = device,\n",
    "                                    device_generation = device if torch.cuda.device_count() <= 1 else torch.device('cuda:1'))\n",
    "           \n",
    "        generated_sequences.extend(generated_sequence)    \n",
    "    results_generation[str(temp)]['train']['empty_prompt'] = {'seq':generated_sequences}\n",
    "    \n",
    "\n",
    "    generated_sequences = []\n",
    "    for i in range(0, n_passages, max_batch_size):\n",
    "        passage = valid_passage[i:i+max_batch_size]\n",
    "        generated_sequence, logprobs = pipeline.generate(prompts = [text.split(' ')[0] for text in passage], \n",
    "                                    text_conditioning = passage, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False,\n",
    "                                     device = device,\n",
    "                                    device_generation = device if torch.cuda.device_count() <= 1 else torch.device('cuda:1'))\n",
    "           \n",
    "        generated_sequences.extend(generated_sequence)    \n",
    "    results_generation[str(temp)]['valid']['word_prompt'] = {'seq':generated_sequences}\n",
    "    \n",
    "    generated_sequences = []\n",
    "    for i in range(0, n_passages, max_batch_size):\n",
    "        passage = valid_passage[i:i+max_batch_size]\n",
    "        generated_sequence, logprobs = pipeline.generate(prompts = [''] * len(passage), \n",
    "                                    text_conditioning = passage, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False,\n",
    "                                     device = device,\n",
    "                                    device_generation = device if torch.cuda.device_count() <= 1 else torch.device('cuda:1'))\n",
    "           \n",
    "        generated_sequences.extend(generated_sequence)    \n",
    "    results_generation[str(temp)]['valid']['empty_prompt'] = {'seq':generated_sequences}\n",
    "    print('Valid Passage:', passage)\n",
    "    print('Valid Generated:', generated_sequence)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for temp in results_generation.keys():\n",
    "    for split in results_generation[temp].keys():\n",
    "        for prompt_type in results_generation[temp][split].keys():\n",
    "            generated_sequences = results_generation[temp][split][prompt_type]['seq']\n",
    "            if prompt_type == 'empty_prompt':\n",
    "                gt_passage = train_passage if split == 'train' else valid_passage\n",
    "                overlap = word_overlap(gt_passage, generated_sequences)\n",
    "                bleu_score = get_bleu_score(gt_passage, generated_sequences)\n",
    "            elif prompt_type == 'word_prompt':\n",
    "                gt_passage = train_passage if split == 'train' else valid_passage\n",
    "                gt_passage = [' '.join(text.split(' ')[1:]) for text in gt_passage]\n",
    "                overlap = word_overlap(gt_passage, generated_sequences)\n",
    "                bleu_score = get_bleu_score(gt_passage, generated_sequences)\n",
    "   \n",
    "            print(f'Temperature: {temp}, Split: {split}, Prompt Type: {prompt_type}, Overlap: {overlap}', 'Bleu Score:', bleu_score)\n",
    "            metrics.append({'temp': temp, 'split': split, 'prompt_type': prompt_type, 'overlap': overlap, 'bleu_score': bleu_score})\n",
    "            \n",
    "# with open(f'{ckpt_path}/results_generation.json', 'w') as f:\n",
    "#     json.dump(metrics, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_path = '/lustre/scwpod02/client/kyutai-interns/hippop/models/mistral_7B'\n",
    "#Must have a params json for pipeline\n",
    "\n",
    "# No embeddings:\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/no_embed_bs16_lr5e-5Mistral7B88d0b42410aa4ec12025/checkpoints/checkpoint_002500'\n",
    "\n",
    "# Length tokens:\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_512t_Mistral7B20ed0018b2a84fba09c4/checkpoints/checkpoint_005000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_256t_Mistral7Be9ffc00fa42bedbc50d0/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_128t_Mistral7B226729d875c65b331ef8/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_64t_Mistral7B9bbea1b3b8dc23079b04/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_32t_Mistral7Bccbc3f29d69bd124c6cf/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_16t_Mistral7B7bc7dcc2ba28873eda96/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/mean_not_causal/checkpoints/checkpoint_007500'\n",
    "\n",
    "\n",
    "# # Continuation:\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/continuation_Mistral7B20ed0018b2a84fba09c4/checkpoints/checkpoint_006000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/mean_finetuned_notcausal_continuationMistral7B20ed0018b2a84fba09c4/checkpoints/checkpoint_005500'\n",
    "\n",
    "# # Cross-Attention:\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/cross_att_5_last_layersMistral7Bdbbb7faebb2f32cf20e9/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/cross_att_fine_tuned_embedder_5_last_layersMistral7Bdbbb7faebb2f32cf20e9/checkpoints/checkpoint_007500'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/cross_att_finetuned_notcausal_continuationMistral7B20ed0018b2a84fba09c4/checkpoints/checkpoint_005000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/cross_att_pretrained_continuationMistral7B20ed0018b2a84fba09c4/checkpoints/checkpoint_008500'\n",
    "\n",
    "with open(f'{ckpt_path}/params.json') as f:\n",
    "    params = json.load(f)\n",
    "print(params)\n",
    "\n",
    "model_name = 'Mistral7B' # Mistral7B, Llama3.2-3B, Gemma7B\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "w_embeds = True\n",
    "max_batch_size = 4\n",
    "\n",
    "# variant = '7b' if model_name == 'Gemma7B' else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify old params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ckpt_path + '/params.json') as f:\n",
    "    params = json.load(f)\n",
    "print(params)\n",
    "# if 'do_pool'  not in params.keys():\n",
    "if 'n_truncated_layers' in params['pooling_module'].keys():\n",
    "    params['n_truncated_layers'] = params['pooling_module']['n_truncated_layers']\n",
    "    del params['pooling_module']['n_truncated_layers']\n",
    "    \n",
    "\n",
    "if params['cross_att'] is not None:\n",
    "    print('here')\n",
    "    params['normalize_embeddings'] = True if params['cross_att'] else False\n",
    "    if params['start_cross_att'] is None:\n",
    "        del params['start_cross_att']\n",
    "    else:\n",
    "        params['cross_att_layers'] = 32 - params[\"start_cross_att\"]\n",
    "        del params['start_cross_att']\n",
    "    params['do_pool'] = False if params['cross_att'] else True\n",
    "else:\n",
    "    params['do_pool'] = True\n",
    "print(params)\n",
    "with open(ckpt_path + '/params.json', 'w') as f:\n",
    "    json.dump(params, f)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in tests:\n",
    "    print('Param:', param)\n",
    "    if param['w_embeds']:\n",
    "        pipeline.pipeline_args.w_embeds = True\n",
    "    else:\n",
    "        pipeline.pipeline_args.w_embeds = False\n",
    "    \n",
    "    final_valid_prompts = [passage.split(' ')[0] for passage in valid_passage][2] \n",
    "    text_valid_conditioning = [passage[:100] for passage in valid_passage][2]\n",
    "    print('Prompt', final_valid_prompts, ' | Passage', text_valid_conditioning)\n",
    "    generated_sequence = pipeline.generate(prompts = final_valid_prompts, \n",
    "                                        text_conditioning = text_valid_conditioning, \n",
    "                                        temperature = param['temperature'], \n",
    "                                        max_tokens = max_tokens,\n",
    "                                        truncate_double_space = False)\n",
    "    print('Valid  word', generated_sequence)\n",
    "    \n",
    "    final_valid_prompts = ['' for passage in train_passage][1]\n",
    "    text_valid_conditioning = [passage[:100] for passage in valid_passage][2]\n",
    "    generated_sequence = pipeline.generate(prompts = final_valid_prompts, \n",
    "                                        text_conditioning = text_valid_conditioning, \n",
    "                                        temperature = param['temperature'], \n",
    "                                        max_tokens = max_tokens,\n",
    "                                        truncate_double_space = False)\n",
    "    print('Valid  empty', generated_sequence)\n",
    "\n",
    "    final_train_prompts =  [passage.split(' ')[0] for passage in train_passage][1] \n",
    "    text_train_conditioning = [passage[:100] for passage in train_passage][1]\n",
    "    print('Prompt', final_train_prompts, ' | Passage', text_train_conditioning)\n",
    "    generated_sequence = pipeline.generate(prompts = final_train_prompts, \n",
    "                                       text_conditioning = text_train_conditioning, \n",
    "                                       temperature = param['temperature'], \n",
    "                                       max_tokens = max_tokens,\n",
    "                                       truncate_double_space = False)\n",
    "    print('Train word', generated_sequence)\n",
    "    final_train_prompts = ['' for passage in train_passage][1] \n",
    "    text_train_conditioning = [passage[:100] for passage in train_passage][1]\n",
    "    generated_sequence = pipeline.generate(prompts = final_train_prompts, \n",
    "                                       text_conditioning = text_train_conditioning, \n",
    "                                       temperature = param['temperature'], \n",
    "                                       max_tokens = max_tokens,\n",
    "                                       truncate_double_space = False)\n",
    "    print('Train empty', generated_sequence)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 information in the doc which enables to answer the question but not good response often in-context\n",
    "# 2 information in the doc which enables to answer the question and good response often in-context\n",
    "# 3 Hard negative passage\n",
    "# 4 Same\n",
    "\n",
    "# prompt_prefix = \"Query: who wrote the song photograph by ringo starr\\nAnswer: Ringo Starr\\n\\nQuery: who is playing the halftime show at super bowl 2016\\nAnswer: Coldplay\\n\\nQuery: where was the world economic forum held this year\\nAnswer: Davos\\n\\nQuery: where are the giant redwoods located in california\\nAnswer: Humboldt County\\n\\nQuery: who has made the most premier league appearances\\nAnswer: Gareth Barry\\n\\nQuery: \"\n",
    "# prompts = ['who has most followers on instagram in world','who did the united states win its independence from', 'locations for the film an englishman who went up a hill', 'who is the valley of the dolls based on']\n",
    "# final_prompts = [prompt_prefix + prompt + '\\nAnswer:' for prompt in prompts]\n",
    "\n",
    "# text_conditioning = [\"This list contains the top 50 accounts with the most followers on the photo and video-sharing social platform Instagram. As of July 2019, the most followed user is Instagram's own account, with over 308 million followers. Cristiano Ronaldo is the most followed individual, with over 177 million followers. Fifteen accounts have exceeded 100 million followers on the site.\",\n",
    "#                      \"During the American Revolution, the legal separation of the thirteen colonies from Great Britain in 1776 actually occurred on July 2, when the Second Continental Congress voted to approve a resolution of independence that had been proposed in June by Richard Henry Lee of Virginia declaring the United States independent from Great Britain's rule. After voting for independence, Congress turned its attention to the Declaration of Independence, a statement explaining this decision, which had been prepared by a Committee of Five, with Thomas Jefferson as its principal author. Congress debated and revised the wording of the Declaration, finally approving it two days later on July 4. A day earlier, John Adams had written to his wife Abigail\",\n",
    "#                      'The village was a primary location for the making of the film \\\"The Englishman Who Went Up a Hill But Came Down a Mountain\\\", which starred Hugh Grant. The hilltop scenes were filmed on the Gyrn, the long hill that overlooks the village. It was also featured in \\\"Monk\\'s Hood\\\", an episode of \\\"The Cadfael Chronicles\\\"',\n",
    "#                      'Valley of the Dolls is the first novel by American writer Jacqueline Susann. Published in 1966, the book was the biggest selling novel of its year. To date, it has sold more than 31 million copies, making it one of the best-selling works in publishing history.']\n",
    "\n",
    "# answers = ['Instagram','Great Britain',\"Llansilin in Powys\",[\"Judy Garland\", \"Carole Landis\", \"Dean Martin\", \"Ethel Merman\"]]\n",
    "\n",
    "n_passages = 4\n",
    "eval_data = '/lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_valid.jsonl'\n",
    "train_data = '/lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_train.jsonl'\n",
    "train_passage = []\n",
    "valid_passage = []\n",
    "with open(train_data, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == n_passages:\n",
    "            break\n",
    "        train_passage.append(json.loads(line)['text'].split('\\n\\n')[1])\n",
    "        \n",
    "with open(eval_data, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == n_passages:\n",
    "            break\n",
    "        valid_passage.append(json.loads(line)['text'].split('\\n\\n')[1])\n",
    "        \n",
    "tests = [{'w_embeds': True, 'temperature': 0 },  {'w_embeds': True, 'temperature': 0.7 }, {'w_embeds': False, 'temperature': 0.7 }]\n",
    "# print('Train passage:', train_passage)\n",
    "# print('Valid passage:', valid_passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditioning = ['Kyutai is a non-profit laboratory dedicated to open research in AI, founded in November 2023 by the iliad Group, CMA CGM and Schmidt Sciences. Launched with an initial team of six leading scientists, who have all worked with Big Tech labs in the USA, Kyutai continues to recruit at the highest level, and also offers internships to research Master’s degree students.']*4\n",
    "prompts = ['who are the founders of Kyutai?', 'when was Kyutai founded?', 'how many scientists were in the initial team?', 'what does Kyutai offer to research Master’s degree students?']\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "generated_sequence = pipeline.generate(prompts = prompts,\n",
    "                                      text_conditioning = conditioning,\n",
    "                                      temperature = 0.5, \n",
    "                                      max_tokens =200,\n",
    "                                      truncate_double_space = False)\n",
    "# random_flip, put the number of the token to flip. \n",
    "print(generated_sequence)\n",
    "\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "generated_sequence, logprobs = pipeline.generate(prompts = ['who has most followers on Instagram in world?'],\n",
    "                                      text_conditioning = [\"This list contains the top 50 accounts with the most followers on the photo and video-sharing social platform Instagram. As of July 2019, the most followed user is Instagram's own account, with over 308 million followers. Cristiano Ronaldo is the most followed individual, with over 177 million followers.\"],\n",
    "                                      temperature = 0.4, \n",
    "                                      max_tokens =200,\n",
    "                                      truncate_double_space = False)\n",
    "print(generated_sequence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuation\n",
    "for param in tests:\n",
    "    print('Param:', param)\n",
    "    if param['w_embeds']:\n",
    "        pipeline.pipeline_args.w_embeds = True\n",
    "    else:\n",
    "        pipeline.pipeline_args.w_embeds = False\n",
    "    \n",
    "    final_valid_prompts = [passage[100:].split(' ')[0] for passage in valid_passage][2] \n",
    "    text_valid_conditioning = [passage[:100] for passage in valid_passage][2]\n",
    "    print('Passage', text_valid_conditioning, ' | Truth', [passage[100:200] for passage in valid_passage][2] )\n",
    "    generated_sequence = pipeline.generate(prompts = final_valid_prompts, \n",
    "                                        text_conditioning = text_valid_conditioning, \n",
    "                                        temperature = param['temperature'], \n",
    "                                        max_tokens = max_tokens,\n",
    "                                        truncate_double_space = False)\n",
    "    print('Valid  word', generated_sequence)\n",
    "    \n",
    "    final_valid_prompts = ['' for passage in train_passage][2]\n",
    "    text_valid_conditioning = [passage[:100] for passage in valid_passage][2]\n",
    "    generated_sequence = pipeline.generate(prompts = final_valid_prompts, \n",
    "                                        text_conditioning = text_valid_conditioning, \n",
    "                                        temperature = param['temperature'], \n",
    "                                        max_tokens = max_tokens,\n",
    "                                        truncate_double_space = False)\n",
    "    print('Valid  empty', generated_sequence)\n",
    "\n",
    "    final_train_prompts =  [passage[100:].split(' ')[0] for passage in train_passage][1] \n",
    "    text_train_conditioning = [passage[:100] for passage in train_passage][1]\n",
    "    print('Passage', text_train_conditioning, ' | Truth', [passage[100:200] for passage in train_passage][1] )\n",
    "    generated_sequence = pipeline.generate(prompts = final_train_prompts, \n",
    "                                       text_conditioning = text_train_conditioning, \n",
    "                                       temperature = param['temperature'], \n",
    "                                       max_tokens = max_tokens,\n",
    "                                       truncate_double_space = False)\n",
    "    print('Train word', generated_sequence)\n",
    "    final_train_prompts = ['' for passage in train_passage][1] \n",
    "    text_train_conditioning = [passage[:100] for passage in train_passage][1]\n",
    "    generated_sequence = pipeline.generate(prompts = final_train_prompts, \n",
    "                                       text_conditioning = text_train_conditioning, \n",
    "                                       temperature = param['temperature'], \n",
    "                                       max_tokens = max_tokens,\n",
    "                                       truncate_double_space = False)\n",
    "    print('Train empty', generated_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tested\n",
    "# run_name = '128_SL_FN_False_0_MLP_False_CA_False_DB'\n",
    "# run_name = '128_SL_FN_False_0_MLP_True_CA_16_CAL_False_SKV_False_DB'\n",
    "# run_name = '128_SL_FN_False_0_MLP_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_0_MLP_True_CA_24_CAL_False_SKV_True_DB'\n",
    "\n",
    "# Finished runs:\n",
    "\n",
    "# run_name = '128_SL_FN_Truemean_1_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB' # 008500\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_4_TRUNC_True_CA_5_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_4_TRUNC_True_CA_24_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_1_MLP_4_TRUNC_True_CA_5_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_1_MLP_4_TRUNC_True_CA_24_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_0_MLP_4_TRUNC_True_CA_5_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_0_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_0_MLP_4_TRUNC_True_CA_24_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truelatent_attention_0_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truelatent_attention_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Trueeos_0_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Trueeos_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truereversed_latent_atttention_0_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truereversed_latent_attention_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_0_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_False_DB'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_False_DB'\n",
    "# run_name = '128_SL_FN_Truemean_0_MLP_8_TRUNC_True_CA_16_CAL_False_SKV_False_DB'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_8_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_False_DB'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB_dist_process'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_4_TRUNC_False_CA_False_DB'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_8_TRUNC_False_CA_False_DB'\n",
    "# run_name = '128_SL_FN_Truemean_0_MLP_4_TRUNC_False_CA_False_DB'\n",
    "# run_name = '128_SL_FN_Truemean_0_MLP_8_TRUNC_False_CA_False_DB'\n",
    "# run_name = '128_SL_FN_False_3_MLP_True_CA_5_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_3_MLP_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_3_MLP_True_CA_24_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_1_MLP_True_CA_5_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_1_MLP_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_1_MLP_True_CA_24_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_0_MLP_True_CA_5_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_3_MLP_True_CA_16_CAL_False_SKV_False_DB'\n",
    "# run_name = '128_SL_FN_False_3_MLP_False_CA_False_DB'\n",
    "# run_name = '128_SL_FN_False_3_MLP_True_CA_16_CAL_True_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_3_MLP_True_CA_16_CAL_False_SKV_True_DB_dist_process'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
