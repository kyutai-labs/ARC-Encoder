{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/hippolytepilchen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "RUN NAME =>  128_SL_FN_Truemean_0_MLP_8_TRUNC_True_CA_16_CAL_False_SKV_False_DB_old_gate_more_params\n",
      "Loading cross att state dict\n",
      "Not only LoRA weights found in the checkpoint. Skipping other weights.\n",
      "Loading and merging LoRA weights...\n",
      "Loading and merging LoRA weights...\n",
      "Evaluating checkpoint 009500\n",
      "Temperature: 0\n",
      "Temperature: 0.5\n",
      "Temperature: 0.7\n",
      "Temperature: 1\n",
      "CKPT: 9500, Temperature: 0, Split: valid, Prompt Type: empty_prompt, Overlap: 0.8472436094856791 Bleu Score: 0.6058873090924113 EM: 0.01 Meteor: 0.83910893562649 Bleu Score Avg: 0.6730484811302462\n",
      "CKPT: 9500, Temperature: 0.5, Split: valid, Prompt Type: empty_prompt, Overlap: 0.8343085925469664 Bleu Score: 0.5590312482884036 EM: 0.01 Meteor: 0.8129617002242356 Bleu Score Avg: 0.6324018358190517\n",
      "CKPT: 9500, Temperature: 0.7, Split: valid, Prompt Type: empty_prompt, Overlap: 0.8312288266091777 Bleu Score: 0.5633220983875208 EM: 0.01 Meteor: 0.8155752637503281 Bleu Score Avg: 0.6311201789051689\n",
      "CKPT: 9500, Temperature: 1, Split: valid, Prompt Type: empty_prompt, Overlap: 0.8173698798891285 Bleu Score: 0.4777772599493344 EM: 0.0 Meteor: 0.775037650278287 Bleu Score Avg: 0.5417564977318037\n",
      "Memory: 20.093032836914062\n",
      "Memory Cached: 51.21875\n",
      "Max Memory Allocated: 51.17201519012451\n",
      "Reset memory ! \n"
     ]
    }
   ],
   "source": [
    "!python embed_llm/generation/evaluation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from embed_llm.models.augmented_model import EmbedAugPipeline\n",
    "from embed_llm.generation.evaluation import  ensure_reproducibility\n",
    "from bertviz import head_view, model_view # type: ignore\n",
    "from embed_llm.models.mistral.generate import get_attention\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_reproducibility(29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Must have a params json for pipeline\n",
    "llm_path = '/lustre/scwpod02/client/kyutai-interns/hippop/models/mistral_7B'\n",
    "\n",
    "# run_name = '128_SL_FN_False_3_MLP_True_CA_24_CAL_False_SKV_True_DB_new_pooled_cross_att'\n",
    "# run_name = '128_SL_FN_Truereversed_latent_attention_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_3_MLP_True_CA_24_CAL_False_SKV_True_DB'\n",
    "run_name = 'best_finetuned_128L_4978e57368'\n",
    "\n",
    "n = '020000' # '008500' #'010000' \n",
    "ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/' + run_name \n",
    "\n",
    "with open(f'{ckpt_path}'+ '/checkpoints/checkpoint_'+n+'/params.json') as f:\n",
    "    params = json.load(f)\n",
    "print(params)\n",
    "\n",
    "model_name = 'Mistral7B' \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    device = torch.device('cuda:0')\n",
    "    print(f'Using {device} for loading')\n",
    "\n",
    "w_embeds = True\n",
    "max_batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline: EmbedAugPipeline = EmbedAugPipeline.load_inference_model(llm_path = llm_path, \n",
    "                                                                   ckpt_path = ckpt_path + '/checkpoints/checkpoint_'+n, \n",
    "                                                                   device = device,\n",
    "                                                                   llm_name = model_name, \n",
    "                                                                   embed_model_name = 'NVEmbed', # Not used if pretrainde ckpt available\n",
    "                                                                    max_batch_size = max_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_passages = 20\n",
    "\n",
    "lim_toks = 128\n",
    "eval_data = '/lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_valid.jsonl'\n",
    "train_data = '/lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_train.jsonl'\n",
    "train_passage = []\n",
    "valid_passage = []\n",
    "\n",
    "with open(train_data, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == n_passages:\n",
    "            break\n",
    "        train_passage.append(pipeline.tokenizer.decode(pipeline.tokenizer.encode(json.loads(line)['text'].split('\\n\\n')[1], eos = True, bos = True)[:lim_toks]))\n",
    "  \n",
    "with open(eval_data, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == n_passages:\n",
    "            break\n",
    "        valid_passage.append(pipeline.tokenizer.decode(pipeline.tokenizer.encode(json.loads(line)['text'].split('\\n\\n')[1], eos = True, bos = True)[:lim_toks]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flipping attempts\n",
    "w_embeds = True\n",
    "temp = 0\n",
    "max_tokens = 128\n",
    "i_token_to_flip = -1\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "\n",
    "temp = [temp] * max_tokens   \n",
    "if max_tokens > i_token_to_flip >= 0:\n",
    "    temp[i_token_to_flip] = 1000\n",
    "    \n",
    "prompt = ''\n",
    "text_conditioning ='Mario Bortolazzi (born 10 January 1965, in Verona) is an Italian professional football coach and a former player, who played as a midfielder. \\\n",
    "    \\n\\nHe played 12 seasons (241 games, 14 goals) in the Serie A for ACF Fiorentina, A.C. Milan, Hellas Verona F.C., Atalanta B.C. and Genoa C.F.C.'\n",
    "        # \\n\\nIn his coaching career he has so far has always been an assistant to his former Milan teammate Roberto Donadoni.\\\n",
    "        #     \\n\\nHonours\\n\\n - Milan\\n - Serie A champion: 1987–88.\\n\\n - Genoa\\n - Anglo-Italian Cup winner: 1995–96.'\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "generated_sequence, attn_or_logprobs, embeddings = pipeline.generate(prompts = prompt, \n",
    "                                    text_conditioning = text_conditioning, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False,\n",
    "                                    random_flip = i_token_to_flip,\n",
    "                                    device = device,\n",
    "                                    device_generation = device if torch.cuda.device_count() <= 1 else torch.device('cuda:1'),\n",
    "                                    return_embeddings = True)\n",
    "print(generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_embeds = True\n",
    "temp = 0\n",
    "max_tokens = 128\n",
    "i_token_to_flip = -1\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "\n",
    "temp = [temp] * max_tokens   \n",
    "if max_tokens > i_token_to_flip >= 0:\n",
    "    temp[i_token_to_flip] = 1000\n",
    "# 1 information in the doc which enables to answer the question but not good response often in-context\n",
    "# 2 information in the doc which enables to answer the question and good response often in-context\n",
    "# 3 Hard negative passage\n",
    "# 4 Same\n",
    "\n",
    "prompt_prefix = \"Query: \"\n",
    "prompts = ['who has most followers on instagram in world','who did the united states win its independence from', 'locations for the film an englishman who went up a hill', 'who is the valley of the dolls based on']\n",
    "prompts = [prompt_prefix + prompt + '\\nAnswer:' for prompt in prompts]\n",
    "\n",
    "conditioning = [\"This list contains the top 50 accounts with the most followers on the photo and video-sharing social platform Instagram. As of July 2019, the most followed user is Instagram's own account, with over 308 million followers. Cristiano Ronaldo is the most followed individual, with over 177 million followers. Fifteen accounts have exceeded 100 million followers on the site.\",\n",
    "                     \"During the American Revolution, the legal separation of the thirteen colonies from Great Britain in 1776 actually occurred on July 2, when the Second Continental Congress voted to approve a resolution of independence that had been proposed in June by Richard Henry Lee of Virginia declaring the United States independent from Great Britain's rule. After voting for independence, Congress turned its attention to the Declaration of Independence, a statement explaining this decision, which had been prepared by a Committee of Five, with Thomas Jefferson as its principal author. Congress debated and revised the wording of the Declaration, finally approving it two days later on July 4. A day earlier, John Adams had written to his wife Abigail\",\n",
    "                     'The village was a primary location for the making of the film \\\"The Englishman Who Went Up a Hill But Came Down a Mountain\\\", which starred Hugh Grant. The hilltop scenes were filmed on the Gyrn, the long hill that overlooks the village. It was also featured in \\\"Monk\\'s Hood\\\", an episode of \\\"The Cadfael Chronicles\\\"',\n",
    "                     'Valley of the Dolls is the first novel by American writer Jacqueline Susann. Published in 1966, the book was the biggest selling novel of its year. To date, it has sold more than 31 million copies, making it one of the best-selling works in publishing history.']\n",
    "\n",
    "# answers = ['Instagram','Great Britain',\"Llansilin in Powys\",[\"Judy Garland\", \"Carole Landis\", \"Dean Martin\", \"Ethel Merman\"]]\n",
    "\n",
    "# conditioning = 'Kyutai is a non-profit laboratory dedicated to open research in AI, founded in November 2023 by the iliad Group, CMA CGM and Schmidt Sciences.'\n",
    "# prompts =  prompt_prefix +'Query: when was founded Kyutai?\\nAnswer: '\n",
    "\n",
    "\n",
    "\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "pipeline.model.llm.pos_to_keep = []\n",
    "generated_sequence, logprobs = pipeline.generate(prompts = prompts, \n",
    "                                    text_conditioning = conditioning, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False,\n",
    "                                    random_flip = i_token_to_flip,\n",
    "                                    device = device,\n",
    "                                    device_generation = device if torch.cuda.device_count() <= 1 else torch.device('cuda:1'),\n",
    "                                    return_embeddings = False)\n",
    "print(generated_sequence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vizualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_att_weights, tokens = get_attention(generated_sequence[0], embeddings, pipeline.tokenizer, pipeline.model.llm, n_tokens = 20)\n",
    "# self_att_weights, tokens = get_attention('He played 12 seasons (243 games, 14 goals) in the Serie A for A.C. Milan, A.F.C. Fiorentina, Hellas Verona, Atalanta B.C. and Genoa C.F.C.', embeddings, pipeline.tokenizer, pipeline.model.llm, n_tokens = 20)\n",
    "\n",
    "head_view(self_att_weights, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_att_weights, tokens = get_attention(generated_sequence[0], embeddings, pipeline.tokenizer, pipeline.model.llm, n_tokens = 20)\n",
    "model_view([att[:,:,1:,1:] for att in self_att_weights], [tokens[0]] + tokens[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_per_head_att = [torch.mean(att, dim = 1, keepdim = True) for att in self_att_weights]\n",
    "overall_mean = torch.mean(torch.stack(self_att_weights), dim = 0)\n",
    "\n",
    "head_view(mean_per_head_att, tokens)\n",
    "# head_view([overall_mean], tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop)\n",
    "temperatures = [0, 0.5, 0.7, 1, 1.5]\n",
    "max_tokens = 150\n",
    "\n",
    "results_generation = {'0':{'train': {'word_prompt':{}, 'empty_prompt':{}}, 'valid': {'word_prompt':{}, 'empty_prompt':{}}}, \n",
    "                        '0.5':{'train': {'word_prompt':{}, 'empty_prompt':{}}, 'valid': {'word_prompt':{}, 'empty_prompt':{}}},\n",
    "                        '0.7':{'train': {'word_prompt':{}, 'empty_prompt':{}}, 'valid': {'word_prompt':{}, 'empty_prompt':{}}},\n",
    "                        '1':{'train': {'word_prompt':{}, 'empty_prompt':{}}, 'valid': {'word_prompt':{}, 'empty_prompt':{}}},\n",
    "                        '1.5':{'train': {'word_prompt':{}, 'empty_prompt':{}}, 'valid': {'word_prompt':{}, 'empty_prompt':{}}}}\n",
    "\n",
    "\n",
    "n_passages = len(train_passage)\n",
    "assert n_passages == len(valid_passage)\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f'Temperature: {temp}')    \n",
    "    generated_sequences = []\n",
    "    \n",
    "    for i in range(0, n_passages, max_batch_size):\n",
    "        passage = train_passage[i:i+max_batch_size]\n",
    "        generated_sequence, logprobs = pipeline.generate(prompts = [text.split(' ')[0] for text in passage], \n",
    "                                    text_conditioning = passage, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False,\n",
    "                                     device = device,\n",
    "                                    device_generation = device if torch.cuda.device_count() <= 1 else torch.device('cuda:1'))\n",
    "           \n",
    "        generated_sequences.extend(generated_sequence)\n",
    "    results_generation[str(temp)]['train']['word_prompt'] = {'seq':generated_sequences}\n",
    "    print('Train Passage:', passage)\n",
    "    print('Train Generated:', generated_sequence)\n",
    "    generated_sequences = []\n",
    "    for i in range(0, n_passages, max_batch_size):\n",
    "        passage = train_passage[i:i+max_batch_size]\n",
    "        generated_sequence, logprobs = pipeline.generate(prompts = [''] * len(passage), \n",
    "                                    text_conditioning = passage, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False,\n",
    "                                     device = device,\n",
    "                                    device_generation = device if torch.cuda.device_count() <= 1 else torch.device('cuda:1'))\n",
    "           \n",
    "        generated_sequences.extend(generated_sequence)    \n",
    "    results_generation[str(temp)]['train']['empty_prompt'] = {'seq':generated_sequences}\n",
    "    \n",
    "\n",
    "    generated_sequences = []\n",
    "    for i in range(0, n_passages, max_batch_size):\n",
    "        passage = valid_passage[i:i+max_batch_size]\n",
    "        generated_sequence, logprobs = pipeline.generate(prompts = [text.split(' ')[0] for text in passage], \n",
    "                                    text_conditioning = passage, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False,\n",
    "                                     device = device,\n",
    "                                    device_generation = device if torch.cuda.device_count() <= 1 else torch.device('cuda:1'))\n",
    "           \n",
    "        generated_sequences.extend(generated_sequence)    \n",
    "    results_generation[str(temp)]['valid']['word_prompt'] = {'seq':generated_sequences}\n",
    "    \n",
    "    generated_sequences = []\n",
    "    for i in range(0, n_passages, max_batch_size):\n",
    "        passage = valid_passage[i:i+max_batch_size]\n",
    "        generated_sequence, logprobs = pipeline.generate(prompts = [''] * len(passage), \n",
    "                                    text_conditioning = passage, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False,\n",
    "                                     device = device,\n",
    "                                    device_generation = device if torch.cuda.device_count() <= 1 else torch.device('cuda:1'))\n",
    "           \n",
    "        generated_sequences.extend(generated_sequence)    \n",
    "    results_generation[str(temp)]['valid']['empty_prompt'] = {'seq':generated_sequences}\n",
    "    print('Valid Passage:', passage)\n",
    "    print('Valid Generated:', generated_sequence)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for temp in results_generation.keys():\n",
    "    for split in results_generation[temp].keys():\n",
    "        for prompt_type in results_generation[temp][split].keys():\n",
    "            generated_sequences = results_generation[temp][split][prompt_type]['seq']\n",
    "            if prompt_type == 'empty_prompt':\n",
    "                gt_passage = train_passage if split == 'train' else valid_passage\n",
    "                overlap = word_overlap(gt_passage, generated_sequences)\n",
    "                bleu_score = get_bleu_score(gt_passage, generated_sequences)\n",
    "            elif prompt_type == 'word_prompt':\n",
    "                gt_passage = train_passage if split == 'train' else valid_passage\n",
    "                gt_passage = [' '.join(text.split(' ')[1:]) for text in gt_passage]\n",
    "                overlap = word_overlap(gt_passage, generated_sequences)\n",
    "                bleu_score = get_bleu_score(gt_passage, generated_sequences)\n",
    "   \n",
    "            print(f'Temperature: {temp}, Split: {split}, Prompt Type: {prompt_type}, Overlap: {overlap}', 'Bleu Score:', bleu_score)\n",
    "            metrics.append({'temp': temp, 'split': split, 'prompt_type': prompt_type, 'overlap': overlap, 'bleu_score': bleu_score})\n",
    "            \n",
    "# with open(f'{ckpt_path}/results_generation.json', 'w') as f:\n",
    "#     json.dump(metrics, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_path = '/lustre/scwpod02/client/kyutai-interns/hippop/models/mistral_7B'\n",
    "#Must have a params json for pipeline\n",
    "\n",
    "# No embeddings:\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/no_embed_bs16_lr5e-5Mistral7B88d0b42410aa4ec12025/checkpoints/checkpoint_002500'\n",
    "\n",
    "# Length tokens:\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_512t_Mistral7B20ed0018b2a84fba09c4/checkpoints/checkpoint_005000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_256t_Mistral7Be9ffc00fa42bedbc50d0/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_128t_Mistral7B226729d875c65b331ef8/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_64t_Mistral7B9bbea1b3b8dc23079b04/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_32t_Mistral7Bccbc3f29d69bd124c6cf/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_16t_Mistral7B7bc7dcc2ba28873eda96/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/mean_not_causal/checkpoints/checkpoint_007500'\n",
    "\n",
    "\n",
    "# # Continuation:\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/continuation_Mistral7B20ed0018b2a84fba09c4/checkpoints/checkpoint_006000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/mean_finetuned_notcausal_continuationMistral7B20ed0018b2a84fba09c4/checkpoints/checkpoint_005500'\n",
    "\n",
    "# # Cross-Attention:\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/cross_att_5_last_layersMistral7Bdbbb7faebb2f32cf20e9/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/cross_att_fine_tuned_embedder_5_last_layersMistral7Bdbbb7faebb2f32cf20e9/checkpoints/checkpoint_007500'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/cross_att_finetuned_notcausal_continuationMistral7B20ed0018b2a84fba09c4/checkpoints/checkpoint_005000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/cross_att_pretrained_continuationMistral7B20ed0018b2a84fba09c4/checkpoints/checkpoint_008500'\n",
    "\n",
    "with open(f'{ckpt_path}/params.json') as f:\n",
    "    params = json.load(f)\n",
    "print(params)\n",
    "\n",
    "model_name = 'Mistral7B' # Mistral7B, Llama3.2-3B, Gemma7B\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "w_embeds = True\n",
    "max_batch_size = 4\n",
    "\n",
    "# variant = '7b' if model_name == 'Gemma7B' else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify old params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ckpt_path + '/params.json') as f:\n",
    "    params = json.load(f)\n",
    "print(params)\n",
    "# if 'do_pool'  not in params.keys():\n",
    "if 'n_truncated_layers' in params['pooling_module'].keys():\n",
    "    params['n_truncated_layers'] = params['pooling_module']['n_truncated_layers']\n",
    "    del params['pooling_module']['n_truncated_layers']\n",
    "    \n",
    "\n",
    "if params['cross_att'] is not None:\n",
    "    print('here')\n",
    "    params['normalize_embeddings'] = True if params['cross_att'] else False\n",
    "    if params['start_cross_att'] is None:\n",
    "        del params['start_cross_att']\n",
    "    else:\n",
    "        params['cross_att_layers'] = 32 - params[\"start_cross_att\"]\n",
    "        del params['start_cross_att']\n",
    "    params['do_pool'] = False if params['cross_att'] else True\n",
    "else:\n",
    "    params['do_pool'] = True\n",
    "print(params)\n",
    "with open(ckpt_path + '/params.json', 'w') as f:\n",
    "    json.dump(params, f)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in tests:\n",
    "    print('Param:', param)\n",
    "    if param['w_embeds']:\n",
    "        pipeline.pipeline_args.w_embeds = True\n",
    "    else:\n",
    "        pipeline.pipeline_args.w_embeds = False\n",
    "    \n",
    "    final_valid_prompts = [passage.split(' ')[0] for passage in valid_passage][2] \n",
    "    text_valid_conditioning = [passage[:100] for passage in valid_passage][2]\n",
    "    print('Prompt', final_valid_prompts, ' | Passage', text_valid_conditioning)\n",
    "    generated_sequence = pipeline.generate(prompts = final_valid_prompts, \n",
    "                                        text_conditioning = text_valid_conditioning, \n",
    "                                        temperature = param['temperature'], \n",
    "                                        max_tokens = max_tokens,\n",
    "                                        truncate_double_space = False)\n",
    "    print('Valid  word', generated_sequence)\n",
    "    \n",
    "    final_valid_prompts = ['' for passage in train_passage][1]\n",
    "    text_valid_conditioning = [passage[:100] for passage in valid_passage][2]\n",
    "    generated_sequence = pipeline.generate(prompts = final_valid_prompts, \n",
    "                                        text_conditioning = text_valid_conditioning, \n",
    "                                        temperature = param['temperature'], \n",
    "                                        max_tokens = max_tokens,\n",
    "                                        truncate_double_space = False)\n",
    "    print('Valid  empty', generated_sequence)\n",
    "\n",
    "    final_train_prompts =  [passage.split(' ')[0] for passage in train_passage][1] \n",
    "    text_train_conditioning = [passage[:100] for passage in train_passage][1]\n",
    "    print('Prompt', final_train_prompts, ' | Passage', text_train_conditioning)\n",
    "    generated_sequence = pipeline.generate(prompts = final_train_prompts, \n",
    "                                       text_conditioning = text_train_conditioning, \n",
    "                                       temperature = param['temperature'], \n",
    "                                       max_tokens = max_tokens,\n",
    "                                       truncate_double_space = False)\n",
    "    print('Train word', generated_sequence)\n",
    "    final_train_prompts = ['' for passage in train_passage][1] \n",
    "    text_train_conditioning = [passage[:100] for passage in train_passage][1]\n",
    "    generated_sequence = pipeline.generate(prompts = final_train_prompts, \n",
    "                                       text_conditioning = text_train_conditioning, \n",
    "                                       temperature = param['temperature'], \n",
    "                                       max_tokens = max_tokens,\n",
    "                                       truncate_double_space = False)\n",
    "    print('Train empty', generated_sequence)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 information in the doc which enables to answer the question but not good response often in-context\n",
    "# 2 information in the doc which enables to answer the question and good response often in-context\n",
    "# 3 Hard negative passage\n",
    "# 4 Same\n",
    "\n",
    "# prompt_prefix = \"Query: who wrote the song photograph by ringo starr\\nAnswer: Ringo Starr\\n\\nQuery: who is playing the halftime show at super bowl 2016\\nAnswer: Coldplay\\n\\nQuery: where was the world economic forum held this year\\nAnswer: Davos\\n\\nQuery: where are the giant redwoods located in california\\nAnswer: Humboldt County\\n\\nQuery: who has made the most premier league appearances\\nAnswer: Gareth Barry\\n\\nQuery: \"\n",
    "# prompts = ['who has most followers on instagram in world','who did the united states win its independence from', 'locations for the film an englishman who went up a hill', 'who is the valley of the dolls based on']\n",
    "# final_prompts = [prompt_prefix + prompt + '\\nAnswer:' for prompt in prompts]\n",
    "\n",
    "# text_conditioning = [\"This list contains the top 50 accounts with the most followers on the photo and video-sharing social platform Instagram. As of July 2019, the most followed user is Instagram's own account, with over 308 million followers. Cristiano Ronaldo is the most followed individual, with over 177 million followers. Fifteen accounts have exceeded 100 million followers on the site.\",\n",
    "#                      \"During the American Revolution, the legal separation of the thirteen colonies from Great Britain in 1776 actually occurred on July 2, when the Second Continental Congress voted to approve a resolution of independence that had been proposed in June by Richard Henry Lee of Virginia declaring the United States independent from Great Britain's rule. After voting for independence, Congress turned its attention to the Declaration of Independence, a statement explaining this decision, which had been prepared by a Committee of Five, with Thomas Jefferson as its principal author. Congress debated and revised the wording of the Declaration, finally approving it two days later on July 4. A day earlier, John Adams had written to his wife Abigail\",\n",
    "#                      'The village was a primary location for the making of the film \\\"The Englishman Who Went Up a Hill But Came Down a Mountain\\\", which starred Hugh Grant. The hilltop scenes were filmed on the Gyrn, the long hill that overlooks the village. It was also featured in \\\"Monk\\'s Hood\\\", an episode of \\\"The Cadfael Chronicles\\\"',\n",
    "#                      'Valley of the Dolls is the first novel by American writer Jacqueline Susann. Published in 1966, the book was the biggest selling novel of its year. To date, it has sold more than 31 million copies, making it one of the best-selling works in publishing history.']\n",
    "\n",
    "# answers = ['Instagram','Great Britain',\"Llansilin in Powys\",[\"Judy Garland\", \"Carole Landis\", \"Dean Martin\", \"Ethel Merman\"]]\n",
    "\n",
    "n_passages = 4\n",
    "eval_data = '/lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_valid.jsonl'\n",
    "train_data = '/lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_train.jsonl'\n",
    "train_passage = []\n",
    "valid_passage = []\n",
    "with open(train_data, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == n_passages:\n",
    "            break\n",
    "        train_passage.append(json.loads(line)['text'].split('\\n\\n')[1])\n",
    "        \n",
    "with open(eval_data, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == n_passages:\n",
    "            break\n",
    "        valid_passage.append(json.loads(line)['text'].split('\\n\\n')[1])\n",
    "        \n",
    "tests = [{'w_embeds': True, 'temperature': 0 },  {'w_embeds': True, 'temperature': 0.7 }, {'w_embeds': False, 'temperature': 0.7 }]\n",
    "# print('Train passage:', train_passage)\n",
    "# print('Valid passage:', valid_passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditioning = ['Kyutai is a non-profit laboratory dedicated to open research in AI, founded in November 2023 by the iliad Group, CMA CGM and Schmidt Sciences. Launched with an initial team of six leading scientists, who have all worked with Big Tech labs in the USA, Kyutai continues to recruit at the highest level, and also offers internships to research Master’s degree students.']*4\n",
    "prompts = ['who are the founders of Kyutai?', 'when was Kyutai founded?', 'how many scientists were in the initial team?', 'what does Kyutai offer to research Master’s degree students?']\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "generated_sequence = pipeline.generate(prompts = prompts,\n",
    "                                      text_conditioning = conditioning,\n",
    "                                      temperature = 0.5, \n",
    "                                      max_tokens =200,\n",
    "                                      truncate_double_space = False)\n",
    "# random_flip, put the number of the token to flip. \n",
    "print(generated_sequence)\n",
    "\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "generated_sequence, logprobs = pipeline.generate(prompts = ['who has most followers on Instagram in world?'],\n",
    "                                      text_conditioning = [\"This list contains the top 50 accounts with the most followers on the photo and video-sharing social platform Instagram. As of July 2019, the most followed user is Instagram's own account, with over 308 million followers. Cristiano Ronaldo is the most followed individual, with over 177 million followers.\"],\n",
    "                                      temperature = 0.4, \n",
    "                                      max_tokens =200,\n",
    "                                      truncate_double_space = False)\n",
    "print(generated_sequence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuation\n",
    "for param in tests:\n",
    "    print('Param:', param)\n",
    "    if param['w_embeds']:\n",
    "        pipeline.pipeline_args.w_embeds = True\n",
    "    else:\n",
    "        pipeline.pipeline_args.w_embeds = False\n",
    "    \n",
    "    final_valid_prompts = [passage[100:].split(' ')[0] for passage in valid_passage][2] \n",
    "    text_valid_conditioning = [passage[:100] for passage in valid_passage][2]\n",
    "    print('Passage', text_valid_conditioning, ' | Truth', [passage[100:200] for passage in valid_passage][2] )\n",
    "    generated_sequence = pipeline.generate(prompts = final_valid_prompts, \n",
    "                                        text_conditioning = text_valid_conditioning, \n",
    "                                        temperature = param['temperature'], \n",
    "                                        max_tokens = max_tokens,\n",
    "                                        truncate_double_space = False)\n",
    "    print('Valid  word', generated_sequence)\n",
    "    \n",
    "    final_valid_prompts = ['' for passage in train_passage][2]\n",
    "    text_valid_conditioning = [passage[:100] for passage in valid_passage][2]\n",
    "    generated_sequence = pipeline.generate(prompts = final_valid_prompts, \n",
    "                                        text_conditioning = text_valid_conditioning, \n",
    "                                        temperature = param['temperature'], \n",
    "                                        max_tokens = max_tokens,\n",
    "                                        truncate_double_space = False)\n",
    "    print('Valid  empty', generated_sequence)\n",
    "\n",
    "    final_train_prompts =  [passage[100:].split(' ')[0] for passage in train_passage][1] \n",
    "    text_train_conditioning = [passage[:100] for passage in train_passage][1]\n",
    "    print('Passage', text_train_conditioning, ' | Truth', [passage[100:200] for passage in train_passage][1] )\n",
    "    generated_sequence = pipeline.generate(prompts = final_train_prompts, \n",
    "                                       text_conditioning = text_train_conditioning, \n",
    "                                       temperature = param['temperature'], \n",
    "                                       max_tokens = max_tokens,\n",
    "                                       truncate_double_space = False)\n",
    "    print('Train word', generated_sequence)\n",
    "    final_train_prompts = ['' for passage in train_passage][1] \n",
    "    text_train_conditioning = [passage[:100] for passage in train_passage][1]\n",
    "    generated_sequence = pipeline.generate(prompts = final_train_prompts, \n",
    "                                       text_conditioning = text_train_conditioning, \n",
    "                                       temperature = param['temperature'], \n",
    "                                       max_tokens = max_tokens,\n",
    "                                       truncate_double_space = False)\n",
    "    print('Train empty', generated_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tested\n",
    "# run_name = '128_SL_FN_False_0_MLP_False_CA_False_DB'\n",
    "# run_name = '128_SL_FN_False_0_MLP_True_CA_16_CAL_False_SKV_False_DB'\n",
    "# run_name = '128_SL_FN_False_0_MLP_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_0_MLP_True_CA_24_CAL_False_SKV_True_DB'\n",
    "\n",
    "# Finished runs:\n",
    "\n",
    "# run_name = '128_SL_FN_Truemean_1_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB' # 008500\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_4_TRUNC_True_CA_5_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_4_TRUNC_True_CA_24_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_1_MLP_4_TRUNC_True_CA_5_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_1_MLP_4_TRUNC_True_CA_24_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_0_MLP_4_TRUNC_True_CA_5_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_0_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_0_MLP_4_TRUNC_True_CA_24_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truelatent_attention_0_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truelatent_attention_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Trueeos_0_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Trueeos_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truereversed_latent_atttention_0_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truereversed_latent_attention_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_0_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_False_DB'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_False_DB'\n",
    "# run_name = '128_SL_FN_Truemean_0_MLP_8_TRUNC_True_CA_16_CAL_False_SKV_False_DB'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_8_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_False_DB'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB_dist_process'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_4_TRUNC_False_CA_False_DB'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_8_TRUNC_False_CA_False_DB'\n",
    "# run_name = '128_SL_FN_Truemean_0_MLP_4_TRUNC_False_CA_False_DB'\n",
    "# run_name = '128_SL_FN_Truemean_0_MLP_8_TRUNC_False_CA_False_DB'\n",
    "# run_name = '128_SL_FN_False_3_MLP_True_CA_5_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_3_MLP_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_3_MLP_True_CA_24_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_1_MLP_True_CA_5_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_1_MLP_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_1_MLP_True_CA_24_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_0_MLP_True_CA_5_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_3_MLP_True_CA_16_CAL_False_SKV_False_DB'\n",
    "# run_name = '128_SL_FN_False_3_MLP_False_CA_False_DB'\n",
    "# run_name = '128_SL_FN_False_3_MLP_True_CA_16_CAL_True_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_3_MLP_True_CA_16_CAL_False_SKV_True_DB_dist_process'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
