{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hippolytepilchen/micromamba/envs/llm_embed/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from embed_llm.models.augmented_model import EmbedAugPipeline\n",
    "from embed_llm.generation.evaluation import word_overlap, get_ppl, get_bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'w_embeds': True, 'norm_wo_embeds': False, 'mlp_project': {'hidden_dim': 4096, 'n_layers': 3, 'act': 'gelu', 'in_dim': 4096, 'out_dim': 4096}, 'training': True, 'param_dtype': 'float32', 'trainable_embedder': True, 'causal': False, 'pooling_module': {'type': 'mean', 'r': 512, 'n_heads': 8, 'n_layers': 1, 'n_truncated_layers': 4}, 'continuation': True, 'cross_att': True, 'start_cross_att': 27, 'do_pool': False}\n"
     ]
    }
   ],
   "source": [
    "llm_path = '/lustre/scwpod02/client/kyutai-interns/hippop/models/mistral_7B'\n",
    "#Must have a params json for pipeline\n",
    "\n",
    "# Finished runs:\n",
    "run_name = '128_SL_FN_False_0_MLP_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_0_MLP_False_CA_False_DB'\n",
    "# run_name = '128_SL_FN_False_0_MLP_True_CA_24_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_0_MLP_True_CA_16_CAL_False_SKV_False_DB'\n",
    "\n",
    "\n",
    "\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/' + run_name + '/checkpoints/checkpoint_005000'\n",
    "ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/' + run_name + '/checkpoints/checkpoint_010000'\n",
    "\n",
    "with open(f'{ckpt_path}/params.json') as f:\n",
    "    params = json.load(f)\n",
    "print(params)\n",
    "\n",
    "model_name = 'Mistral7B' \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "w_embeds = True\n",
    "max_batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:04<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cross att state dict\n",
      "Not only LoRA weights found in the checkpoint. Skipping other weights.\n",
      "Loading and merging LoRA weights...\n"
     ]
    }
   ],
   "source": [
    "pipeline: EmbedAugPipeline = EmbedAugPipeline.load_inference_model(llm_path = llm_path, \n",
    "                                                                   ckpt_path = ckpt_path, \n",
    "                                                                   device = device,\n",
    "                                                                   llm_name = model_name, \n",
    "                                                                   embed_model_name = 'NVEmbed', # Not used if pretrainde ckpt available\n",
    "                                                                    max_batch_size = max_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_passages = 20\n",
    "\n",
    "lim_chars = 300\n",
    "eval_data = '/lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_valid.jsonl'\n",
    "train_data = '/lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_train.jsonl'\n",
    "train_passage = []\n",
    "valid_passage = []\n",
    "\n",
    "with open(train_data, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == n_passages:\n",
    "            break\n",
    "        train_passage.append(json.loads(line)['text'].split('\\n\\n')[1][:lim_chars].split(' ')[:-1].join(' '))\n",
    "        \n",
    "with open(eval_data, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == n_passages:\n",
    "            break\n",
    "        valid_passage.append(json.loads(line)['text'].split('\\n\\n')[1][:lim_chars].split(' ')[:-1].join(' '))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flipping attempts\n",
    "\n",
    "temp = 0.5\n",
    "max_tokens = 128\n",
    "\n",
    "prompt = 'Mario'\n",
    "# prompt = ''\n",
    "\n",
    "i_token_to_flip = -1\n",
    "\n",
    "text_conditioning = 'Mario Bortolazzi (born 10 January 1965, in Verona) is an Italian professional football coach and a former player, who played as a midfielder. \\\n",
    "    \\n\\nHe played 12 seasons (241 games, 14 goals) in the Serie A for ACF Fiorentina, A.C. Milan, Hellas Verona F.C., Atalanta B.C. and Genoa C.F.C. \\\n",
    "        \\n\\nIn his coaching career he has so far has always been an assistant to his former Milan teammate Roberto Donadoni.\\\n",
    "            \\n\\nHonours\\n\\n - Milan\\n - Serie A champion: 1987–88.\\n\\n - Genoa\\n - Anglo-Italian Cup winner: 1995–96.'\n",
    "            \n",
    "generated_sequence, logprobs = pipeline.generate(prompts = prompt, \n",
    "                                    text_conditioning = text_conditioning, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False\n",
    "                                    random_flip = i_token_to_flip,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = [0, 0.5, 0.7, 1, 1.5]\n",
    "max_tokens = 150\n",
    "\n",
    "results_generation = {'0':{'train': {'word_prompt':{}, 'empty_prompt':{}}, 'valid': {'word_prompt':{}, 'empty_prompt':{}}}, \n",
    "                        '0.5':{'train': {'word_prompt':{}, 'empty_prompt':{}}, 'valid': {'word_prompt':{}, 'empty_prompt':{}}},\n",
    "                        '0.7':{'train': {'word_prompt':{}, 'empty_prompt':{}}, 'valid': {'word_prompt':{}, 'empty_prompt':{}}},\n",
    "                        '1':{'train': {'word_prompt':{}, 'empty_prompt':{}}, 'valid': {'word_prompt':{}, 'empty_prompt':{}}},\n",
    "                        '1.5':{'train': {'word_prompt':{}, 'empty_prompt':{}}, 'valid': {'word_prompt':{}, 'empty_prompt':{}}}}\n",
    "\n",
    "ground_truth_train_tokens = [pipeline.tokenizer.encode(text, bos = False, eos = True) for text in train_passage]\n",
    "ground_truth_valid_tokens = [pipeline.tokenizer.encode(text, bos = False, eos = True) for text in valid_passage]\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f'Temperature: {temp}')\n",
    "    print('Train Passage:')\n",
    "    \n",
    "    generated_sequences = []\n",
    "    pred_log_probs = []\n",
    "    for passage in train_passage[::max_batch_size]:\n",
    "        generated_sequence, logprobs = pipeline.generate(prompts = [text.split(' ')[0] for text in passage], \n",
    "                                    text_conditioning = passage, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False)\n",
    "           \n",
    "        generated_sequences.extend(generated_sequence)    \n",
    "        pred_log_probs.append(logprobs)\n",
    "    results_generation[str(temp)]['train']['word_prompt'] = {'seq':generated_sequences, 'logprobs':pred_log_probs}\n",
    "    print('Train Passage:', passage)\n",
    "    print('Train Generated:', generated_sequence)\n",
    "    generated_sequences = []\n",
    "    pred_log_probs = []\n",
    "    for passage in train_passage[::max_batch_size]:\n",
    "        generated_sequence, logprobs = pipeline.generate(prompts = [''] * len(passage), \n",
    "                                    text_conditioning = passage, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False)\n",
    "           \n",
    "        generated_sequences.extend(generated_sequence)    \n",
    "        pred_log_probs.append(logprobs)\n",
    "    results_generation[str(temp)]['train']['empty_prompt'] = {'seq':generated_sequences, 'logprobs':pred_log_probs}\n",
    "    \n",
    "\n",
    "    generated_sequences = []\n",
    "    pred_log_probs = []\n",
    "    for passage in valid_passage[::max_batch_size]:\n",
    "        generated_sequence, logprobs = pipeline.generate(prompts = [text.split(' ')[0] for text in passage], \n",
    "                                    text_conditioning = passage, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False)\n",
    "           \n",
    "        generated_sequences.extend(generated_sequence)    \n",
    "        pred_log_probs.append(logprobs)\n",
    "    results_generation[str(temp)]['valid']['word_prompt'] = {'seq':generated_sequences, 'logprobs':pred_log_probs}\n",
    "    \n",
    "    generated_sequences = []\n",
    "    pred_log_probs = []\n",
    "    for passage in valid_passage[::max_batch_size]:\n",
    "        generated_sequence, logprobs = pipeline.generate(prompts = [''] * len(passage), \n",
    "                                    text_conditioning = passage, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False)\n",
    "           \n",
    "        generated_sequences.extend(generated_sequence)    \n",
    "        pred_log_probs.append(logprobs)\n",
    "    results_generation[str(temp)]['valid']['empty_prompt'] = {'seq':generated_sequences, 'logprobs':pred_log_probs}\n",
    "    print('Valid Passage:', passage)\n",
    "    print('Valid Generated:', generated_sequence)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp in results_generation.keys():\n",
    "    for split in results_generation[temp].keys():\n",
    "        for prompt_type in results_generation[temp][split].keys():\n",
    "            generated_sequences = results_generation[temp][split][prompt_type]['seq']\n",
    "            pred_log_probs = results_generation[temp][split][prompt_type]['logprobs']\n",
    "            ppl = get_ppl(ground_truth_train_tokens if split == 'train' else ground_truth_valid_tokens, \n",
    "                          pred_log_probs)\n",
    "            overlap = word_overlap(train_passage if split == 'train' else valid_passage\n",
    "                                   ,generated_sequences)\n",
    "            print(f'Temperature: {temp}, Split: {split}, Prompt Type: {prompt_type}, PPL: {ppl}, Overlap: {overlap}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_path = '/lustre/scwpod02/client/kyutai-interns/hippop/models/mistral_7B'\n",
    "#Must have a params json for pipeline\n",
    "\n",
    "# No embeddings:\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/no_embed_bs16_lr5e-5Mistral7B88d0b42410aa4ec12025/checkpoints/checkpoint_002500'\n",
    "\n",
    "# Length tokens:\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_512t_Mistral7B20ed0018b2a84fba09c4/checkpoints/checkpoint_005000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_256t_Mistral7Be9ffc00fa42bedbc50d0/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_128t_Mistral7B226729d875c65b331ef8/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_64t_Mistral7B9bbea1b3b8dc23079b04/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_32t_Mistral7Bccbc3f29d69bd124c6cf/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_16t_Mistral7B7bc7dcc2ba28873eda96/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/mean_not_causal/checkpoints/checkpoint_007500'\n",
    "\n",
    "\n",
    "# # Continuation:\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/continuation_Mistral7B20ed0018b2a84fba09c4/checkpoints/checkpoint_006000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/mean_finetuned_notcausal_continuationMistral7B20ed0018b2a84fba09c4/checkpoints/checkpoint_005500'\n",
    "\n",
    "# # Cross-Attention:\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/cross_att_5_last_layersMistral7Bdbbb7faebb2f32cf20e9/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/cross_att_fine_tuned_embedder_5_last_layersMistral7Bdbbb7faebb2f32cf20e9/checkpoints/checkpoint_007500'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/cross_att_finetuned_notcausal_continuationMistral7B20ed0018b2a84fba09c4/checkpoints/checkpoint_005000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/cross_att_pretrained_continuationMistral7B20ed0018b2a84fba09c4/checkpoints/checkpoint_008500'\n",
    "\n",
    "with open(f'{ckpt_path}/params.json') as f:\n",
    "    params = json.load(f)\n",
    "print(params)\n",
    "\n",
    "model_name = 'Mistral7B' # Mistral7B, Llama3.2-3B, Gemma7B\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "w_embeds = True\n",
    "max_batch_size = 4\n",
    "\n",
    "# variant = '7b' if model_name == 'Gemma7B' else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify old params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'w_embeds': True, 'norm_wo_embeds': False, 'mlp_project': {'hidden_dim': 4096, 'n_layers': 3, 'act': 'gelu', 'in_dim': 4096, 'out_dim': 4096}, 'training': True, 'param_dtype': 'float32', 'trainable_embedder': True, 'causal': False, 'pooling_module': {'type': 'mean', 'r': 512, 'n_heads': 8, 'n_layers': 1}, 'continuation': True, 'cross_att': True, 'do_pool': False, 'n_truncated_layers': 4, 'normalize_embeddings': True, 'cross_att_layers': 5}\n",
      "here\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'start_cross_att'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhere\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormalize_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcross_att\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstart_cross_att\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_cross_att\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'start_cross_att'"
     ]
    }
   ],
   "source": [
    "with open(ckpt_path + '/params.json') as f:\n",
    "    params = json.load(f)\n",
    "print(params)\n",
    "# if 'do_pool'  not in params.keys():\n",
    "if 'n_truncated_layers' in params['pooling_module'].keys():\n",
    "    params['n_truncated_layers'] = params['pooling_module']['n_truncated_layers']\n",
    "    del params['pooling_module']['n_truncated_layers']\n",
    "    \n",
    "\n",
    "if params['cross_att'] is not None:\n",
    "    print('here')\n",
    "    params['normalize_embeddings'] = True if params['cross_att'] else False\n",
    "    if params['start_cross_att'] is None:\n",
    "        del params['start_cross_att']\n",
    "    else:\n",
    "        params['cross_att_layers'] = 32 - params[\"start_cross_att\"]\n",
    "        del params['start_cross_att']\n",
    "    params['do_pool'] = False if params['cross_att'] else True\n",
    "else:\n",
    "    params['do_pool'] = True\n",
    "print(params)\n",
    "with open(ckpt_path + '/params.json', 'w') as f:\n",
    "    json.dump(params, f)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param: {'w_embeds': True, 'temperature': 0}\n",
      "Prompt The  | Passage The Roman Republic (Repubblica Romana) was a sister republic of the First French Republic. It was pr\n",
      "Valid  word ['Roman Republic (Repubblica) was a short-lian state, a republican state, was a short-republican state, was a republican state, was a republican state, was a republican state, was a republican republic, was a republican republic, was a republican republic, was a republican republic, was a republican republic, was a republican republic, was a republican republic, was a republican republic, was a republican republic, republic, republic, republic, republic, republic republic, republic, republic republic, republic republic, republic republic, republic republic, republic republic, republic, republic republic republic, republic, republic republic, republic, republic, republic republic, republic, republic, republic, republic, republic, republic, republic, republic, republic, republic, republic, republic, republic, republic, republic, the Republic, republic, republic, republic, republic, republic, republic, republic, the Republic,, republic']\n",
      "Valid  empty ['The Roman Republic (Republica) was a short-lian state, established in 28 BC, was a short-lian state, established in 28 BC, was a short-Roman Republic, established in 28 BC, was a short-Roman Republic, established in 28 BC, was a republican state, established in 28 BC, was a republican state, established in 28 BC, was a republican state, established in the Republic in the Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, the Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic']\n",
      "Prompt Cochamó  | Passage Cochamó is a Chilean town and commune located in Llanquihue Province, Los Lagos Region. The capital \n",
      "Train word ['is administered by the commune council, which is part of the province of Los Lagos region.']\n",
      "Train empty ['The population is 1, which is the capital of the commune.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCho\\nCho\\nCho\\nCho\\nCho\\nCho\\nCho\\nCho\\nCho\\nCho\\nCho\\nCho\\nCho\\nCho\\nCho\\nChoCho\\nCho\\nCho\\nChoChoChochochochochochochochochochochochochochochocho.cho.o Chocho.o Cham.o Cham.o Cham.. Chocho.. Cho.........................................']\n",
      "Param: {'w_embeds': True, 'temperature': 0.7}\n",
      "Prompt The  | Passage The Roman Republic (Repubblica Romana) was a sister republic of the First French Republic. It was pr\n",
      "Valid  word ['Roman Republic was the first modern republic, a provisory (Latin: Res Publica) was the first modern republic, established in 150 BC, was a short-lasting from 153 BC. It was established in 150 BC. The first modern republic was established in 153 BC. It was the first modern republic, established in 150 BC. It was established in 150 BC. The republic was established in 177 the18171817) and the first republic of Rome in the Republic to the the to Republic, of republic the Republic was. that Republic was of Roman, to Republic the of the Republic of the the Republic the Rome was to.- Roman republican, in the Republic. to was that was the Republic of the- Republic, and the Roman Republic of the first republic and had. of Romanic, the Republic of the the,- the-']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m final_valid_prompts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m passage \u001b[38;5;129;01min\u001b[39;00m train_passage][\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     19\u001b[0m text_valid_conditioning \u001b[38;5;241m=\u001b[39m [passage[:\u001b[38;5;241m100\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m passage \u001b[38;5;129;01min\u001b[39;00m valid_passage][\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m---> 20\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfinal_valid_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mtext_conditioning\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtext_valid_conditioning\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mtruncate_double_space\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValid  empty\u001b[39m\u001b[38;5;124m'\u001b[39m, generated_sequence)\n\u001b[1;32m     27\u001b[0m final_train_prompts \u001b[38;5;241m=\u001b[39m  [passage\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m passage \u001b[38;5;129;01min\u001b[39;00m train_passage][\u001b[38;5;241m1\u001b[39m] \n",
      "File \u001b[0;32m~/micromamba/envs/llm_embed/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/embed_llm/embed_llm/models/augmented_model.py:594\u001b[0m, in \u001b[0;36mEmbedAugPipeline.generate_mistral\u001b[0;34m(self, prompts, text_conditioning, device, max_tokens, temperature, truncate_double_space)\u001b[0m\n\u001b[1;32m    590\u001b[0m encoded_prompts \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode(prompt, bos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, eos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts\n\u001b[1;32m    592\u001b[0m ]\n\u001b[1;32m    593\u001b[0m eos_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_id\n\u001b[0;32m--> 594\u001b[0m generated_tokens, logprobs \u001b[38;5;241m=\u001b[39m \u001b[43mmistral_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoded_prompts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoded_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkv_seqlens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_att\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkv_seqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnorm_wo_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_wo_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    605\u001b[0m produced_text \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    606\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode(generated_tokens[i])\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(generated_tokens))\n\u001b[1;32m    608\u001b[0m ]\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m truncate_double_space:\n",
      "File \u001b[0;32m~/micromamba/envs/llm_embed/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/embed_llm/embed_llm/models/mistral/generate.py:137\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(encoded_prompts, model, max_tokens, temperature, embeddings, chunk_size, eos_id, norm_wo_embeds, kv_seqlens)\u001b[0m\n\u001b[1;32m    134\u001b[0m generated_tensors\u001b[38;5;241m.\u001b[39mappend(next_token[:, \u001b[38;5;28;01mNone\u001b[39;00m])\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, Transformer):\n\u001b[0;32m--> 137\u001b[0m     last_token_prelogits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnext_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseqlens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm_wo_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_wo_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, CrossAttTransformer):\n\u001b[1;32m    145\u001b[0m     last_token_prelogits \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    146\u001b[0m         next_token,\n\u001b[1;32m    147\u001b[0m         seqlens\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m B,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         cache\u001b[38;5;241m=\u001b[39mcache,\n\u001b[1;32m    151\u001b[0m     )\n",
      "File \u001b[0;32m~/code/embed_llm/embed_llm/models/mistral/transformer.py:327\u001b[0m, in \u001b[0;36mTransformer.generate\u001b[0;34m(self, input_ids, seqlens, embeddings, cache, norm_wo_embeds)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    320\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;66;03m# images: list[torch.Tensor | None,\u001b[39;00m\n\u001b[1;32m    326\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 327\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_partial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm_wo_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_wo_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# , images=images)\u001b[39;00m\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline_rank \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_pipeline_ranks \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;66;03m# ignore the intermediate activations as we'll get the final output from\u001b[39;00m\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;66;03m# the last stage\u001b[39;00m\n\u001b[1;32m    337\u001b[0m         outs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(\n\u001b[1;32m    338\u001b[0m             h\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size, device\u001b[38;5;241m=\u001b[39mh\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mh\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    339\u001b[0m         )\n",
      "File \u001b[0;32m~/code/embed_llm/embed_llm/models/mistral/transformer.py:240\u001b[0m, in \u001b[0;36mTransformer.generate_partial\u001b[0;34m(self, input_ids, seqlens, embeddings, cache, norm_wo_embeds)\u001b[0m\n\u001b[1;32m    237\u001b[0m     seqlens \u001b[38;5;241m=\u001b[39m [size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m size \u001b[38;5;129;01min\u001b[39;00m seqlens]\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 240\u001b[0m     input_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mcache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_input_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseqlens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m     input_metadata \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    243\u001b[0m         SimpleInputMetadata\u001b[38;5;241m.\u001b[39mfrom_seqlens(seqlens, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers))\n\u001b[1;32m    245\u001b[0m     ]\n",
      "File \u001b[0;32m~/code/embed_llm/embed_llm/models/mistral/cache.py:271\u001b[0m, in \u001b[0;36mBufferCache.get_input_metadata\u001b[0;34m(self, seqlens)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(seqlens) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, seqlens\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cache_size \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_sizes:\n\u001b[0;32m--> 271\u001b[0m     metadata\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_input_metadata_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseqlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseqpos\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m metadata\n",
      "File \u001b[0;32m~/code/embed_llm/embed_llm/models/mistral/cache.py:320\u001b[0m, in \u001b[0;36mBufferCache._get_input_metadata_layer\u001b[0;34m(self, cache_size, seqlens, seqpos)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    313\u001b[0m     mask \u001b[38;5;241m=\u001b[39m BlockDiagonalCausalWithOffsetPaddedKeysMask\u001b[38;5;241m.\u001b[39mfrom_seqlens(\n\u001b[1;32m    314\u001b[0m         q_seqlen\u001b[38;5;241m=\u001b[39mseqlens,\n\u001b[1;32m    315\u001b[0m         kv_padding\u001b[38;5;241m=\u001b[39mcache_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m    319\u001b[0m     )\n\u001b[0;32m--> 320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCacheInputMetadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_cache_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mto_cache_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcached_elements\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached_elements\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_positions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_positions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mto_cache_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_prefill\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubsequent_prefill\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseqlens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:2\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, positions, to_cache_mask, cached_elements, cache_positions, prefill, mask, seqlens)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for param in tests:\n",
    "    print('Param:', param)\n",
    "    if param['w_embeds']:\n",
    "        pipeline.pipeline_args.w_embeds = True\n",
    "    else:\n",
    "        pipeline.pipeline_args.w_embeds = False\n",
    "    \n",
    "    final_valid_prompts = [passage.split(' ')[0] for passage in valid_passage][2] \n",
    "    text_valid_conditioning = [passage[:100] for passage in valid_passage][2]\n",
    "    print('Prompt', final_valid_prompts, ' | Passage', text_valid_conditioning)\n",
    "    generated_sequence = pipeline.generate(prompts = final_valid_prompts, \n",
    "                                        text_conditioning = text_valid_conditioning, \n",
    "                                        temperature = param['temperature'], \n",
    "                                        max_tokens = max_tokens,\n",
    "                                        truncate_double_space = False)\n",
    "    print('Valid  word', generated_sequence)\n",
    "    \n",
    "    final_valid_prompts = ['' for passage in train_passage][1]\n",
    "    text_valid_conditioning = [passage[:100] for passage in valid_passage][2]\n",
    "    generated_sequence = pipeline.generate(prompts = final_valid_prompts, \n",
    "                                        text_conditioning = text_valid_conditioning, \n",
    "                                        temperature = param['temperature'], \n",
    "                                        max_tokens = max_tokens,\n",
    "                                        truncate_double_space = False)\n",
    "    print('Valid  empty', generated_sequence)\n",
    "\n",
    "    final_train_prompts =  [passage.split(' ')[0] for passage in train_passage][1] \n",
    "    text_train_conditioning = [passage[:100] for passage in train_passage][1]\n",
    "    print('Prompt', final_train_prompts, ' | Passage', text_train_conditioning)\n",
    "    generated_sequence = pipeline.generate(prompts = final_train_prompts, \n",
    "                                       text_conditioning = text_train_conditioning, \n",
    "                                       temperature = param['temperature'], \n",
    "                                       max_tokens = max_tokens,\n",
    "                                       truncate_double_space = False)\n",
    "    print('Train word', generated_sequence)\n",
    "    final_train_prompts = ['' for passage in train_passage][1] \n",
    "    text_train_conditioning = [passage[:100] for passage in train_passage][1]\n",
    "    generated_sequence = pipeline.generate(prompts = final_train_prompts, \n",
    "                                       text_conditioning = text_train_conditioning, \n",
    "                                       temperature = param['temperature'], \n",
    "                                       max_tokens = max_tokens,\n",
    "                                       truncate_double_space = False)\n",
    "    print('Train empty', generated_sequence)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 information in the doc which enables to answer the question but not good response often in-context\n",
    "# 2 information in the doc which enables to answer the question and good response often in-context\n",
    "# 3 Hard negative passage\n",
    "# 4 Same\n",
    "\n",
    "# prompt_prefix = \"Query: who wrote the song photograph by ringo starr\\nAnswer: Ringo Starr\\n\\nQuery: who is playing the halftime show at super bowl 2016\\nAnswer: Coldplay\\n\\nQuery: where was the world economic forum held this year\\nAnswer: Davos\\n\\nQuery: where are the giant redwoods located in california\\nAnswer: Humboldt County\\n\\nQuery: who has made the most premier league appearances\\nAnswer: Gareth Barry\\n\\nQuery: \"\n",
    "# prompts = ['who has most followers on instagram in world','who did the united states win its independence from', 'locations for the film an englishman who went up a hill', 'who is the valley of the dolls based on']\n",
    "# final_prompts = [prompt_prefix + prompt + '\\nAnswer:' for prompt in prompts]\n",
    "\n",
    "# text_conditioning = [\"This list contains the top 50 accounts with the most followers on the photo and video-sharing social platform Instagram. As of July 2019, the most followed user is Instagram's own account, with over 308 million followers. Cristiano Ronaldo is the most followed individual, with over 177 million followers. Fifteen accounts have exceeded 100 million followers on the site.\",\n",
    "#                      \"During the American Revolution, the legal separation of the thirteen colonies from Great Britain in 1776 actually occurred on July 2, when the Second Continental Congress voted to approve a resolution of independence that had been proposed in June by Richard Henry Lee of Virginia declaring the United States independent from Great Britain's rule. After voting for independence, Congress turned its attention to the Declaration of Independence, a statement explaining this decision, which had been prepared by a Committee of Five, with Thomas Jefferson as its principal author. Congress debated and revised the wording of the Declaration, finally approving it two days later on July 4. A day earlier, John Adams had written to his wife Abigail\",\n",
    "#                      'The village was a primary location for the making of the film \\\"The Englishman Who Went Up a Hill But Came Down a Mountain\\\", which starred Hugh Grant. The hilltop scenes were filmed on the Gyrn, the long hill that overlooks the village. It was also featured in \\\"Monk\\'s Hood\\\", an episode of \\\"The Cadfael Chronicles\\\"',\n",
    "#                      'Valley of the Dolls is the first novel by American writer Jacqueline Susann. Published in 1966, the book was the biggest selling novel of its year. To date, it has sold more than 31 million copies, making it one of the best-selling works in publishing history.']\n",
    "\n",
    "# answers = ['Instagram','Great Britain',\"Llansilin in Powys\",[\"Judy Garland\", \"Carole Landis\", \"Dean Martin\", \"Ethel Merman\"]]\n",
    "\n",
    "n_passages = 4\n",
    "eval_data = '/lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_valid.jsonl'\n",
    "train_data = '/lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_train.jsonl'\n",
    "train_passage = []\n",
    "valid_passage = []\n",
    "with open(train_data, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == n_passages:\n",
    "            break\n",
    "        train_passage.append(json.loads(line)['text'].split('\\n\\n')[1])\n",
    "        \n",
    "with open(eval_data, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == n_passages:\n",
    "            break\n",
    "        valid_passage.append(json.loads(line)['text'].split('\\n\\n')[1])\n",
    "        \n",
    "tests = [{'w_embeds': True, 'temperature': 0 },  {'w_embeds': True, 'temperature': 0.7 }, {'w_embeds': False, 'temperature': 0.7 }]\n",
    "# print('Train passage:', train_passage)\n",
    "# print('Valid passage:', valid_passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditioning = ['Kyutai is a non-profit laboratory dedicated to open research in AI, founded in November 2023 by the iliad Group, CMA CGM and Schmidt Sciences. Launched with an initial team of six leading scientists, who have all worked with Big Tech labs in the USA, Kyutai continues to recruit at the highest level, and also offers internships to research Master’s degree students.']*4\n",
    "prompts = ['who are the founders of Kyutai?', 'when was Kyutai founded?', 'how many scientists were in the initial team?', 'what does Kyutai offer to research Master’s degree students?']\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "generated_sequence = pipeline.generate(prompts = prompts,\n",
    "                                      text_conditioning = conditioning,\n",
    "                                      temperature = 0.5, \n",
    "                                      max_tokens =200,\n",
    "                                      truncate_double_space = False)\n",
    "# random_flip, put the number of the token to flip. \n",
    "print(generated_sequence)\n",
    "\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "generated_sequence, logprobs = pipeline.generate(prompts = ['who has most followers on Instagram in world?'],\n",
    "                                      text_conditioning = [\"This list contains the top 50 accounts with the most followers on the photo and video-sharing social platform Instagram. As of July 2019, the most followed user is Instagram's own account, with over 308 million followers. Cristiano Ronaldo is the most followed individual, with over 177 million followers.\"],\n",
    "                                      temperature = 0.4, \n",
    "                                      max_tokens =200,\n",
    "                                      truncate_double_space = False)\n",
    "print(generated_sequence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param: {'w_embeds': True, 'temperature': 0}\n",
      "Passage The Roman Republic (Repubblica Romana) was a sister republic of the First French Republic. It was pr  | Truth oclaimed on 18 February 1798 after Louis-Alexandre Berthier, a general of Napoleon, had occupied the\n",
      "Valid  word ['the first Roman Republic in 509 BC, and the first Roman Republic was established. The Roman Republic was a period of great expansion for Rome.']\n",
      "Valid  empty ['# 1910 in Romanian literature\\n\\nThis article presents a list of the literary events and publications of Romania in 1910.']\n",
      "Passage Cochamó is a Chilean town and commune located in Llanquihue Province, Los Lagos Region. The capital   | Truth of the commune is the town of Río Puelo, which is named after the Puelo River.\n",
      "Train word ['the province of Chile is located in the Coquimbo Region, in the Limarí Province, in the Ovalle commune.']\n",
      "Train empty ['# 1999 Chilean local elections\\n\\nThe **1999 Chilean local elections** were held on 19 December 1999.']\n",
      "Param: {'w_embeds': True, 'temperature': 0.7}\n",
      "Passage The Roman Republic (Repubblica Romana) was a sister republic of the First French Republic. It was pr  | Truth oclaimed on 18 February 1798 after Louis-Alexandre Berthier, a general of Napoleon, had occupied the\n",
      "Valid  word ['as the first modern Roman republic, the Roman Republic was a political system in ancient Rome that lasted for nearly 500 years, from 509 BC to 27 BC.']\n",
      "Valid  empty ['## Details\\n\\nThe Roman Republic was one of the most powerful states of the ancient world, and its influence can be seen in the legal systems of modern nations.']\n",
      "Passage Cochamó is a Chilean town and commune located in Llanquihue Province, Los Lagos Region. The capital   | Truth of the commune is the town of Río Puelo, which is named after the Puelo River.\n",
      "Train word ['Cochamó is located in the south of Chile, in the commune of Cochamó, province of Llanquihue, in the Los Lagos Region.']\n",
      "Train empty ['## The New York Times has published a profile of the Chilean town of La Ligua, which has been chosen to host the 2018 edition of the Cocamelon festival.']\n",
      "Param: {'w_embeds': False, 'temperature': 0.7}\n",
      "Passage The Roman Republic (Repubblica Romana) was a sister republic of the First French Republic. It was pr  | Truth oclaimed on 18 February 1798 after Louis-Alexandre Berthier, a general of Napoleon, had occupied the\n",
      "Valid  word ['\"the first and only truly successful mass-market, mass-produced sports car\", the Porsche 911 was introduced in 1963 as the Porsche 901. It was a thoroughly modern sports car, with a flat-six engine mounted behind the rear axle and a rear-mounted transaxle, a layout that was unusual for the time. The car was an instant success, and Porsche sold over 100,000 911s in its first decade of production.\\n\\nIn 1964, Porsche changed the name of the 901 to 911, to avoid a potential conflict with French car maker Peugeot, which had a trademark on the name \"00\". The 911 was available in coupe and targa body styles, and a convertible was added in 1982. The 9']\n",
      "Valid  empty [\"## Your one-stop shop for all your building needs\\n\\nThe company was founded in 1947 by George H. Hosseini. Today, his son, George H. Hosseini Jr., is the CEO.\\n\\nThe company started out as a small lumberyard in the heart of downtown Los Angeles. It was a family-owned business that focused on providing building materials for residential and commercial construction projects.\\n\\nThe company's first major project was the construction of the Los Angeles Memorial Coliseum. It was the first time that the company had worked on such a large project.\\n\\nOver the years, the company has grown and expanded. It now has 15 locations in Southern California and Nevada.\\n\\nThe company's headquarters are in downtown Los Angeles.\\n\\nProducts and services\\n\\nThe company provides a wide range of building materials and services.\\n\\nProducts: \\n - Lumber\\n - Ply\"]\n",
      "Passage Cochamó is a Chilean town and commune located in Llanquihue Province, Los Lagos Region. The capital   | Truth of the commune is the town of Río Puelo, which is named after the Puelo River.\n",
      "Train word [\"the year 2006.\\n\\nEarly life\\n\\nBorn in Gauteng, South Africa, he was educated at St. Stithians College. He was an all-rounder at the school and was selected to represent the school's 1st XI cricket team. He also represented the school's 1st XV rugby team. He was also a member of the school's athletics team.\\n\\nEarly career\\n\\nUpon leaving school, he studied at the University of Cape Town. He also played for the university's cricket team. He was a member of the 1st XI team which won the 1994–95 University Sports South African University Championship. He was also a member of the 1st XV rugby team.\\n\\nIn 1995, he moved to the United Kingdom. He played club cricket for the Kent Cricket Club. He also played for the club's\"]\n",
      "Train empty ['## Latest revision as of 16:47, 13 September 2012\\n\\nThe first documented use of the name \"Meadowbrook\" was in 1906, when it was used as the name of a planned subdivision in the northwest corner of the city. This name was used for the area for many years, but in 1922, the name \"Meadowbrook\" was officially adopted for the entire area.\\n\\nThe first major development in Meadowbrook was the construction of a streetcar line in 1908. The line was constructed by the Seattle and Lake Washington Railway and Navigation Company, which was the predecessor of the Seattle, Lake Washington and Northern Railway. The line ran from downtown Seattle along the north shore of Lake Washington to the area of Meadowbrook. It was originally named the \"Lake Washington Line\", but was later renamed']\n"
     ]
    }
   ],
   "source": [
    "# Continuation\n",
    "for param in tests:\n",
    "    print('Param:', param)\n",
    "    if param['w_embeds']:\n",
    "        pipeline.pipeline_args.w_embeds = True\n",
    "    else:\n",
    "        pipeline.pipeline_args.w_embeds = False\n",
    "    \n",
    "    final_valid_prompts = [passage[100:].split(' ')[0] for passage in valid_passage][2] \n",
    "    text_valid_conditioning = [passage[:100] for passage in valid_passage][2]\n",
    "    print('Passage', text_valid_conditioning, ' | Truth', [passage[100:200] for passage in valid_passage][2] )\n",
    "    generated_sequence = pipeline.generate(prompts = final_valid_prompts, \n",
    "                                        text_conditioning = text_valid_conditioning, \n",
    "                                        temperature = param['temperature'], \n",
    "                                        max_tokens = max_tokens,\n",
    "                                        truncate_double_space = False)\n",
    "    print('Valid  word', generated_sequence)\n",
    "    \n",
    "    final_valid_prompts = ['' for passage in train_passage][2]\n",
    "    text_valid_conditioning = [passage[:100] for passage in valid_passage][2]\n",
    "    generated_sequence = pipeline.generate(prompts = final_valid_prompts, \n",
    "                                        text_conditioning = text_valid_conditioning, \n",
    "                                        temperature = param['temperature'], \n",
    "                                        max_tokens = max_tokens,\n",
    "                                        truncate_double_space = False)\n",
    "    print('Valid  empty', generated_sequence)\n",
    "\n",
    "    final_train_prompts =  [passage[100:].split(' ')[0] for passage in train_passage][1] \n",
    "    text_train_conditioning = [passage[:100] for passage in train_passage][1]\n",
    "    print('Passage', text_train_conditioning, ' | Truth', [passage[100:200] for passage in train_passage][1] )\n",
    "    generated_sequence = pipeline.generate(prompts = final_train_prompts, \n",
    "                                       text_conditioning = text_train_conditioning, \n",
    "                                       temperature = param['temperature'], \n",
    "                                       max_tokens = max_tokens,\n",
    "                                       truncate_double_space = False)\n",
    "    print('Train word', generated_sequence)\n",
    "    final_train_prompts = ['' for passage in train_passage][1] \n",
    "    text_train_conditioning = [passage[:100] for passage in train_passage][1]\n",
    "    generated_sequence = pipeline.generate(prompts = final_train_prompts, \n",
    "                                       text_conditioning = text_train_conditioning, \n",
    "                                       temperature = param['temperature'], \n",
    "                                       max_tokens = max_tokens,\n",
    "                                       truncate_double_space = False)\n",
    "    print('Train empty', generated_sequence)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
