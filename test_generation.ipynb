{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hippolytepilchen/micromamba/envs/llm_embed/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "import torch\n",
    "\n",
    "from embed_llm.generation.evaluation import (\n",
    "    create_prompt,\n",
    "    create_prompt_prefix,\n",
    "    ensure_reproducibility,\n",
    ")\n",
    "from embed_llm.generation.metrics import (\n",
    "    get_approx_em,\n",
    ")\n",
    "from embed_llm.models.augmented_model import EmbedAugPipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_reproducibility(29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 for loading\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Must have a params json for pipeline\n",
    "llm_path = '/lustre/scwpod02/client/kyutai-interns/hippop/models/mistral_7B'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "run_name = 'Compr2_L16_Cont_MTAConv_2Norm'\n",
    "# run_name = None\n",
    "\n",
    "last_ckpt = '010000' # '008500' #'010000' \n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    device = torch.device('cuda:0')\n",
    "    print(f'Using {device} for loading')\n",
    "\n",
    "w_embeds = True\n",
    "max_batch_size = 4\n",
    "instruct_ckpt = None\n",
    "# instruct_ckpt = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/TrainEmbed_CA_Cont_Distill_Instruct/checkpoints/checkpoint_010000'\n",
    "# instruct_ckpt =\"/lustre/scwpod02/client/kyutai-interns/hippop/tmp/ToyDecompressingTests_LLM_FT_MaxEmb_true_reversed/checkpoints/checkpoint_020000\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If n_layers is 1, hidden_dim must be equal to out_dim,                 \n",
      " but hidden_dim is not equal to out_dim so hidden_dim is set to out_dim\n",
      "Loading MLP projector\n"
     ]
    }
   ],
   "source": [
    "pipeline: EmbedAugPipeline = EmbedAugPipeline.load_inference_model(\n",
    "    llm_path=llm_path,\n",
    "    ckpt_path= None if run_name is None else (\"/lustre/scwpod02/client/kyutai-interns/hippop/tmp/\"\n",
    "    + run_name\n",
    "    + \"/checkpoints/checkpoint_\"\n",
    "    + last_ckpt),\n",
    "    device=device,\n",
    "    llm_name=\"Mistral7B\",\n",
    "    embed_model_name=\"NVEmbed\",  # Not used if pretrainde ckpt available\n",
    "    max_batch_size=max_batch_size,\n",
    "    instruct_ckpt=instruct_ckpt,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QA tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = \"/lustre/scwpod02/client/kyutai-interns/hippop/processed_data/eval_ReadComp/squad_test.jsonl\"\n",
    "# eval_data = '/lustre/scwpod02/client/kyutai-interns/hippop/processed_data/eval_QA_NVEmbed/nq_open_data.jsonl' # nq_data.jsonl\n",
    "context = []\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "\n",
    "with open(eval_data, \"r\") as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                questions.append(data[\"question\"].strip())\n",
    "\n",
    "                if isinstance(data[\"answer\"], str):\n",
    "                    answers.append([data[\"answer\"].strip()])\n",
    "\n",
    "                elif isinstance(data[\"answer\"], list):\n",
    "                    answers.append(data[\"answer\"])\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid answer type\")\n",
    "                # Take the first ranked retrieved passage\n",
    "                context.append(data[\"passages\"][:5])\n",
    "\n",
    "c = list(zip(questions, context, answers))\n",
    "fixed_random = random.Random(0.3)\n",
    "fixed_random.shuffle(c)\n",
    "questions, context, answers = zip(*c)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pipeline.model.mlp_project)\n",
    "# print('MLP',pipeline.model.mlp_project)\n",
    "# print('MLP stats', 'Layer 1', torch.mean(pipeline.model.mlp_project.layers[0].layer1.weight),\n",
    "#       'Layer 2', torch.mean(pipeline.model.mlp_project.layers[0].layer2.weight))\n",
    "# print('MLP stats', 'Layer 1', torch.std(pipeline.model.mlp_project.layers[0].layer1.weight),\n",
    "#       'Layer 2', torch.std(pipeline.model.mlp_project.layers[0].layer2.weight))\n",
    "\n",
    "# import math \n",
    "\n",
    "# pipeline.model.mlp_project.layers[0].layer2.weight = torch.nn.Parameter(-pipeline.model.mlp_project.layers[0].layer1.weight)\n",
    "\n",
    "# torch.nn.init.constant_(pipeline.model.mlp_project.layers[0].rms_norm.weight, 0.6)\n",
    "# torch.nn.init.kaiming_uniform_(pipeline.model.mlp_project.layers[0].layer1.weight, a=math.sqrt(5))Â£\n",
    "# torch.nn.init.kaiming_uniform_(pipeline.model.mlp_project.layers[0].layer2.weight, a=math.sqrt(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATION\n",
      "Given Query: What could someone be investigated for?\n",
      "GT Answer: ['allegations of professional misconduct', 'professional misconduct', 'professional misconduct']\n",
      "Context Given [\"The functions of the teacher's colleges may include setting out clear standards of practice, providing for the ongoing education of teachers, investigating complaints involving members, conducting hearings into allegations of professional misconduct and taking appropriate disciplinary action and accrediting teacher education programs. In many situations teachers in publicly funded schools must be members in good standing with the college, and private schools may also require their teachers to be college peoples. In other areas these roles may belong to the State Board of Education, the Superintendent of Public Instruction, the State Education Agency or other governmental bodies. In still other areas Teaching Unions may be responsible for some or all of these duties.\"][...]\n",
      "Answer in context? 1 \n",
      "Prediction: Driving while intoxicated \n",
      "\n",
      "\n",
      "Given Query: What type of algorithm is trial division?\n",
      "GT Answer: ['deterministic', 'deterministic algorithm', 'deterministic', 'deterministic', 'deterministic']\n",
      "Context Given ['Modern primality tests for general numbers n can be divided into two main classes, probabilistic (or \"Monte Carlo\") and deterministic algorithms. Deterministic algorithms provide a way to tell for sure whether a given number is prime or not. For example, trial division is a deterministic algorithm because, if performed correctly, it will always identify a prime number as prime and a composite number as composite. Probabilistic algorithms are normally faster, but do not completely prove that a number is prime. These tests rely on testing a given number in a partly random way. For example, a given test might pass all the time if applied to a prime number, but pass only with probability p if applied to a composite number. If we repeat the test n times and pass every time, then the probability that our number is composite is 1/(1-p)n, which decreases exponentially with the number of tests, so we can be as sure as we like (though never perfectly sure) that the number is prime. On the other hand, if the test ever fails, then we know that the number is composite.'][...]\n",
      "Answer in context? 1 \n",
      "Prediction: Primality test \n",
      "\n",
      "\n",
      "Given Query: Where did Maududi's books place Islam?\n",
      "GT Answer: ['in a modern context', 'a modern context', 'modern context']\n",
      "Context Given ['Sayyid Abul Ala Maududi was an important early twentieth-century figure in the Islamic revival in India, and then after independence from Britain, in Pakistan. Trained as a lawyer he chose the profession of journalism, and wrote about contemporary issues and most importantly about Islam and Islamic law. Maududi founded the Jamaat-e-Islami party in 1941 and remained its leader until 1972. However, Maududi had much more impact through his writing than through his political organising. His extremely influential books (translated into many languages) placed Islam in a modern context, and influenced not only conservative ulema but liberal modernizer Islamists such as al-Faruqi, whose \"Islamization of Knowledge\" carried forward some of Maududi\\'s key principles.'][...]\n",
      "Answer in context? 1 \n",
      "Prediction: First \n",
      "\n",
      "\n",
      "Given Query: Were the tapes able to be restored and processed without destroying historical legitimacy or did some aspects of the tapes lose legitimacy?\n",
      "GT Answer: ['without destroying historical legitimacy', 'Lowry Digital f', 'without destroying historical legitimacy', 'processed to remove random noise and camera shake without destroying historical legitimacy', 'without destroying historical legitimacy']\n",
      "Context Given ['With a budget of $230,000, the surviving original lunar broadcast data from Apollo 11 was compiled by Nafzger and assigned to Lowry Digital for restoration. The video was processed to remove random noise and camera shake without destroying historical legitimacy. The images were from tapes in Australia, the CBS News archive, and kinescope recordings made at Johnson Space Center. The restored video, remaining in black and white, contains conservative digital enhancements and did not include sound quality improvements.'][...]\n",
      "Answer in context? 1 \n",
      "Prediction: The tapes were able to be restored and processed without destroying historical legitimacy or did some aspects of the tapes lose legitimacy. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w_embeds = True\n",
    "temp = 0\n",
    "max_tokens = 64\n",
    "max_embeds = 1\n",
    "icl_examples = 3\n",
    "compressed_doc_in_icl = False\n",
    "icl_w_document = True\n",
    "query_w_doc = False\n",
    "max_bs = 4\n",
    "rag = False\n",
    "\n",
    "device_count = torch.cuda.device_count()\n",
    "other_device = device if device_count <= 1 else torch.device(\"cuda:1\")\n",
    "\n",
    "# temp = [temp] * max_tokens   \n",
    "# if max_tokens > i_token_to_flip >= 0:\n",
    "#     temp[i_token_to_flip] = 1000\n",
    "\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "    \n",
    "prompt_str, to_embed_str = create_prompt_prefix(\n",
    "    queries=questions,\n",
    "    answers=[answer[0] for answer in answers],\n",
    "    docs=None if not icl_w_document else [\"\\n\".join(cont[:max_embeds]) for cont in context],\n",
    "    max_examples=icl_examples,\n",
    "    compressed_doc_in_icl=compressed_doc_in_icl,\n",
    ")\n",
    "\n",
    "generated_sequences = []\n",
    "\n",
    "queries = list(questions[icl_examples:])\n",
    "docs = list(context[icl_examples:])\n",
    "truths = list(answers[icl_examples:])\n",
    "queries.reverse()\n",
    "docs.reverse()\n",
    "truths.reverse()\n",
    "texts_to_embed = []\n",
    "batch_list_prompts = []\n",
    "for query, doc in zip(queries[:max_bs], docs[:max_bs]):\n",
    "    batch_list_prompt, text_to_embed = create_prompt(\n",
    "        prefix_prompt=prompt_str,\n",
    "        prefix_embed=to_embed_str,\n",
    "        doc=\"\\n\".join(doc[:max_embeds]),\n",
    "        w_embeds=w_embeds,\n",
    "        query=query,\n",
    "        wdoc=query_w_doc\n",
    "        )\n",
    "    batch_list_prompts.append(batch_list_prompt)\n",
    "    texts_to_embed.append(text_to_embed)\n",
    "\n",
    "# for i, cont in enumerate(context_prompt if pipeline.pipeline_args.w_prefix_prompt else no_context_prompt):\n",
    "#     print(f'{i}',cont)\n",
    "\n",
    "\n",
    "    # print(no_context_prompt[i] if not pipeline.pipeline_args.w_prefix_prompt else context_prompt[i])\n",
    "    # print(f'Ground truth answer: {a}\\n')\n",
    "\n",
    "generated_sequence = pipeline.generate(\n",
    "    text_to_embed=texts_to_embed,\n",
    "    batch_list_prompts=batch_list_prompts,\n",
    "    temperature=temp,\n",
    "    max_tokens=max_tokens,\n",
    "    truncate_line=True,\n",
    "    device=device,\n",
    "    device_generation=other_device,\n",
    ")\n",
    "\n",
    "print('GENERATION')\n",
    "for i, (q, a, d, g) in enumerate(zip(queries[:max_bs], truths[:max_bs], docs[:max_bs], generated_sequence)):\n",
    "    print(f'Given Query: {q}\\nGT Answer: {a}\\nContext Given {d[:max_embeds]}[...]\\nAnswer in context? {get_approx_em(d,a[0])}','\\nPrediction:', g, '\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditioning = ['Kyutai is a non-profit laboratory dedicated to open research in AI, founded in November 2023 by the iliad Group, CMA CGM and Schmidt Sciences. Launched with an initial team of six leading scientists, who have all worked with Big Tech labs in the USA, Kyutai continues to recruit at the highest level, and also offers internships to research Masterâs degree students.']*4\n",
    "# prompts = ['who are the founders of Kyutai?', 'when was Kyutai founded?', 'how many scientists were in the initial team?', 'what does Kyutai offer to research Masterâs degree students?']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
