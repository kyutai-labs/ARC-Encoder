{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hippolytepilchen/micromamba/envs/llm_embed/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from embed_llm.models.augmented_model import EmbedAugPipeline\n",
    "from embed_llm.generation.evaluation import  ensure_reproducibility\n",
    "from bertviz import head_view, model_view # type: ignore\n",
    "from embed_llm.models.mistral.generate import get_attention\n",
    "from embed_llm.generation.metrics import (\n",
    "    word_overlap,\n",
    "    get_bleu_score,\n",
    "    get_meteor,\n",
    "    get_em,\n",
    "    get_f1_score,\n",
    "    metric_max_over_ground_truths,\n",
    "    get_approx_em\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from embed_llm.generation.evaluation import (create_prompt, create_prompt_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_reproducibility(29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 for loading\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Must have a params json for pipeline\n",
    "llm_path = '/lustre/scwpod02/client/kyutai-interns/hippop/models/mistral_7B'\n",
    "\n",
    "\n",
    "\n",
    "# run_name = 'ToyPretraining_LLM_False_Emb_False_MaxEmb_1_0.2cont_0alpha_16BS_tmp' # Best one embed \n",
    "# run_name = 'ToyInstruct_LLM_False_Emb_False_MaxEmb_3_alpha_2'\n",
    "# run_name = 'ToyPretraining_LLM_False_Emb_False_MaxEmb_3_fullcont_16BS_alternativeCA'\n",
    "# run_name = 'ToyPretraining_LLM_False_Emb_False_MaxEmb_3_fullcont_16BS'\n",
    "# run_name = 'ToyPretraining_LLM_False_Emb_False_MaxEmb_3_fullcont_16BS_beginCA'\n",
    "# run_name = 'DistillTraining_mid_MaxEmb_3_50cont_0alpha_1tmp'\n",
    "# run_name = 'ToyPretraining_LLM_False_Emb_False_MaxEmb_3_fullrec_16BS'\n",
    "# run_name = None\n",
    "\n",
    "last_ckpt = '030000' # '008500' #'010000' \n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    device = torch.device('cuda:0')\n",
    "    print(f'Using {device} for loading')\n",
    "\n",
    "w_embeds = True\n",
    "max_batch_size = 4\n",
    "instruct_ckpt = None\n",
    "# instruct_ckpt = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/Instruct_mid_1embeds_alpha0_lowlr_newdata/checkpoints/checkpoint_010000'\n",
    "# instruct_ckpt =\"/lustre/scwpod02/client/kyutai-interns/hippop/tmp/ToyDecompressingTests_LLM_FT_MaxEmb_true_reversed/checkpoints/checkpoint_020000\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:04<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If n_layers is 1, hidden_dim must be equal to out_dim, \n",
      " but hidden_dim is not equal to out_dim so hidden_dim is set to out_dim\n",
      "Loading MLP projector\n"
     ]
    }
   ],
   "source": [
    "pipeline: EmbedAugPipeline = EmbedAugPipeline.load_inference_model(\n",
    "    llm_path=llm_path,\n",
    "    ckpt_path= None if run_name is None else (\"/lustre/scwpod02/client/kyutai-interns/hippop/tmp/\"\n",
    "    + run_name\n",
    "    + \"/checkpoints/checkpoint_\"\n",
    "    + last_ckpt),\n",
    "    device=device,\n",
    "    llm_name=\"Mistral7B\",\n",
    "    embed_model_name=\"NVEmbed\",  # Not used if pretrainde ckpt available\n",
    "    max_batch_size=max_batch_size,\n",
    "    instruct_ckpt=instruct_ckpt,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QA tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = '/lustre/scwpod02/client/kyutai-interns/hippop/processed_data/eval_QA_NVEmbed/nq_open_data.jsonl' # nq_data.jsonl\n",
    "context = []\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "\n",
    "with open(eval_data, \"r\") as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                questions.append(data[\"question\"].strip())\n",
    "\n",
    "                if isinstance(data[\"answer\"], str):\n",
    "                    answers.append([data[\"answer\"].strip()])\n",
    "\n",
    "                elif isinstance(data[\"answer\"], list):\n",
    "                    answers.append(data[\"answer\"])\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid answer type\")\n",
    "                # Take the first ranked retrieved passage\n",
    "                context.append(data[\"passages\"][:5])\n",
    "\n",
    "c = list(zip(questions, context, answers))\n",
    "fixed_random = random.Random(0.2)\n",
    "fixed_random.shuffle(c)\n",
    "questions, context, answers = zip(*c)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Prefix:\n",
      " Question: three act puccini opera first performed in 1900\n",
      "Answer: Tosca\n",
      "\n",
      "Question: where is urinary bladder located in human body\n",
      "Answer: on the pelvic floor\n",
      "\n",
      "Question: term limits for house of representatives and senate\n",
      "Answer: Unlimited six-year terms\n",
      "\n",
      "Question: who played stonewall jackson in gods and generals\n",
      "Answer: Stephen Lang\n",
      "\n",
      "Question: who played morticia in the addams family tv show\n",
      "Answer: Carolyn Sue Jones\n",
      "\n",
      "\n",
      "GENERATION\n",
      "Given Query: fast and furious 7 red car abu dhabi\n",
      "GT Answer: ['The Lykan Hypersport']\n",
      "Context Given [\"July 2019, Abu Dhabi allocated $163 million to finance global entertainment partners as part of its plan to diversify the economy and wean it off oil. Many Hollywood and other national film production teams have used parts of the UAE as filming locations. Neighboring Dubai gets a lot of attention, but in recent years Abu Dhabi has become a popular destination. The Etihad Towers and Emirates Palace Hotel were some of the city's landmarks used as filming locations for the movie Furious 7, in which cars rush through the building and smashed through the windows of the towers. In 2018,\", \"to appear in the film Furious 7, one of which is available for public viewing at the W Motors Gallery in Dubai. It is the most expensive car ever featured in the Fast and the Furious films, though the vehicles seen on screen were driveable stunt models rather than production vehicles. The Lykan HyperSport's first pre-production model debuted at the Dubai International Motor Show on 5 October 2013. Other venues where the Lykan HyperSport was displayed include the Dubai International Boat Show in the United Arab Emirates; the Historical, Vintage, and Classical Cars Museum in Kuwait; and Cohen&Cunil in Marbella, Spain.\", 'Portrayed in Furious 7, many action scenes are filmed here.', 'After wrapping up the Mumbai schedule, the filming team left for Abu Dhabi. The Abu Dhabi schedule of shooting was intense and action-packed. In Abu Dhabi, filming started early May, and took place at various locations in Abu Dhabi including the Corniche, Liwa oasis, Hyatt Capital Gate Hotel, Qasr al Sarab, Emirates Palace and Yas Island. In one of the chase sequences in the film, 120 cars were involved. The film also had used Formula 1 cars. Even after the brain surgery, all the stunts were performed by Roshan himself. The stunts were designed by Andy Armstrong and shot in Abu Dhabi. The Abu Dhabi', \"A Lykan HyperSport was featured in the film Fast and Furious 7. The film's car coordinator Dennis McCarthy explained in an interview that the Lykan HyperSports used in the film were not production models but purpose-built by W Motors for the film using the same moulds, but cheaper material (fibreglass instead of carbon fibre) and a simpler chassis. Of the ten produced for the film, one was returned to W Motors and is displayed in their showroom. The other nine were destroyed during the course of filming. Several Lykan HyperSport replicas were also used in the 2018 British Fast & Furious Live show. At least one has since been sold and was imported to the United States by Sam Hard (Hard Up Garage) and Ed Bolian (VINwiki). It will be built into a driveable car using an extensively modified Porsche Boxster chassis by Casey Putsch and Genius Garage under license from W Motors. The build is being documented on YouTube.\"][...]\n",
      "Answer in context? 1 \n",
      "Prediction: Lykan Hypersport \n",
      "\n",
      "\n",
      "Given Query: where is arachidonic acid found in the body\n",
      "GT Answer: ['brain', 'muscles', 'liver']\n",
      "Context Given [\"Arachidonic acid (arachidonic's) has 20 carbons, is present in animal visceral fat (brain, liver, kidney, lung, spleen), and is a 5,8,11,14-tetra-unsaturated fatty acid. I caused by the decomposition of cell membrane in the phospholipid. Prostaglandin, and important as starting materials for the thromboxane, leukotriene such as are known as a series of metabolic pathway to give eicosanoids, arachidonic acid cascade are compounds. C19H31CO2H, IUPAC organization name (5Z , 8Z , 11  Z , 14Z)-icosa-5,8,11,14-tetraenoic acid, numerical representation 20: 4 (5,8,11,14), n-6, molecular weight 304.47, boiling point 169- 171 °C. CAS Registry Number 506-32-1.\", \"Arachidonic acid is a polyunsaturated fatty acid present in the phospholipids (especially phosphatidylethanolamine, phosphatidylcholine, and phosphatidylinositides) of membranes of the body's cells, and is abundant in the brain, muscles, and liver. Skeletal muscle is an especially active site of arachidonic acid retention, accounting for roughly 10-20% of the phospholipid fatty acid content typically. In addition to being involved in cellular signaling as a lipid second messenger involved in the regulation of signaling enzymes, such as PLC-γ, PLC-δ, and PKC-α, -β, and -γ isoforms, arachidonic acid is a key inflammatory intermediate and can also act as a vasodilator. (Note separate synthetic pathways, as described in section below.)\", 'Arachidonic acid (AA, sometimes ARA) is a polyunsaturated omega-6 fatty acid 20:4(ω-6), or 20:4(5,8,11,14). It is structurally related to the saturated arachidic acid found in cupuaçu butter. Its name derives from the New Latin word arachis (peanut), but it is important to note that peanut oil does not contain any arachidonic acid.', 'acids (EETs) by epoxygenase. Arachidonic acid is freed from phospholipid by hydrolysis, catalyzed by the phospholipase A2 (PLA2). Arachidonic acid for signaling purposes appears to be derived by the action of group IVA cytosolic phospholipase A2 (cPLA2, 85 kDa), whereas inflammatory arachidonic acid is generated by the action of a low-molecular-weight secretory PLA2 (sPLA2, 14-18 kDa). Arachidonic acid is a precursor to a wide range of eicosanoids: The production of these derivatives and their actions in the body are collectively known as the \"arachidonic acid cascade\"; see essential fatty acid interactions and the enzyme and metabolite linkages given in the previous paragraph for more details.', 'Arachidonic acid is marketed as an anabolic bodybuilding supplement in a variety of products. Supplementation of arachidonic acid (1,500 mg/day for eight weeks) has been shown to increase lean body mass, strength, and anaerobic power in experienced resistance-trained men. This was demonstrated in a placebo-controlled study at the University of Tampa. Thirty men (aged 20.4 ± 2.1 years) took arachidonic acid or a placebo for eight weeks, and participated in a controlled resistance-training program. After eight weeks, lean body mass (LBM) had increased significantly, and to a greater extent, in the AA group (1.62 kg) vs. placebo (0.09 kg) (p<0.05). The change'][...]\n",
      "Answer in context? 1 \n",
      "Prediction: in the cell membrane \n",
      "\n",
      "\n",
      "Given Query: who wrote cant get you out of my head lyrics\n",
      "GT Answer: ['Cathy Dennis and Rob Davis', 'Rob Davis', 'Cathy Dennis']\n",
      "Context Given ['\"Can\\'t Get You Out of My Head\" is a song that was recorded by Australian singer Kylie Minogue for her eighth studio album Fever (2001). Parlophone Records released the song as the album\\'s lead single on 8 September 2001. \"Can\\'t Get You Out of My Head\", which was written and produced by Cathy Dennis and Rob Davis, is a dance-pop, techno-pop and neo-disco song that is known for its \"la la la\" hook. Its lyrics are about obsession with a love interest. Music critics praised the song\\'s production and Minogue\\'s vocals and labelled it a highlight of Fever. The song reached number one on charts in 40 countries worldwide. It peaked at number one on the UK Singles Chart for four weeks and was certified two-times platinum by the British Phonographic Industry (BPI). It also topped the', 'In 2000, British singer-songwriter Cathy Dennis and English songwriter Rob Davis had been brought together by Universal Publishing to work on new music. The session for \"Can\\'t Get You Out of My Head\" began with Davis generating a 125 bpm drum loop using the computer program Cubase. Dennis improvised with the line \"I just can\\'t get you out of my head\", which later became the song\\'s lyric. After three and a half hours, Davis and Dennis had recorded the demo for \"Can\\'t Get You Out of My Head\" and the vocals were recorded the same day; the pair said the recording process was \"very natural and fluid\", and did not rely on heavy instrumentation. Prior', '\"Can\\'t Get You Out of My Head\" is three minutes and fifty seconds long. In their book The New Rolling Stone Album Guide, Nathan Brackett and Christian David Hoard labelled it a neo-disco track. Justin Myers of the Official Charts Company characterized it as a dance-pop song, while Stereogum\\'s Tom Breinan described it as a techno-pop anthem. \"Can\\'t Get You Out of My Head\" is written in the key of D minor. The song, which does not follow the common verse–chorus structure, is composed of numerous fragmented sections. According to Davis, it \"breaks a few rules as it starts with a chorus and in comes the \\'la\\'s\\'\". Minogue chants a \"la la la\" hook that is often noted', 'the song\\'s most appealing part by music critics. According to BBC Radio 2, the song\\'s composition is \"deceptively simple, but its veins run with the whole history of electronic music\". The writer described the song\\'s bassline as \"pulsing\" and influenced by the music of English rock band New Order and German electronic music band Kraftwerk. \"Can\\'t Get You Out of My Head\" is about an obsession with an unknown person, who according to The Guardian\\'s Dorian Lansky could be \"a partner, an evasive one-night stand or someone who doesn\\'t know [the song\\'s narrator] exists\". Writing for the same newspaper, Everett True identified a \"darker element\" in the simple lyrics and said this sentiment is echoed in Minogue\\'s', 'the Herald Sun, Cameron Adams placed \"Can\\'t Get You Out of My Head\" at the top of his list of Minogue\\'s best songs and called it \"a happy accident\". Adams wrote, \"if you could program a computer to formulate the perfect pop song, it would sound like this\". Reviewing The Abbey Road Sessions version of the song, Tim Sendra of AllMusic said the \"most interesting reboot\" on the album took place on \"Can\\'t Get You Out of My Head\", saying the \"insistent strings push the song along with tightly coiled electricity that is impossible to resist\". Sal Cinquemani of'][...]\n",
      "Answer in context? 1 \n",
      "Prediction: Cathy Dennis \n",
      "\n",
      "\n",
      "Given Query: who has the most big ten championships in football\n",
      "GT Answer: ['Michigan']\n",
      "Context Given ['8× National Champions: 1942, 1954, 1957, 1961, 1968, 1970, 2002, 2014 ; 39× Big Ten Champions: 1916, 1917, 1920, 1935, 1939, 1942, 1944, 1949, 1954, 1955, 1957, 1961, 1968–1970, 1972–1977, 1979, 1981, 1984, 1986, 1993, 1996, 1998, 2002, 2005–2009, 2010 (vacated), 2014, 2017–2020 ; 2× Leaders Division Champions: 2012, 2013 ; 6× East Division Champions: 2014–2019 ; 2× OAC Champions: 1906, 1912', 'Most wins in college football history (976) ; Most winning seasons of any program (121) ; Most appearances in the final AP Poll (61) ; More conference titles in the Big Ten than any other FBS program with a single conference (43) ; One of only six programs with a winning record against every FBS conference', 'Big Ten Championships (4): ; 1995, 1997, 2008, 2016', 'This list goes through the 2020 season. † Ohio State vacated 12 wins and its Big Ten title in 2010 due to NCAA sanctions. †† Numbers of division and conference championships shown reflect Big Ten history only and do not include division and conference championships in former conferences. Maryland and Rutgers joined the Big Ten in 2014, and Nebraska joined in 2011. Number of Claimed National Championships, as well as win-loss-tie records, include all seasons played, regardless of conference membership.', 'Ohio State joined the Big Ten in 1912; before that they were a member of the Ohio Athletic Conference and won two OAC titles. Ohio State has won a championship in the Big Ten 39 times, second-most in the conference and third most conference titles of any school in any conference. † Co-champions'][...]\n",
      "Answer in context? 0 \n",
      "Prediction: Ohio State \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w_embeds = True\n",
    "temp = 0\n",
    "max_tokens = 64\n",
    "max_embeds = 1\n",
    "icl_examples = 5\n",
    "max_bs = 4\n",
    "rag = False\n",
    "\n",
    "device_count = torch.cuda.device_count()\n",
    "other_device = device if device_count <= 1 else torch.device(\"cuda:1\")\n",
    "\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "    \n",
    "prompt_prefix = create_prompt_prefix(\n",
    "    queries=questions,\n",
    "    answers=[answer[0] for answer in answers],\n",
    "    docs=context if rag and not w_embeds else None,\n",
    "    max_examples=icl_examples,\n",
    ")\n",
    "\n",
    "generated_sequences = []\n",
    "\n",
    "queries = list(questions[icl_examples:])\n",
    "docs = list(context[icl_examples:])\n",
    "truths = list(answers[icl_examples:])\n",
    "queries.reverse()\n",
    "docs.reverse()\n",
    "truths.reverse()\n",
    "\n",
    "if w_embeds:\n",
    "\n",
    "    no_context_prompt = [\n",
    "        create_prompt(\n",
    "            prefix=prompt_prefix, doc=\"\", query=query, wdoc=False\n",
    "        )\n",
    "        for query in queries[:max_bs]\n",
    "    ]\n",
    "\n",
    "    context_prompt = [\n",
    "        create_prompt(\n",
    "            prefix=\" answer the question following the examples:\\n\\n\"\n",
    "            + prompt_prefix,\n",
    "            doc=\"\",\n",
    "            query=query,\n",
    "            wdoc=False,\n",
    "        )\n",
    "        for query in queries[:max_bs]\n",
    "    ]\n",
    "\n",
    "else:\n",
    "\n",
    "\n",
    "    no_context_prompt = [\n",
    "        create_prompt(\n",
    "            prefix=prompt_prefix,\n",
    "            doc=doc if rag else '',\n",
    "            query=query,\n",
    "            wdoc=True,\n",
    "        )\n",
    "        for query, doc in zip(\n",
    "            queries[:max_bs],\n",
    "            docs[:max_bs],\n",
    "        )\n",
    "    ]\n",
    "\n",
    "\n",
    "# for i, cont in enumerate(context_prompt if pipeline.pipeline_args.w_prefix_prompt else no_context_prompt):\n",
    "#     print(f'{i}',cont)\n",
    "\n",
    "\n",
    "print('Prompt Prefix:\\n', prompt_prefix)\n",
    "    # print(no_context_prompt[i] if not pipeline.pipeline_args.w_prefix_prompt else context_prompt[i])\n",
    "    # print(f'Ground truth answer: {a}\\n')\n",
    "generated_sequence = pipeline.generate(\n",
    "    prompt_pre_embed= (['']*len(queries[:max_bs]) if not pipeline.pipeline_args.w_prefix_prompt \n",
    "        else ['Based on the context ']*len(queries[:max_bs])),\n",
    "    prompt_post_embed = context_prompt if pipeline.pipeline_args.w_prefix_prompt else no_context_prompt,\n",
    "    text_conditioning= [doc[:max_embeds] for doc in docs[:max_bs]] if w_embeds else None,\n",
    "    temperature=temp,\n",
    "    max_tokens=max_tokens,\n",
    "    truncate_line=True,\n",
    "    device=device,\n",
    "    device_generation=other_device,\n",
    ")\n",
    "\n",
    "print('GENERATION')\n",
    "for i, (q, a, d, g) in enumerate(zip(queries[:max_bs], truths[:max_bs], docs[:max_bs], generated_sequence)):\n",
    "    print(f'Given Query: {q}\\nGT Answer: {a}\\nContext Given {d}[...]\\nAnswer in context? {get_approx_em(d,a[0])}','\\nPrediction:', g, '\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconstruct tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniini \n",
      "\n",
      "\n",
      "A written by Bob Russell and Bob Hebert written by Bob Russell written by Bob He He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written \n",
      "\n",
      "\n",
      "1717111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w_embeds = True\n",
    "temp = 0\n",
    "max_tokens = 256\n",
    "n_context = 0\n",
    "i_token_to_flip = -1\n",
    "\n",
    "device_count = torch.cuda.device_count()\n",
    "other_device = device if device_count <= 1 else torch.device(\"cuda:1\")\n",
    "\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "\n",
    "temp = [temp] * max_tokens   \n",
    "if max_tokens > i_token_to_flip >= 0:\n",
    "    temp[i_token_to_flip] = -1\n",
    "\n",
    "# 1 information in the doc which enables to answer the question but not good response often in-context\n",
    "# 2 information in the doc which enables to answer the question and good response often in-context\n",
    "# 3 Hard negative passage\n",
    "\n",
    "\n",
    "\n",
    "prompt_prefix = [\"Query: who wrote the song photograph by ringo starr\\nAnswer: Ringo Starr\\n\\n\",\"Query: who is playing the halftime show at super bowl 2016\\nAnswer: Coldplay\\n\\n\",\"Query: where was the world economic forum held this year\\nAnswer: Davos\\n\\n\",\"Query: where are the giant redwoods located in california\\nAnswer: Humboldt County\\n\\n\",\"Query: who has made the most premier league appearances\\nAnswer: Gareth Barry\\n\\n\"]\n",
    "prompt_prefix = ''.join(prompt_prefix[:n_context])\n",
    "# prompt_prefix = \"Query: \"\n",
    "\n",
    "# prompts = ['who has most followers on instagram in world','who did the united states win its independence from', 'what is the capital of France']\n",
    "prompts = [\"what is the hot coffee mod in san andreas\",\"who wrote he ain't heavy he's my brother lyrics\",\"when was the last time anyone was on the moon\"]\n",
    "context_prompts = [' answer the question following the examples ' +prompt_prefix + 'Query: '+ prompt  + '\\nAnswer:' for prompt in prompts]\n",
    "# no_context_prompts = [prompt_prefix + 'Query: '+ prompt + '\\nAnswer:' for prompt in prompts]\n",
    "no_context_prompts = ['' for prompt in prompts]\n",
    "\n",
    "# conditioning = [\"This list contains the top 50 accounts with the most followers on the photo and video-sharing social platform Instagram. As of July 2019, the most followed user is Instagram's own account, with over 308 million followers. Cristiano Ronaldo is the most followed individual, with over 177 million followers. Fifteen accounts have exceeded 100 million followers on the site.\",\n",
    "#                 \"During the American Revolution, the legal separation of the thirteen colonies from Great Britain in 1776 actually occurred on July 2, when the Second Continental Congress voted to approve a resolution of independence that had been proposed in June by Richard Henry Lee of Virginia declaring the United States independent from Great Britain's rule. After voting for independence, Congress turned its attention to the Declaration of Independence, a statement explaining this decision, which had been prepared by a Committee of Five, with Thomas Jefferson as its principal author. Congress debated and revised the wording of the Declaration, finally approving it two days later on July 4. A day earlier, John Adams had written to his wife Abigail\",\n",
    "#                 \"France is a country located primarily in Western Europe. Its overseas regions and territories include French Guiana in South America, Saint Pierre and Miquelon in the North Atlantic, the French West Indies, and many islands in Oceania and the Indian Ocean, giving it one of the largest discontiguous exclusive economic zones in the world. Metropolitan France shares borders with Belgium and Luxembourg to the north, Germany to the northeast, Switzerland to the east, Italy and Monaco to the southeast, Andorra and Spain to the south, and a maritime border with the United Kingdom to the northwest. Its metropolitan area extends from the Rhine to the Atlantic Ocean and from the Mediterranean Sea to the English Channel and the North Sea. \"]\n",
    "\n",
    "conditioning = ['Hot Coffee is a normally inaccessible mini-game in Grand Theft Auto: San Andreas. The mini-game portrays crudely animated sexual intercourse between the main character and a chosen partner. After Patrick Wildenborg, a software engineer who also went by the alias \\\"PatrickW\\\", modified the game to make the mini-game accessible, Hot Coffee quickly gained notoriety worldwide, impacting consumer culture, politics and the video game industry as a whole. Rockstar initially blamed a \\\"determined group of hackers\\\" for hacking the base game and creating the mini-game from scratch. This claim was eventually refuted, as the mini-game\\'s code and assets had been developed by Rockstar and were already present, unfinished and abandoned, on the game disc: the mod simply made the existing content available to players. Rockstar would go on to indicate that they expected the ESRB rating to remain unchanged, as they had no control',\n",
    "                \"He Ain't Heavy, He's My Brother written by Bob Russell and Bobby Scott; all other titles written by Neil Diamond.\",\n",
    "                \"17, 1970, as part of the Lunokhod program. To date, the last human to stand on the Moon was Eugene Cernan, who as part of the Apollo 17 mission, walked on the Moon in December 1972. Apollo 17 was followed by several uncrewed interplanetary missions operated by NASA. One of the notable interplanetary missions is Voyager 1, the first artificial object to leave our Solar System into interstellar space on August 25, 2012. It is also the most distant artificial object from Earth. The probe passed the heliopause at 121 AU to enter interstellar space. Voyager 1 is currently at a distance of 145.11 AU (21.708 billion kilometers; 13.489 billion miles) from Earth as of January 1, 2019.\"]\n",
    "\n",
    "# answers = ['Instagram','Great Britain',\"Paris\"]\n",
    "# answers = [\"a normally inaccessible mini-game\", [\"Bobby Scott\", \"Bob Russell\"],[\"14 December 1972 UTC\", \"December 1972\"]]\n",
    "\n",
    "# conditioning = 'Kyutai is a non-profit laboratory dedicated to open research in AI, founded in November 2023 by the iliad Group, CMA CGM and Schmidt Sciences.'\n",
    "# prompts =  prompt_prefix +'Query: when was founded Kyutai?\\nAnswer: '\n",
    "\n",
    "\n",
    "\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "\n",
    "generated_sequence = pipeline.generate(\n",
    "    prompt_pre_embed = (['']*len(conditioning) if not pipeline.pipeline_args.w_prefix_prompt \n",
    "    else ['Based on the context ']*len(conditioning)), \n",
    "    prompt_post_embed = context_prompts if pipeline.pipeline_args.w_prefix_prompt  else no_context_prompts,\n",
    "    text_conditioning=conditioning,\n",
    "    temperature=temp,\n",
    "    max_tokens=max_tokens,\n",
    "    truncate_double_space=True,\n",
    "    device=device,\n",
    "    device_generation=other_device,\n",
    ")\n",
    "\n",
    "for seq in generated_sequence:\n",
    "    print(seq, '\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim_toks = pipeline.pipeline_args.max_seq_len\n",
    "\n",
    "atlas_eval_data = \"/lustre/scwpod02/client/kyutai-interns/hippop/processed_data/wiki_passages_pretraining/valid_atlas_enwiki-dec2021_standard.jsonl\"\n",
    "atlas_valid_passage = []\n",
    "dump_eval_data = '/lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_valid.jsonl'\n",
    "dump_valid_passage = []\n",
    "\n",
    "with open(atlas_eval_data, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        atlas_valid_passage.append(\n",
    "            pipeline.tokenizer.decode(\n",
    "                pipeline.tokenizer.encode(\n",
    "                    json.loads(line)[\"text\"], eos=True, bos=True\n",
    "                )[:lim_toks]\n",
    "            )\n",
    "            )\n",
    "random.shuffle(atlas_valid_passage)\n",
    "\n",
    "\n",
    "with open(dump_eval_data, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        text = json.loads(line)[\"text\"]\n",
    "        dump_valid_passage.append(\n",
    "                pipeline.tokenizer.decode(\n",
    "                    pipeline.tokenizer.encode(\n",
    "                        text if \"\\n\\n\" not in   text else text.split(\"\\n\\n\")[1], eos=True, bos=True\n",
    "                    )[:lim_toks]\n",
    "                )\n",
    "            )\n",
    "random.shuffle(dump_valid_passage)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_passages = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 0.0\n",
    "device_count = torch.cuda.device_count()\n",
    "other_device = device if device_count <= 1 else torch.device(\"cuda:1\")\n",
    "atlas_generated_sequences = []\n",
    "for i in range(0, n_passages, max_batch_size):\n",
    "    passage = atlas_valid_passage[i : i + max_batch_size]\n",
    "    generated_sequence, logprobs = pipeline.generate(\n",
    "        prompt_pre_embed = (['']*len(passage) if not pipeline.pipeline_args.w_prefix_prompt \n",
    "        else ['In other words, background: ']*len(passage)), \n",
    "        prompt_post_embed = (['']*len(passage) if not pipeline.pipeline_args.w_prefix_prompt \n",
    "        else [' is just another way of saying: ']*len(passage)),\n",
    "        text_conditioning=passage,\n",
    "        temperature=temp,\n",
    "        max_tokens=lim_toks,\n",
    "        truncate_double_space=False,\n",
    "        device=device,\n",
    "        device_generation=other_device,\n",
    "    )\n",
    "\n",
    "    atlas_generated_sequences.extend(generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 0.0\n",
    "device_count = torch.cuda.device_count()\n",
    "other_device = device if device_count <= 1 else torch.device(\"cuda:1\")\n",
    "dump_generated_sequences = []\n",
    "for i in range(0, n_passages, max_batch_size):\n",
    "    passage = dump_valid_passage[i : i + max_batch_size]\n",
    "    generated_sequence, logprobs = pipeline.generate(\n",
    "        prompt_pre_embed = (['']*len(passage) if not pipeline.pipeline_args.w_prefix_prompt \n",
    "        else ['In other words, background: ']*len(passage)), \n",
    "        prompt_post_embed = (['']*len(passage) if not pipeline.pipeline_args.w_prefix_prompt \n",
    "        else [' is just another way of saying: ']*len(passage)),\n",
    "        text_conditioning=passage,\n",
    "        temperature=temp,\n",
    "        max_tokens=lim_toks,\n",
    "        truncate_double_space=False,\n",
    "        device=device,\n",
    "        device_generation=other_device,\n",
    "    )\n",
    "\n",
    "    dump_generated_sequences.extend(generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG scores 0.8125772827951994\n",
      "AVG scores TRUNC 0.8183385157580503\n",
      "Very small bleu 1\n",
      "Mismatch 31\n"
     ]
    }
   ],
   "source": [
    "avg_bleu = 0\n",
    "n_mismatch = 0\n",
    "n_very_small_blue = 0\n",
    "for gen, gt in zip(dump_generated_sequences, dump_valid_passage):\n",
    "    bleu_score = get_bleu_score(gt, gen)\n",
    "    avg_bleu += bleu_score\n",
    "    \n",
    "    try:\n",
    "        if bleu_score != get_bleu_score(gt, gen, trunc = True):\n",
    "            n_mismatch += 1\n",
    "            # print('MISMATCH {}'.format(n_mismatch))\n",
    "            # print('Ge:', gen)\n",
    "            # print('GT:', gt)\n",
    "            # print(bleu_score, ' | ', get_bleu_score(gt, gen, trunc = True))\n",
    "            if bleu_score < 0.1:\n",
    "                n_very_small_blue += 1\n",
    "            \n",
    "        elif bleu_score < 0.1:\n",
    "            n_very_small_blue += 1\n",
    "            print('VERY SMALL BLEU {}'.format(n_very_small_blue))\n",
    "            print('Ge:', gen)\n",
    "            print('GT:', gt)\n",
    "            print(bleu_score)\n",
    "    except ValueError:\n",
    "        print('Passages skipped')\n",
    "\n",
    "print('AVG scores',get_bleu_score(dump_valid_passage, dump_generated_sequences))\n",
    "print('AVG scores TRUNC',get_bleu_score(dump_valid_passage, dump_generated_sequences, trunc = True))\n",
    "print('Very small bleu', n_very_small_blue)\n",
    "print('Mismatch', n_mismatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERY SMALL BLEU 1\n",
      "Ge: 0 A subsequence of a string whose characters are common in two strings (S1 and S2) can be computed efficiently using the notion of common subsequence. The problem has a simple algorithmic solution; both standard algorithms run in linear time in DSPSS (this is still faster than in Python) for a computer. It has been well studied in mathematics, its basic notions of deletions and appearances, the problem of finding Maximal Common Subsequences for sequences of four or more characters, for extra characters with few constraints that have the property of overlapping in more than one way. This problem (and its generalization to image alignment and image reconstruction) fits in studies of the geometry of editing theories (in Russian), dynamical systems, and computer science, and is motivated by the mean comparison of DNA sequences and the reconstruction of common subsequences at short edit distances. If one computes long subsequences of S1 and S2, such that the computations on such subsequences would not be revised by chance if they were returned from a computer. What one gets from this computation is a relation that is evidently useful to calculate the global similarity, not particularly biological or mathematical.\n",
      "GT:  A common subsequence of two strings S and T is a string whose characters appear in the same order (not necessarily consecutively) both in S and in T. The problem of computing a longest common subsequence has been well studied in computer science. It can be solved in polynomial time by dynamic programming; this basic algorithm has additional speedups for small alphabets (the Method of Four Russians), for strings with few differences, for strings with few matching pairs of characters, etc. This problem and its generalizations to more complex forms of edit distance have important applications in areas that include bioinformatics (in the comparison of DNA and protein sequences and the reconstruction of evolutionary trees), geology (in stratigraphy), and computer science (in data comparison and revision control). One motivation for studying the longest common subsequences of random strings, given already by Chvátal and Sankoff, is to calibrate the computations of longest common subsequences on strings that are not random. If such a computation returns a subsequence that is significantly longer than what would be obtained at random, one might infer from this result that the match is meaningful or significant.\n",
      "0.09710923867339806\n",
      "VERY SMALL BLEU 3\n",
      "Ge: Zapp brought the bones of this thing out into the world. \"All I say,\" says a documentary film, in which they made sure you were a track which really does sound exactly like Real Tunes. For the first time on record, Fender Rhodes can hear them - fanatic with a giant, tenth-tall way!\" and the fans decided on Grand Funk III to split up again only 4 days after they overdubbed their voices. All Zapp stayed until summer of 1985, trying to talk him out of it, although the album wasn't an 'unqualified success' in the US. The album became the first topping both releases to Billboard Top 200 Melter.\"\n",
      "GT:  bring the balls out of this thing.\" Zappa said, \"All I did was, in a documentary way, make a record which tells you exactly what they really sound like. For the first time on record you can hear Grand Funk Railroad ... and they're fantastic, fan-tastic with an F three times taller than you!\" Grand Funk decided on the first day of overdubs to split up again, although Zappa stayed until 4 a.m. trying to talk them out of it. The album wasn't a commercial success, only making it to #52 in the Billboard Top 200''. The album was the final release to feature both bassist Mel Schacher and keyboardist Craig Frost.\n",
      "0.09549153712369463\n",
      "VERY SMALL BLEU 4\n",
      "Ge: Bordeaux, most inland wines are serious bottles, although undergoing an ageing-process. Sometimes, white wine can be an exception in Bordeaux. The must of château-barrel ageing requires 6 barrels of must for each, but some new châteaux (which number as high as 20 meters above the oak barrel) can impart a different degree of ageing. Just recently, the volume of wine to be added to Bordeaux is heritage-protected. In order to make barrel ageing, the châteaux have to be racked from time to time in clear mold. This process is attacked by living organisms, as recently as itmakes sense since adding the châteaux to the blend can also be done. Additionally, periods of ageing es.eseseseseseseseses\n",
      "GT:  In Bordeaux, most serious wines undergo barrel-ageing, although white wines can be an exception. Usually, six months of ageing in-barrel is required, but some châteaux barrel-age for as much as 20 months. The number of new barrels (which impart a higher degree of oak flavor to the wine) can vary from vintage to vintage, just as the duration of barrel-ageing. Only recently, addition of oak chips has been made legal in Bordeaux. During barrel-ageing, the wine needs to be racked in order to clear it of lees. This process is being challenged by some producers, as mentioned abovem since ageing on the lees can also add richness to the wine.\n",
      "0.0\n",
      "VERY SMALL BLEU 5\n",
      "Ge: Mr. Critical was mostly positive review of both Monroe's resurrection, with fellow reviewers highlighting Wells's development of the character. Aviation Monthly's Jonathan Keller praised Monroe's complementary plotting as a \"clever balance between Savage and Wells for a piece of fiction\". Paul Nardini stated that Superman Had to Have Murdered Himself had never altered his own blog, saying: \"If I hadn't told my series intro earlier, I'd also be looking askance at the latter's failure to continue from the book\". Amazon.com commended Monroe's plotting as a \"strong counterpoint to any implications of the original story\", whereas Kenneth Beggs of Family Friendly Gaming described the book's \"realistic aspects\" and \"friendship with the main characters\". Keeping the vital fictional elements of the tale offers that the series addresses a \"nasty, chilling, and bloody mash-up of vampires and zombies\" with \"punctuated dialogue\". Audiences Magazine praised the book for its suitability for schoolchildren. Starfleet.com wrote, \"Monroe is a smart, powerful plotline while\n",
      "GT:  Critical reception of Mr. Monster was mostly positive, with multiple reviewers highlighting Wells's further development of both the protagonist and plot. A Savannah Morning News review complimented John Cleaver's characterization as \"a nifty balancing act for Wells to have pulled off\". Lee Mandelo praised Wells's expansion of Serial Killer into a series, stating: \"If his own blog hadn't told me otherwise, I would never have guessed he hadn't intended a sequel from the beginning\". Alternative Magazine also commended Wells's continuation of the storyline, saying that Mr. Monster \"immediately addresses any faults with its predecessor's ending\" while keeping the \"strong aspects of the original story\". Kirkus Reviews noted that \"John's realistic familial relationships and friendships offer a counterbalance to the bloody, fantastical elements of the tale\" and recommended the novel for \"fans of genre mash-ups\". A School Library Journal reviewer described Mr. Monster as \"compelling, quick-paced, and chilling\". Publishers Weekly wrote that the book \"stands out with taut, sharp writing, strong plotting,\n",
      "0.0\n",
      "Passages skipped\n",
      "Passages skipped\n",
      "VERY SMALL BLEU 7\n",
      "Ge: London-Smiths Blacksmiths, except backyard enterprises - Craftsmen chose blacksmiths to provide transportation, abbreviated trolley, and horse carriage shops. ; The Concert Hall that served as an entertainment venue for 1840s to 1884 was the first operating theatre in the area. ; Foresight Tavern - This community-owned tavern established the London Correspondence Lodges and provided meeting places for local freemasons like their masters. ; Mullet Lodge was constructed in Phase I. Powerful London St. Andrews Methodist Church buildings are similar to what could be found in the early 19th century when the first horse drawn trolley arrived from an Irish settlement. These included resort municipalities with orange hues like Orangeville, Ontario. Built in the area of the settlement where the townships of Huron also fanned out, many builders were establishing a reputation for themselves in the region, bringing a need for paved roads through the village. The community was representative of early London's streets, docks, lakes and canals. ; Concert Hall when it came to a decision to construct a traveling house for the town\n",
      "GT: Blacksmith Shop-Entrepreneurs, like blacksmiths, chose transportation crossroads to establish off-farm businesses. ; Corbett Tavern - An 1840s tavern that provided accommodation, food and stabling for horses to the traveling public and served as the community meeting place. ; Lochaber Church - This Free Presbyterian Church was constructed in 1884. ; Mount Moriah Lodge - Masonic orders held their first meetings in local taverns until purpose built halls like this one could be constructed. The interior is representative of an early 19th-century London Masonic Hall. ; Purple Hill Lodge - Established by the Protestant Irish immigrants who brought Orangeism with them when they came to Canada. Many settled areas in the region of what is now southwestern Ontario built meeting halls for the orange order, including townships in the London district. These buildings were also a focal point for the community, providing a place where settlers could get to know their neighbors through dances, dinners, recitals and concerts. Representative of the first stage of urban development at a transportation crossroads. \n",
      "0.0\n",
      "VERY SMALL BLEU 8\n",
      "Ge: Perry Botkin: Special effects, piano, lead guitar ; Mainly Joe Harnell: Piano, RCA Studio Orchestra ; David Levine: Guitar, Tom Ferguson: Guitar, Mike Reeves: Corrado. ; strings: Jesse Levy, Ben Cohen, Neil Stubenhaus, Bruce Ditmas, Pete Jolly, Sol Infante ; guitar: Izzy Sherr ; percussion: Grover Washington, Jr., Paul Enger, Jerry Zenkin, Harvie Swartz, Fred Seibert, David Laub, Amy Assante, Sahib Shihab, Joe Napolitano, Terry Nuttycombe, Rene Vinyl ; cellos: Allan Shrock, Ervin Roth ; strings: Kermit Moore, Bill Green, Gerald Friedman ; guitar: Hank Cicalo, Buddy Rich, Tony Levin, Getty Peterson, Gayle Levant, Blossom Allen, John Word ; flutes: Buddy Rich, Chuck Rainey, Bruce Barth ; flutes: Jim Peterson, CAPA, Seymour Barab ; contractor: Leo S. Brubaker ; father: George Palmer \n",
      "GT: Special effects: Perry Botkin, Jr. ; Piano: Tom Hensley, Mike Lang, Pete Jolly ; Guitar: Lee Ritenour, David Cohen, Neil Levang ; Bass: Max Bennett, Reinie Press, Steve LaFever ; drums: Joe Correro, Sol Gubin ; percussion: Gene Estes ; violins: Iz Baker, Paul Shure, Jerry Vinci, Sid Sharp, Tibor Zelig, Henry Ferber, Assa Drori, Jimmie Getzoff, Harry Bluestone, Erno Neufeld, Nate Ross ; violas: Dave Schwartz, Allan Harshman, Gerry Nuttycombe, Sven Reher ; cellos: Ray Kramer, Fred Seykora, Armand Kaproff ; woodwinds: Johnny Rotella, Gene Cipriano, Ronnie Lang, Bud Shank, Bill Green ; Harp: Gayle Levant ; trumpets: Bill Peterson, Bud Brisbois, Tony Terran, Cappy Lewis, Buddy Childers ; trombones: Charles Loper, Dick Nash ; Musical contractor: Charles H. Stern ; Engine\n",
      "0.0\n",
      "VERY SMALL BLEU 11\n",
      "Ge: volcanic vents to the East covered the area, and a fault trench formed during the eastward slope of the Fissure. The east side of the Fissure opens the valley of the Lander River, and the north side is the range, which was filled with diorite, while the hooping vents filled with porphyry. The fault trench composes these faults into \"mineral-bearing \"ore\". The faults called \"mineral ore\" stated that \"pockets are abundant\". The miners made the ore widely through the thin black slate of the East Fissure and nearly all their upper layer in \"like a charity case\", although only parts of the upper and lower layers were found or plowed. With them the fault trench extended in all directions, and \"rich sextuple ore\" was almost 600 ft wide.\n",
      "GT:  Volcanic vents to the east covered the area during the Tertiary, and a fault fissure opened the east slope of the Virginia Range. The east slope of the range forms the footwall of the Lode, and is composed of diorite, while the hanging wall is composed of andesite, which the miners called \"porphyry\". The fault fissures filled these fissures with \"mineral-bearing quartz\". The miners stated \"porphyry makes ore\". The ore bodies were thinly scattered through the wide Lode \"like plums in a charity pudding\", and nearly all of them were found in the wide upper section and along or near the east wall. Although the miners extended their work in all directions, only \"sixteen large and rich ore bodies\" were found, most less than 600 ft in depth.\n",
      "0.0\n",
      "VERY SMALL BLEU 12\n",
      "Ge: 6th from the Events in Ireland.............\n",
      "GT:  Events from the 6th century in Ireland.\n",
      "0.0\n",
      "VERY SMALL BLEU 13\n",
      "Ge: Halichard 4, Jens Kehrle 4, Hans Kuys 3, Asitsholts 3, Volodymyr Kolodzey 3. AAU Amsterdam, Netherlands: Amsterdam Arena, April 17-19, 1976 – UEFA National Final in Amsterdam 101:108 (wellmans in favour of VALL 59-79) Jens Kehrle:- 84, Steve Grazier, Bryce Wells, Frank Maravitch, Vivian Horton, Tucker Smith. 3rd All Star All-Star Week 1978-79 AAU Amsterdam, Netherlands: Universe 25-39 (Nag 15-21) IN: Hans Peters, Bill Croft, Gary Fenn, Dirk Hagens (16), John Lee 20-23, Steve Grazier 20-23, Emmett Ford 16-47, John Hankma, Pete Mildenhall, Everett Dawson, Carl Krapp: 131 Jopal Zone. N 2-1 (238).\n",
      "GT:  Heidrich 4, Hans Kaltschmidt 4, Jürgen Kolze 4, Volker Asshoff 3. Apollohal, Amsterdam, April 17, 1976: USA All Stars in Netherlands – USA All Stars in West Germany 109-89 (att:51-39) USA All Stars in Netherlands : Owen Wells, Steven Bravard, Gary Freeman, Vince Fritz, Tyrone Marioneaux, Buff Kirkland, Hank Smith. 5th All-Star Gala 1977-78 Apollohal, Amsterdam, April 23, 1978: North – South 158-165 NORTH (Jim Parks): Al Davis 33, Dan Henderson 21, Pete Miller 20, John Franken 20, Hank Smith 18, Gary Freeman 16, Everett Fopma 13, John Wayne Croft 9, Emill Hagens 7, Bert Kragtwijk 1. SOUTH (Manny Cramford): Billy Taylor 2\n",
      "0.0\n",
      "Error with update: \n",
      "Ground-Truth:  Untung Pakai Esia  \n",
      "Pred:  untuk Pangai Esia   \n",
      "AVG scores 0.41971318539118835\n",
      "Error with update: \n",
      "Ground-Truth:  Untung Pakai Esia  \n",
      "Pred:  untuk Pangai Esia \n",
      "AVG scores TRUNC 0.43351392936413774\n",
      "Very small bleu 13\n",
      "Mismatch 50\n"
     ]
    }
   ],
   "source": [
    "avg_bleu = 0\n",
    "n_mismatch = 0\n",
    "n_very_small_blue = 0\n",
    "for gen, gt in zip(atlas_generated_sequences, atlas_valid_passage):\n",
    "    try:\n",
    "        bleu_score = get_bleu_score(gt, gen)\n",
    "        avg_bleu += bleu_score\n",
    "    except ValueError:\n",
    "        print('Passages skipped')\n",
    "    \n",
    "    try:\n",
    "        if bleu_score != get_bleu_score(gt, gen, trunc = True):\n",
    "            n_mismatch += 1\n",
    "            # print('MISMATCH {}'.format(n_mismatch))\n",
    "            # print('Ge:', gen)\n",
    "            # print('GT:', gt)\n",
    "            # print(bleu_score, ' | ', get_bleu_score(gt, gen, trunc = True))\n",
    "            if bleu_score < 0.1:\n",
    "                n_very_small_blue += 1\n",
    "            \n",
    "        elif bleu_score < 0.1:\n",
    "            n_very_small_blue += 1\n",
    "            print('VERY SMALL BLEU {}'.format(n_very_small_blue))\n",
    "            print('Ge:', gen)\n",
    "            print('GT:', gt)\n",
    "            print(bleu_score)\n",
    "    except ValueError:\n",
    "        print('Passages skipped')\n",
    "\n",
    "print('AVG scores',get_bleu_score(atlas_valid_passage, atlas_generated_sequences))\n",
    "print('AVG scores TRUNC',get_bleu_score(atlas_valid_passage, atlas_generated_sequences, trunc = True))\n",
    "print('Very small bleu', n_very_small_blue)\n",
    "print('Mismatch', n_mismatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hippolytepilchen/.cache/huggingface/modules/transformers_modules/nvidia/NV-Embed-v2/5130cf1daf847c1bacee854a6ef1ca939e747fb2/modeling_nvembed.py:349: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(batch_dict.get('input_ids').to(batch_dict.get('input_ids')).long()),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Pre Embed: [8, 0]\n",
      "Prompt Post Embed: [10, 1]\n",
      "['\\n\\nMario Alberto Bortolazzi (born 12 January 1964) is a former professional footballer who played as a midfielder. He played for 12 seasons at A.C. Milan, winning the Serie A championship in 1991–92, and the Coppa Italia in 1993–94. He was transferred to Genoa C.F.C. for 500,000 lire, and made his debut on 25 September 1995, in a match against Fiorentina at', 'Mario Bortolazzi\\n\\nMario Bortolazzi (born 12 January 1965 in Genoa) is an Italian former professional football player and coach. He played 12 seasons in Serie A, for A.C. Milan, Atalanta, Fiorentina, and Hellas Verona. He played 246 games in Serie A, scoring 14 goals. 1990–91 season, he played 12 games in Serie B, for A.C. Milan. 1991–92 season,']\n"
     ]
    }
   ],
   "source": [
    "# Flipping attempts\n",
    "w_embeds = True\n",
    "temp = 0\n",
    "max_tokens = 128\n",
    "i_token_to_flip = -1\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "\n",
    "temp = [temp] * max_tokens   \n",
    "if max_tokens > i_token_to_flip >= 0:\n",
    "    temp[i_token_to_flip] = 1000\n",
    "    \n",
    "prompt = ''\n",
    "text_conditioning ='Mario Bortolazzi (born 10 January 1965, in Verona) is an Italian professional football coach and a former player, who played as a midfielder. \\\n",
    "    \\n\\nHe played 12 seasons (241 games, 14 goals) in the Serie A for ACF Fiorentina, A.C. Milan, Hellas Verona F.C., Atalanta B.C. and Genoa C.F.C.'\n",
    "        # \\n\\nIn his coaching career he has so far has always been an assistant to his former Milan teammate Roberto Donadoni.\\\n",
    "        #     \\n\\nHonours\\n\\n - Milan\\n - Serie A champion: 1987–88.\\n\\n - Genoa\\n - Anglo-Italian Cup winner: 1995–96.'\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "generated_sequence, attn_or_logprobs, embeddings = pipeline.generate(prompt_pre_embed = ['In other words, background: ',''], \n",
    "                                    prompt_post_embed = [' is just another way of saying: ',''],\n",
    "                                    text_conditioning = [text_conditioning]*2, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False,\n",
    "                                    random_flip = i_token_to_flip,\n",
    "                                    device = device,\n",
    "                                    device_generation = device if torch.cuda.device_count() <= 1 else torch.device('cuda:1'),\n",
    "                                    return_embeddings = True)\n",
    "print(generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_passages = 20\n",
    "\n",
    "lim_toks = 128\n",
    "eval_data = '/lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_valid.jsonl'\n",
    "train_data = '/lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_train.jsonl'\n",
    "train_passage = []\n",
    "valid_passage = []\n",
    "\n",
    "with open(train_data, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == n_passages:\n",
    "            break\n",
    "        train_passage.append(pipeline.tokenizer.decode(pipeline.tokenizer.encode(json.loads(line)['text'].split('\\n\\n')[1], eos = True, bos = True)[:lim_toks]))\n",
    "  \n",
    "with open(eval_data, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == n_passages:\n",
    "            break\n",
    "        valid_passage.append(pipeline.tokenizer.decode(pipeline.tokenizer.encode(json.loads(line)['text'].split('\\n\\n')[1], eos = True, bos = True)[:lim_toks]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_embeds = True\n",
    "temp = 0\n",
    "max_tokens = 128\n",
    "i_token_to_flip = -1\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "\n",
    "temp = [temp] * max_tokens   \n",
    "if max_tokens > i_token_to_flip >= 0:\n",
    "    temp[i_token_to_flip] = 1000\n",
    "\n",
    "# prompt_prefix = \"Query: who wrote the song photograph by ringo starr\\nAnswer: Ringo Starr\\n\\nQuery: who is playing the halftime show at super bowl 2016\\nAnswer: Coldplay\\n\\nQuery: where was the world economic forum held this year\\nAnswer: Davos\\n\\nQuery: where are the giant redwoods located in california\\nAnswer: Humboldt County\\n\\nQuery: who has made the most premier league appearances\\nAnswer: Gareth Barry\\n\\nQuery: \"\n",
    "prompt_prefix = \"Query: \"\n",
    "prompts = ['who has most followers on instagram in world','who did the united states win its independence from', 'locations for the film an englishman who went up a hill', 'who is the valley of the dolls based on']\n",
    "prompts = [prompt_prefix + prompt + '\\nAnswer:' for prompt in prompts]\n",
    "\n",
    "conditioning = [\"This list contains the top 50 accounts with the most followers on the photo and video-sharing social platform Instagram. As of July 2019, the most followed user is Instagram's own account, with over 308 million followers. Cristiano Ronaldo is the most followed individual, with over 177 million followers. Fifteen accounts have exceeded 100 million followers on the site.\",\n",
    "                     \"During the American Revolution, the legal separation of the thirteen colonies from Great Britain in 1776 actually occurred on July 2, when the Second Continental Congress voted to approve a resolution of independence that had been proposed in June by Richard Henry Lee of Virginia declaring the United States independent from Great Britain's rule. After voting for independence, Congress turned its attention to the Declaration of Independence, a statement explaining this decision, which had been prepared by a Committee of Five, with Thomas Jefferson as its principal author. Congress debated and revised the wording of the Declaration, finally approving it two days later on July 4. A day earlier, John Adams had written to his wife Abigail\",\n",
    "                     'The village was a primary location for the making of the film \\\"The Englishman Who Went Up a Hill But Came Down a Mountain\\\", which starred Hugh Grant. The hilltop scenes were filmed on the Gyrn, the long hill that overlooks the village. It was also featured in \\\"Monk\\'s Hood\\\", an episode of \\\"The Cadfael Chronicles\\\"',\n",
    "                     'Valley of the Dolls is the first novel by American writer Jacqueline Susann. Published in 1966, the book was the biggest selling novel of its year. To date, it has sold more than 31 million copies, making it one of the best-selling works in publishing history.']\n",
    "\n",
    "# answers = ['Instagram','Great Britain',\"Llansilin in Powys\",[\"Judy Garland\", \"Carole Landis\", \"Dean Martin\", \"Ethel Merman\"]]\n",
    "\n",
    "# conditioning = 'Kyutai is a non-profit laboratory dedicated to open research in AI, founded in November 2023 by the iliad Group, CMA CGM and Schmidt Sciences.'\n",
    "# prompts =  prompt_prefix +'Query: when was founded Kyutai?\\nAnswer: '\n",
    "\n",
    "\n",
    "\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "pipeline.model.llm.pos_to_keep = []\n",
    "generated_sequence, logprobs = pipeline.generate(prompts = prompts, \n",
    "                                    text_conditioning = conditioning, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False,\n",
    "                                    random_flip = i_token_to_flip,\n",
    "                                    device = device,\n",
    "                                    device_generation = device if torch.cuda.device_count() <= 1 else torch.device('cuda:1'),\n",
    "                                    return_embeddings = False)\n",
    "print(generated_sequence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vizualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_att_weights, tokens = get_attention(generated_sequence[0], embeddings, pipeline.tokenizer, pipeline.model.llm, n_tokens = 20)\n",
    "# self_att_weights, tokens = get_attention('He played 12 seasons (243 games, 14 goals) in the Serie A for A.C. Milan, A.F.C. Fiorentina, Hellas Verona, Atalanta B.C. and Genoa C.F.C.', embeddings, pipeline.tokenizer, pipeline.model.llm, n_tokens = 20)\n",
    "\n",
    "head_view(self_att_weights, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_att_weights, tokens = get_attention(generated_sequence[0], embeddings, pipeline.tokenizer, pipeline.model.llm, n_tokens = 20)\n",
    "model_view([att[:,:,1:,1:] for att in self_att_weights], [tokens[0]] + tokens[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_per_head_att = [torch.mean(att, dim = 1, keepdim = True) for att in self_att_weights]\n",
    "overall_mean = torch.mean(torch.stack(self_att_weights), dim = 0)\n",
    "\n",
    "head_view(mean_per_head_att, tokens)\n",
    "# head_view([overall_mean], tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop)\n",
    "temperatures = [0, 0.5, 0.7, 1, 1.5]\n",
    "max_tokens = 150\n",
    "\n",
    "results_generation = {'0':{'train': {'word_prompt':{}, 'empty_prompt':{}}, 'valid': {'word_prompt':{}, 'empty_prompt':{}}}, \n",
    "                        '0.5':{'train': {'word_prompt':{}, 'empty_prompt':{}}, 'valid': {'word_prompt':{}, 'empty_prompt':{}}},\n",
    "                        '0.7':{'train': {'word_prompt':{}, 'empty_prompt':{}}, 'valid': {'word_prompt':{}, 'empty_prompt':{}}},\n",
    "                        '1':{'train': {'word_prompt':{}, 'empty_prompt':{}}, 'valid': {'word_prompt':{}, 'empty_prompt':{}}},\n",
    "                        '1.5':{'train': {'word_prompt':{}, 'empty_prompt':{}}, 'valid': {'word_prompt':{}, 'empty_prompt':{}}}}\n",
    "\n",
    "\n",
    "n_passages = len(train_passage)\n",
    "assert n_passages == len(valid_passage)\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f'Temperature: {temp}')    \n",
    "    generated_sequences = []\n",
    "    \n",
    "    for i in range(0, n_passages, max_batch_size):\n",
    "        passage = train_passage[i:i+max_batch_size]\n",
    "        generated_sequence, logprobs = pipeline.generate(prompts = [text.split(' ')[0] for text in passage], \n",
    "                                    text_conditioning = passage, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False,\n",
    "                                     device = device,\n",
    "                                    device_generation = device if torch.cuda.device_count() <= 1 else torch.device('cuda:1'))\n",
    "           \n",
    "        generated_sequences.extend(generated_sequence)\n",
    "    results_generation[str(temp)]['train']['word_prompt'] = {'seq':generated_sequences}\n",
    "    print('Train Passage:', passage)\n",
    "    print('Train Generated:', generated_sequence)\n",
    "    generated_sequences = []\n",
    "    for i in range(0, n_passages, max_batch_size):\n",
    "        passage = train_passage[i:i+max_batch_size]\n",
    "        generated_sequence, logprobs = pipeline.generate(prompts = [''] * len(passage), \n",
    "                                    text_conditioning = passage, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False,\n",
    "                                     device = device,\n",
    "                                    device_generation = device if torch.cuda.device_count() <= 1 else torch.device('cuda:1'))\n",
    "           \n",
    "        generated_sequences.extend(generated_sequence)    \n",
    "    results_generation[str(temp)]['train']['empty_prompt'] = {'seq':generated_sequences}\n",
    "    \n",
    "\n",
    "    generated_sequences = []\n",
    "    for i in range(0, n_passages, max_batch_size):\n",
    "        passage = valid_passage[i:i+max_batch_size]\n",
    "        generated_sequence, logprobs = pipeline.generate(prompts = [text.split(' ')[0] for text in passage], \n",
    "                                    text_conditioning = passage, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False,\n",
    "                                     device = device,\n",
    "                                    device_generation = device if torch.cuda.device_count() <= 1 else torch.device('cuda:1'))\n",
    "           \n",
    "        generated_sequences.extend(generated_sequence)    \n",
    "    results_generation[str(temp)]['valid']['word_prompt'] = {'seq':generated_sequences}\n",
    "    \n",
    "    generated_sequences = []\n",
    "    for i in range(0, n_passages, max_batch_size):\n",
    "        passage = valid_passage[i:i+max_batch_size]\n",
    "        generated_sequence, logprobs = pipeline.generate(prompts = [''] * len(passage), \n",
    "                                    text_conditioning = passage, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False,\n",
    "                                     device = device,\n",
    "                                    device_generation = device if torch.cuda.device_count() <= 1 else torch.device('cuda:1'))\n",
    "           \n",
    "        generated_sequences.extend(generated_sequence)    \n",
    "    results_generation[str(temp)]['valid']['empty_prompt'] = {'seq':generated_sequences}\n",
    "    print('Valid Passage:', passage)\n",
    "    print('Valid Generated:', generated_sequence)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for temp in results_generation.keys():\n",
    "    for split in results_generation[temp].keys():\n",
    "        for prompt_type in results_generation[temp][split].keys():\n",
    "            generated_sequences = results_generation[temp][split][prompt_type]['seq']\n",
    "            if prompt_type == 'empty_prompt':\n",
    "                gt_passage = train_passage if split == 'train' else valid_passage\n",
    "                overlap = word_overlap(gt_passage, generated_sequences)\n",
    "                bleu_score = get_bleu_score(gt_passage, generated_sequences)\n",
    "            elif prompt_type == 'word_prompt':\n",
    "                gt_passage = train_passage if split == 'train' else valid_passage\n",
    "                gt_passage = [' '.join(text.split(' ')[1:]) for text in gt_passage]\n",
    "                overlap = word_overlap(gt_passage, generated_sequences)\n",
    "                bleu_score = get_bleu_score(gt_passage, generated_sequences)\n",
    "   \n",
    "            print(f'Temperature: {temp}, Split: {split}, Prompt Type: {prompt_type}, Overlap: {overlap}', 'Bleu Score:', bleu_score)\n",
    "            metrics.append({'temp': temp, 'split': split, 'prompt_type': prompt_type, 'overlap': overlap, 'bleu_score': bleu_score})\n",
    "            \n",
    "# with open(f'{ckpt_path}/results_generation.json', 'w') as f:\n",
    "#     json.dump(metrics, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_path = '/lustre/scwpod02/client/kyutai-interns/hippop/models/mistral_7B'\n",
    "#Must have a params json for pipeline\n",
    "\n",
    "# No embeddings:\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/no_embed_bs16_lr5e-5Mistral7B88d0b42410aa4ec12025/checkpoints/checkpoint_002500'\n",
    "\n",
    "# Length tokens:\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_512t_Mistral7B20ed0018b2a84fba09c4/checkpoints/checkpoint_005000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_256t_Mistral7Be9ffc00fa42bedbc50d0/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_128t_Mistral7B226729d875c65b331ef8/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_64t_Mistral7B9bbea1b3b8dc23079b04/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_32t_Mistral7Bccbc3f29d69bd124c6cf/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_16t_Mistral7B7bc7dcc2ba28873eda96/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/mean_not_causal/checkpoints/checkpoint_007500'\n",
    "\n",
    "\n",
    "# # Continuation:\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/continuation_Mistral7B20ed0018b2a84fba09c4/checkpoints/checkpoint_006000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/mean_finetuned_notcausal_continuationMistral7B20ed0018b2a84fba09c4/checkpoints/checkpoint_005500'\n",
    "\n",
    "# # Cross-Attention:\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/cross_att_5_last_layersMistral7Bdbbb7faebb2f32cf20e9/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/cross_att_fine_tuned_embedder_5_last_layersMistral7Bdbbb7faebb2f32cf20e9/checkpoints/checkpoint_007500'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/cross_att_finetuned_notcausal_continuationMistral7B20ed0018b2a84fba09c4/checkpoints/checkpoint_005000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/cross_att_pretrained_continuationMistral7B20ed0018b2a84fba09c4/checkpoints/checkpoint_008500'\n",
    "\n",
    "with open(f'{ckpt_path}/params.json') as f:\n",
    "    params = json.load(f)\n",
    "print(params)\n",
    "\n",
    "model_name = 'Mistral7B' # Mistral7B, Llama3.2-3B, Gemma7B\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "w_embeds = True\n",
    "max_batch_size = 4\n",
    "\n",
    "# variant = '7b' if model_name == 'Gemma7B' else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify old params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ckpt_path + '/params.json') as f:\n",
    "    params = json.load(f)\n",
    "print(params)\n",
    "# if 'do_pool'  not in params.keys():\n",
    "if 'n_truncated_layers' in params['pooling_module'].keys():\n",
    "    params['n_truncated_layers'] = params['pooling_module']['n_truncated_layers']\n",
    "    del params['pooling_module']['n_truncated_layers']\n",
    "    \n",
    "\n",
    "if params['cross_att'] is not None:\n",
    "    print('here')\n",
    "    params['normalize_embeddings'] = True if params['cross_att'] else False\n",
    "    if params['start_cross_att'] is None:\n",
    "        del params['start_cross_att']\n",
    "    else:\n",
    "        params['cross_att_layers'] = 32 - params[\"start_cross_att\"]\n",
    "        del params['start_cross_att']\n",
    "    params['do_pool'] = False if params['cross_att'] else True\n",
    "else:\n",
    "    params['do_pool'] = True\n",
    "print(params)\n",
    "with open(ckpt_path + '/params.json', 'w') as f:\n",
    "    json.dump(params, f)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in tests:\n",
    "    print('Param:', param)\n",
    "    if param['w_embeds']:\n",
    "        pipeline.pipeline_args.w_embeds = True\n",
    "    else:\n",
    "        pipeline.pipeline_args.w_embeds = False\n",
    "    \n",
    "    final_valid_prompts = [passage.split(' ')[0] for passage in valid_passage][2] \n",
    "    text_valid_conditioning = [passage[:100] for passage in valid_passage][2]\n",
    "    print('Prompt', final_valid_prompts, ' | Passage', text_valid_conditioning)\n",
    "    generated_sequence = pipeline.generate(prompts = final_valid_prompts, \n",
    "                                        text_conditioning = text_valid_conditioning, \n",
    "                                        temperature = param['temperature'], \n",
    "                                        max_tokens = max_tokens,\n",
    "                                        truncate_double_space = False)\n",
    "    print('Valid  word', generated_sequence)\n",
    "    \n",
    "    final_valid_prompts = ['' for passage in train_passage][1]\n",
    "    text_valid_conditioning = [passage[:100] for passage in valid_passage][2]\n",
    "    generated_sequence = pipeline.generate(prompts = final_valid_prompts, \n",
    "                                        text_conditioning = text_valid_conditioning, \n",
    "                                        temperature = param['temperature'], \n",
    "                                        max_tokens = max_tokens,\n",
    "                                        truncate_double_space = False)\n",
    "    print('Valid  empty', generated_sequence)\n",
    "\n",
    "    final_train_prompts =  [passage.split(' ')[0] for passage in train_passage][1] \n",
    "    text_train_conditioning = [passage[:100] for passage in train_passage][1]\n",
    "    print('Prompt', final_train_prompts, ' | Passage', text_train_conditioning)\n",
    "    generated_sequence = pipeline.generate(prompts = final_train_prompts, \n",
    "                                       text_conditioning = text_train_conditioning, \n",
    "                                       temperature = param['temperature'], \n",
    "                                       max_tokens = max_tokens,\n",
    "                                       truncate_double_space = False)\n",
    "    print('Train word', generated_sequence)\n",
    "    final_train_prompts = ['' for passage in train_passage][1] \n",
    "    text_train_conditioning = [passage[:100] for passage in train_passage][1]\n",
    "    generated_sequence = pipeline.generate(prompts = final_train_prompts, \n",
    "                                       text_conditioning = text_train_conditioning, \n",
    "                                       temperature = param['temperature'], \n",
    "                                       max_tokens = max_tokens,\n",
    "                                       truncate_double_space = False)\n",
    "    print('Train empty', generated_sequence)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 information in the doc which enables to answer the question but not good response often in-context\n",
    "# 2 information in the doc which enables to answer the question and good response often in-context\n",
    "# 3 Hard negative passage\n",
    "# 4 Same\n",
    "\n",
    "# prompt_prefix = \"Query: who wrote the song photograph by ringo starr\\nAnswer: Ringo Starr\\n\\nQuery: who is playing the halftime show at super bowl 2016\\nAnswer: Coldplay\\n\\nQuery: where was the world economic forum held this year\\nAnswer: Davos\\n\\nQuery: where are the giant redwoods located in california\\nAnswer: Humboldt County\\n\\nQuery: who has made the most premier league appearances\\nAnswer: Gareth Barry\\n\\nQuery: \"\n",
    "# prompts = ['who has most followers on instagram in world','who did the united states win its independence from', 'locations for the film an englishman who went up a hill', 'who is the valley of the dolls based on']\n",
    "# final_prompts = [prompt_prefix + prompt + '\\nAnswer:' for prompt in prompts]\n",
    "\n",
    "# text_conditioning = [\"This list contains the top 50 accounts with the most followers on the photo and video-sharing social platform Instagram. As of July 2019, the most followed user is Instagram's own account, with over 308 million followers. Cristiano Ronaldo is the most followed individual, with over 177 million followers. Fifteen accounts have exceeded 100 million followers on the site.\",\n",
    "#                      \"During the American Revolution, the legal separation of the thirteen colonies from Great Britain in 1776 actually occurred on July 2, when the Second Continental Congress voted to approve a resolution of independence that had been proposed in June by Richard Henry Lee of Virginia declaring the United States independent from Great Britain's rule. After voting for independence, Congress turned its attention to the Declaration of Independence, a statement explaining this decision, which had been prepared by a Committee of Five, with Thomas Jefferson as its principal author. Congress debated and revised the wording of the Declaration, finally approving it two days later on July 4. A day earlier, John Adams had written to his wife Abigail\",\n",
    "#                      'The village was a primary location for the making of the film \\\"The Englishman Who Went Up a Hill But Came Down a Mountain\\\", which starred Hugh Grant. The hilltop scenes were filmed on the Gyrn, the long hill that overlooks the village. It was also featured in \\\"Monk\\'s Hood\\\", an episode of \\\"The Cadfael Chronicles\\\"',\n",
    "#                      'Valley of the Dolls is the first novel by American writer Jacqueline Susann. Published in 1966, the book was the biggest selling novel of its year. To date, it has sold more than 31 million copies, making it one of the best-selling works in publishing history.']\n",
    "\n",
    "# answers = ['Instagram','Great Britain',\"Llansilin in Powys\",[\"Judy Garland\", \"Carole Landis\", \"Dean Martin\", \"Ethel Merman\"]]\n",
    "\n",
    "n_passages = 4\n",
    "eval_data = '/lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_valid.jsonl'\n",
    "train_data = '/lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_train.jsonl'\n",
    "train_passage = []\n",
    "valid_passage = []\n",
    "with open(train_data, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == n_passages:\n",
    "            break\n",
    "        train_passage.append(json.loads(line)['text'].split('\\n\\n')[1])\n",
    "        \n",
    "with open(eval_data, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == n_passages:\n",
    "            break\n",
    "        valid_passage.append(json.loads(line)['text'].split('\\n\\n')[1])\n",
    "        \n",
    "tests = [{'w_embeds': True, 'temperature': 0 },  {'w_embeds': True, 'temperature': 0.7 }, {'w_embeds': False, 'temperature': 0.7 }]\n",
    "# print('Train passage:', train_passage)\n",
    "# print('Valid passage:', valid_passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditioning = ['Kyutai is a non-profit laboratory dedicated to open research in AI, founded in November 2023 by the iliad Group, CMA CGM and Schmidt Sciences. Launched with an initial team of six leading scientists, who have all worked with Big Tech labs in the USA, Kyutai continues to recruit at the highest level, and also offers internships to research Master’s degree students.']*4\n",
    "prompts = ['who are the founders of Kyutai?', 'when was Kyutai founded?', 'how many scientists were in the initial team?', 'what does Kyutai offer to research Master’s degree students?']\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "generated_sequence = pipeline.generate(prompts = prompts,\n",
    "                                      text_conditioning = conditioning,\n",
    "                                      temperature = 0.5, \n",
    "                                      max_tokens =200,\n",
    "                                      truncate_double_space = False)\n",
    "# random_flip, put the number of the token to flip. \n",
    "print(generated_sequence)\n",
    "\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "generated_sequence, logprobs = pipeline.generate(prompts = ['who has most followers on Instagram in world?'],\n",
    "                                      text_conditioning = [\"This list contains the top 50 accounts with the most followers on the photo and video-sharing social platform Instagram. As of July 2019, the most followed user is Instagram's own account, with over 308 million followers. Cristiano Ronaldo is the most followed individual, with over 177 million followers.\"],\n",
    "                                      temperature = 0.4, \n",
    "                                      max_tokens =200,\n",
    "                                      truncate_double_space = False)\n",
    "print(generated_sequence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuation\n",
    "for param in tests:\n",
    "    print('Param:', param)\n",
    "    if param['w_embeds']:\n",
    "        pipeline.pipeline_args.w_embeds = True\n",
    "    else:\n",
    "        pipeline.pipeline_args.w_embeds = False\n",
    "    \n",
    "    final_valid_prompts = [passage[100:].split(' ')[0] for passage in valid_passage][2] \n",
    "    text_valid_conditioning = [passage[:100] for passage in valid_passage][2]\n",
    "    print('Passage', text_valid_conditioning, ' | Truth', [passage[100:200] for passage in valid_passage][2] )\n",
    "    generated_sequence = pipeline.generate(prompts = final_valid_prompts, \n",
    "                                        text_conditioning = text_valid_conditioning, \n",
    "                                        temperature = param['temperature'], \n",
    "                                        max_tokens = max_tokens,\n",
    "                                        truncate_double_space = False)\n",
    "    print('Valid  word', generated_sequence)\n",
    "    \n",
    "    final_valid_prompts = ['' for passage in train_passage][2]\n",
    "    text_valid_conditioning = [passage[:100] for passage in valid_passage][2]\n",
    "    generated_sequence = pipeline.generate(prompts = final_valid_prompts, \n",
    "                                        text_conditioning = text_valid_conditioning, \n",
    "                                        temperature = param['temperature'], \n",
    "                                        max_tokens = max_tokens,\n",
    "                                        truncate_double_space = False)\n",
    "    print('Valid  empty', generated_sequence)\n",
    "\n",
    "    final_train_prompts =  [passage[100:].split(' ')[0] for passage in train_passage][1] \n",
    "    text_train_conditioning = [passage[:100] for passage in train_passage][1]\n",
    "    print('Passage', text_train_conditioning, ' | Truth', [passage[100:200] for passage in train_passage][1] )\n",
    "    generated_sequence = pipeline.generate(prompts = final_train_prompts, \n",
    "                                       text_conditioning = text_train_conditioning, \n",
    "                                       temperature = param['temperature'], \n",
    "                                       max_tokens = max_tokens,\n",
    "                                       truncate_double_space = False)\n",
    "    print('Train word', generated_sequence)\n",
    "    final_train_prompts = ['' for passage in train_passage][1] \n",
    "    text_train_conditioning = [passage[:100] for passage in train_passage][1]\n",
    "    generated_sequence = pipeline.generate(prompts = final_train_prompts, \n",
    "                                       text_conditioning = text_train_conditioning, \n",
    "                                       temperature = param['temperature'], \n",
    "                                       max_tokens = max_tokens,\n",
    "                                       truncate_double_space = False)\n",
    "    print('Train empty', generated_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tested\n",
    "# run_name = '128_SL_FN_False_0_MLP_False_CA_False_DB'\n",
    "# run_name = '128_SL_FN_False_0_MLP_True_CA_16_CAL_False_SKV_False_DB'\n",
    "# run_name = '128_SL_FN_False_0_MLP_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_0_MLP_True_CA_24_CAL_False_SKV_True_DB'\n",
    "\n",
    "# Finished runs:\n",
    "\n",
    "# run_name = '128_SL_FN_Truemean_1_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB' # 008500\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_4_TRUNC_True_CA_5_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_4_TRUNC_True_CA_24_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_1_MLP_4_TRUNC_True_CA_5_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_1_MLP_4_TRUNC_True_CA_24_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_0_MLP_4_TRUNC_True_CA_5_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_0_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_0_MLP_4_TRUNC_True_CA_24_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truelatent_attention_0_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truelatent_attention_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Trueeos_0_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Trueeos_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truereversed_latent_atttention_0_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truereversed_latent_attention_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_0_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_False_DB'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_False_DB'\n",
    "# run_name = '128_SL_FN_Truemean_0_MLP_8_TRUNC_True_CA_16_CAL_False_SKV_False_DB'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_8_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_False_DB'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB_dist_process'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_4_TRUNC_False_CA_False_DB'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_8_TRUNC_False_CA_False_DB'\n",
    "# run_name = '128_SL_FN_Truemean_0_MLP_4_TRUNC_False_CA_False_DB'\n",
    "# run_name = '128_SL_FN_Truemean_0_MLP_8_TRUNC_False_CA_False_DB'\n",
    "# run_name = '128_SL_FN_False_3_MLP_True_CA_5_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_3_MLP_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_3_MLP_True_CA_24_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_1_MLP_True_CA_5_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_1_MLP_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_1_MLP_True_CA_24_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_0_MLP_True_CA_5_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_3_MLP_True_CA_16_CAL_False_SKV_False_DB'\n",
    "# run_name = '128_SL_FN_False_3_MLP_False_CA_False_DB'\n",
    "# run_name = '128_SL_FN_False_3_MLP_True_CA_16_CAL_True_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_3_MLP_True_CA_16_CAL_False_SKV_True_DB_dist_process'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/ToyDecompressingTests_LLM_FT_MaxEmb_1_one_over_2/checkpoints/checkpoint_020000/instruct.json'\n",
    "\n",
    "dico = {'do': True, 'kl_pretraining': False, 'alpha': 1.0, 'tune_llm': True, 'tune_embedder': False, 'decompress_usage': 'one_over_two_reconstruction'}\n",
    "\n",
    "with open(path, 'w') as f:\n",
    "    json.dump(dico, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
