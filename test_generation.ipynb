{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hippolytepilchen/micromamba/envs/llm_embed/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from embed_llm.models.augmented_model import EmbedAugPipeline\n",
    "from embed_llm.generation.evaluation import word_overlap, get_bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 29\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'w_embeds': True, 'norm_wo_embeds': False, 'mlp_project': {'hidden_dim': 4096, 'n_layers': 0, 'act': 'gelu', 'in_dim': 4096, 'out_dim': 4096}, 'training': True, 'param_dtype': 'float32', 'embedder_name': 'NVEmbed', 'trainable_embedder': False, 'causal': True, 'do_pool': True, 'n_truncated_layers': 4, 'normalize_embeddings': True, 'pooling_module': {'type': 'eos', 'r': 512, 'n_heads': 8, 'n_layers': 1}, 'continuation': False, 'shared_kv': False, 'cross_att': True, 'cross_att_layers': 16, 'do_both': True}\n"
     ]
    }
   ],
   "source": [
    "llm_path = '/lustre/scwpod02/client/kyutai-interns/hippop/models/mistral_7B'\n",
    "#Must have a params json for pipeline\n",
    "\n",
    "# Finished runs:\n",
    "run_name = '128_SL_FN_False_0_MLP_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_0_MLP_False_CA_False_DB'\n",
    "# run_name = '128_SL_FN_False_0_MLP_True_CA_24_CAL_False_SKV_True_DB'\n",
    "\n",
    "# run_name = '128_SL_FN_False_0_MLP_True_CA_16_CAL_False_SKV_False_DB'\n",
    "# run_name = '128_SL_FN_False_0_MLP_False_CA_False_DB'\n",
    "\n",
    "\n",
    "\n",
    "ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/' + run_name \n",
    "\n",
    "with open(f'{ckpt_path}'+ '/checkpoints/checkpoint_010000/params.json') as f:\n",
    "    params = json.load(f)\n",
    "print(params)\n",
    "\n",
    "model_name = 'Mistral7B' \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "w_embeds = True\n",
    "max_batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:04<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cross att state dict\n",
      "Not only LoRA weights found in the checkpoint. Skipping other weights.\n",
      "Loading and merging LoRA weights...\n"
     ]
    }
   ],
   "source": [
    "pipeline: EmbedAugPipeline = EmbedAugPipeline.load_inference_model(llm_path = llm_path, \n",
    "                                                                   ckpt_path = ckpt_path + '/checkpoints/checkpoint_010000', \n",
    "                                                                   device = device,\n",
    "                                                                   llm_name = model_name, \n",
    "                                                                   embed_model_name = 'NVEmbed', # Not used if pretrainde ckpt available\n",
    "                                                                    max_batch_size = max_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_passages = 20\n",
    "\n",
    "lim_toks = 128\n",
    "eval_data = '/lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_valid.jsonl'\n",
    "train_data = '/lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_train.jsonl'\n",
    "train_passage = []\n",
    "valid_passage = []\n",
    "\n",
    "with open(train_data, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == n_passages:\n",
    "            break\n",
    "        train_passage.append(pipeline.tokenizer.decode(pipeline.tokenizer.encode(json.loads(line)['text'].split('\\n\\n')[1], eos = True, bos = True)[:lim_toks]))\n",
    "        \n",
    "with open(eval_data, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == n_passages:\n",
    "            break\n",
    "        valid_passage.append(pipeline.tokenizer.decode(pipeline.tokenizer.encode(json.loads(line)['text'].split('\\n\\n')[1], eos = True, bos = True)[:lim_toks]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hippolytepilchen/.cache/huggingface/modules/transformers_modules/nvidia/NV-Embed-v2/5130cf1daf847c1bacee854a6ef1ca939e747fb2/modeling_nvembed.py:349: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(batch_dict.get('input_ids').to(batch_dict.get('input_ids')).long()),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['map_bortoli.svg\\n\\nMario Bortoli (born 1951) is an Italian former professional football player. He played 12 seasons (236 games, 74 goals) in the Serie A for Genoa, Atalanta, Lazio, Fiorentina and Verona. He was part of the team that won the 1984–85 Coppa Italia with Verona. He was midfielder and coach of the team.']\n"
     ]
    }
   ],
   "source": [
    "# Flipping attempts\n",
    "w_embeds = True\n",
    "temp = 0.7\n",
    "max_tokens = 128\n",
    "i_token_to_flip = -1\n",
    "\n",
    "prompt = ''\n",
    "\n",
    "text_conditioning ='Mario Bortolazzi (born 10 January 1965, in Verona) is an Italian professional football coach and a former player, who played as a midfielder. \\\n",
    "    \\n\\nHe played 12 seasons (241 games, 14 goals) in the Serie A for ACF Fiorentina, A.C. Milan, Hellas Verona F.C., Atalanta B.C. and Genoa C.F.C.'\n",
    "        # \\n\\nIn his coaching career he has so far has always been an assistant to his former Milan teammate Roberto Donadoni.\\\n",
    "        #     \\n\\nHonours\\n\\n - Milan\\n - Serie A champion: 1987–88.\\n\\n - Genoa\\n - Anglo-Italian Cup winner: 1995–96.'\n",
    "\n",
    "\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "    \n",
    "if i_token_to_flip >= 0:\n",
    "    temp = [temp] * max_tokens\n",
    "    temp[i_token_to_flip] = 100\n",
    "            \n",
    "generated_sequence, logprobs = pipeline.generate(prompts = prompt, \n",
    "                                    text_conditioning = text_conditioning, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False,\n",
    "                                    random_flip = i_token_to_flip)\n",
    "print(generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0\n",
      "Train Passage: ['Salvador María de Iturbide y Huarte (17 July 1820 – 7 June 1856) was the eighth child (and third son) of Agustín I of Mexico and Empress Ana Maria Huarte. He was married in 1845 to Doña María del Rosario de Marzán y Guisasola. His descendants, through his son Salvador de Iturbide y de Marzán, are the current pretenders to the Mexican Throne. He was in the Secretary Mexican Legation in Washington, D.C. in', 'Parnaíba (U-17) is a river monitor of the Brazilian Navy. She is currently the last monitor in service.', \"St. Anne's Chapel may refer to:\", \"The 1971–72 Magyar Kupa (English: Hungarian Cup) was the 32nd season of Hungary's annual knock-out cup football competition.\"]\n",
      "Train Generated: ['de Hita\\n\\nSalvador de Hita (1808 – 25 September 1858) was the Mexican Agente-General in charge of the Fourth Marquisate of the Legion of the Mexican Emigrants. He was the son of Juan de Dios de Hita and Rosario de Arroyo. He married Ana Josefa Carrillo y Pardo. He was the uncle of Juan Nepomuceno de Dios de Hita. He was the brother of Juan Nepomuceno de Dios de Hita. He was the brother of Juan Nepomuceno de Dios de Hita. He was the brother of Juan Nepomuceno de Dios de H', 'U-17\\n\\nParnaíba U-17 is the last Brazilian monitor of the 2000s. She was commissioned into the service of the Navy, but was quickly disbanded.днєвнаднєвнаднєвнаднєвнаднєвнаднєвнаindustrialднєвнаindustrial industrindustrial industrindustrial industrindustrial industrindustrial industrindustrial industrindustrial industrindustrial industrindustrial industrindustrial industrindustrial industrindustrial industrindustrial industrindustrial industrindustrial industr', 'Anne’s Chapel\\n\\nSt. Anne’s Chapel may refer to: ISBN 978-1-935084-26-7 ISBN 978-1-935084-26-7 ISBN 978-1-935084-26-7 ISBN 978-1-935084-26-7 ISBN 978-1-935084-26-7 ISBN 978-1-935084-26-7 ISBN 978-1-9', '1972 Hung-Kuen Cup\\n\\nThe 1972 Hung-Kuen Cup is a Chinese martial arts film directed by and starring Yi Mit. It is the eleventh of the thirty-six annual installments of the series.\\n\\nPlot\\n\\nNam Sao, who had been living in peace in his village, was forced to leave his home after his father was knocked out by a devilish magician. He reached out to his brother, who was a powerful general, for help.док, who was a powerful general, for help.док, who was a powerful general, for help.док, who was a powerful general, for']\n",
      "Valid Passage: [\"The Oriental Club in London is an exclusive gentlemen's club established in 1824 that also admits ladies since 1952, although ladies could not be full members until 2010. Charles Graves describes it as fine in quality as White's but with the space of infinitely larger clubs. It is located in Stratford Place, near Oxford Street and Bond Street.\", 'Jerez is a town and municipality in the Mexican state of Zacatecas. To distinguish the two, the town is officially called Jerez de García Salinas to honor a 19th-century reformer. The town of Jerez is the local government of 128 other communities, a rural area noted for its production of fruit trees and dairy. The town was named a Pueblo Mágico to attract tourism, as it lies close to the state capital of Zacatecas and offers handcrafts, traditional food and architecture.', 'Mulwewa was a mission founded by White Fathers missionaries on the west side of Lake Tanganyika, in what is now the Democratic Republic of the Congo. It is at Massanze, near Uvira.', 'Emeril is an unincorporated community in the Canadian province of Newfoundland and Labrador.']\n",
      "Valid Generated: ['/******/ (function(modules) { // webpackBootstrap\\n\\n/******/ \\t// The module cache\\n\\n/******/ \\tvar installedModules = {};\\n\\n/******/ \\t// The require function\\n\\n/******/ \\tfunction __webpack_require__(moduleId) {\\n\\n/******/ \\t\\t// Check if module is in cache\\n\\n/******/ \\t\\tif(installedModules[moduleId]) {\\n\\n/******/ \\t\\t\\treturn installedModules[moduleId].exports;\\n\\n/******/ \\t\\t}\\n\\n/******/ \\t\\t// Create a new module (to be added to the cache)\\n\\n/******/ \\t\\tvar module = installedModules[moduleId', '/******/ (function(modules) { // webpackBootstrap\\n\\n/******/ \\t// The module cache\\n\\n/******/ \\tvar installedModules = {};\\n\\n/******/ \\t// The require function\\n\\n/******/ \\tfunction __webpack_require__(moduleId) {\\n\\n/******/ \\t\\t// Check if module is in cache\\n\\n/******/ \\t\\tif(installedModules[moduleId]) {\\n\\n/******/ \\t\\t\\treturn installedModules[moduleId].exports;\\n\\n/******/ \\t\\t}\\n\\n/******/ \\t\\t// Create a new module (to be added to the cache)\\n\\n/******/ \\t\\tvar module = { exports: {} };', \"/******/ (function(modules) { // webpackBootstrap\\n\\n/******/\\t// The module cache\\n\\n/******/\\tvar installedModules = {};\\n\\n/******/\\t// The require function\\n\\n/******/\\tfunction __webpack_require__(moduleId) {\\n\\n/******/\\t\\t// Check if we've already required this module.\\n\\n/******/\\t\\tif (installedModules[moduleId]) {\\n\\n/******/\\t\\t\\treturn installedModules[moduleId].exports;\\n\\n/******/\\t\\t}\\n\\n/******/\\t\\t// Create a new module (to be added to the cache)\\n\\n/******/\\t\\tvar module = installedModules[moduleId] = {\\n\\n/\", '/******/ (function(modules) { // webpackBootstrap\\n\\nvar __webpack_exports__ = {};\\n\\n// module by module\\n// id, loaded, module function for callback, exports, parent module\\nvar __webpack_require__ = function __webpack_require__(id, loaded, cb) {\\n    if (loaded) return __webpack_exports__[id];\\n    var mod = __webpack_module__[id] = {\\n        exports: {},\\n        parent: __webpack_parent__[id] || null\\n    };\\n    if (loaded) return __webpack_exports__[id];\\n    var execModule = function() {\\n        var __webpack_exec__ = function']\n",
      "Temperature: 0.5\n",
      "Train Passage: ['Salvador María de Iturbide y Huarte (17 July 1820 – 7 June 1856) was the eighth child (and third son) of Agustín I of Mexico and Empress Ana Maria Huarte. He was married in 1845 to Doña María del Rosario de Marzán y Guisasola. His descendants, through his son Salvador de Iturbide y de Marzán, are the current pretenders to the Mexican Throne. He was in the Secretary Mexican Legation in Washington, D.C. in', 'Parnaíba (U-17) is a river monitor of the Brazilian Navy. She is currently the last monitor in service.', \"St. Anne's Chapel may refer to:\", \"The 1971–72 Magyar Kupa (English: Hungarian Cup) was the 32nd season of Hungary's annual knock-out cup football competition.\"]\n",
      "Train Generated: ['de Hita\\n\\nDon Salvador de Hita y Urrea (1818 – 20 January 1885) was the eighth Marquis of the House of the Mexican Agnatic Royal Lineage. He was born in Mexico City, and was the son of Don Manuel de Hita y Guzmán, 6th Marquis of the House of the Mexican Agnatic Royal Lineage, and his wife, Doña Ana María Urrea y Arriola.\\n\\nHe was first secretary to the Preliminary Commission of the Treaty of Guadalupe-Hidalgo. He was a current member of the Jockey Club, the Emperors Club, and the Society of Friends', 'U-17\\n\\nParnaíba U-17 is the last Brazilian monitor of the 2000s. She is currently in service with the Navy, as part of the 5th Riverine Squadron.мннннннннннннннннннннннннннннннннннннннннннннннннннннннннннннннннннннннннннннннннннннннннннннннннннн', 'Anne’s Chapel\\n\\nSt. Anne’s Chapel may refer to:iëїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїїї', '1972 Hungerford Cup (season)\\n\\nThe 1972 Hungerford Cup was the 31st annual edition of the Hungerford Cup, a knockout competition for English football teams. It started on 5 October and finished with the 5th-place match on 18 December. Magpie-Blues were the sole outright winners of this competition, defeating Kettyle United in the final.рільна збірна УРСРрільна збірна УРСРрільна збірна УРСРрільна збірна УРСРрільна збірна УР']\n",
      "Valid Passage: [\"The Oriental Club in London is an exclusive gentlemen's club established in 1824 that also admits ladies since 1952, although ladies could not be full members until 2010. Charles Graves describes it as fine in quality as White's but with the space of infinitely larger clubs. It is located in Stratford Place, near Oxford Street and Bond Street.\", 'Jerez is a town and municipality in the Mexican state of Zacatecas. To distinguish the two, the town is officially called Jerez de García Salinas to honor a 19th-century reformer. The town of Jerez is the local government of 128 other communities, a rural area noted for its production of fruit trees and dairy. The town was named a Pueblo Mágico to attract tourism, as it lies close to the state capital of Zacatecas and offers handcrafts, traditional food and architecture.', 'Mulwewa was a mission founded by White Fathers missionaries on the west side of Lake Tanganyika, in what is now the Democratic Republic of the Congo. It is at Massanze, near Uvira.', 'Emeril is an unincorporated community in the Canadian province of Newfoundland and Labrador.']\n",
      "Valid Generated: ['elevens\\n\\nelevens is a 1994 full-length Orientalist Strange Club Gravity LP, released on CD by the London label White Label Recordings. It was the first of two albums to be released by Strange Club Gravity in 1994, the other being The Charles Place. The band\\'s members include Adam Fox, Dave Gorman, and Joey Piper. The lyrics were described as \"concrete poetry\" in a 2005 interview with Dave Gorman.д.д.д.д. industria.\\n\\nThe album was re-mastered and re-released in 2015.д', 'rope town\\n\\nJerez is a Mexican-American artist who has garnered critical acclaim for his work in the fields of photography and design. In 2001, he was named one of the 100 Best Magazines of the Year by the Guardian. His other notable productions include the 1996 production of the Zucchini, the 2008 film Deux, and the 2017 series of short stories, Capitán.с.с.\\n\\nThe state of the municipality is governed by a local government board, which is headed by a mayor and a deputy mayor. The town has a population of 19,', 'lewa\\n\\nmulenge mwana mwezi mwezi mwezi is a Tanzanian missionary founded on the Ukerewe Island in the west of Lake Victoria. The name was taken from the Congo Republic, where the white fathers had settled in the 19th century. The word mulenge means \"brothers\" in the Bantu languages.ддддддддддддддддддддддддддддддддддддддддддддддддддддддддддддддддддд', 'include\\t(../inc/hw.inc)\\n\\nEMERIL is an unofficial 80386/80486/80586/80586PX/80586DX/80586DX2/80586DX4/80586DX6/80586DX10/80586DX12/80586DX15/80586DX16/80586DX17/80586DX19/80586DX2']\n",
      "Temperature: 0.7\n",
      "Train Passage: ['Salvador María de Iturbide y Huarte (17 July 1820 – 7 June 1856) was the eighth child (and third son) of Agustín I of Mexico and Empress Ana Maria Huarte. He was married in 1845 to Doña María del Rosario de Marzán y Guisasola. His descendants, through his son Salvador de Iturbide y de Marzán, are the current pretenders to the Mexican Throne. He was in the Secretary Mexican Legation in Washington, D.C. in', 'Parnaíba (U-17) is a river monitor of the Brazilian Navy. She is currently the last monitor in service.', \"St. Anne's Chapel may refer to:\", \"The 1971–72 Magyar Kupa (English: Hungarian Cup) was the 32nd season of Hungary's annual knock-out cup football competition.\"]\n",
      "Train Generated: ['de Hita y Urreta\\n\\nSalvador de Hita y Urreta (25 January 1788 – 19 September 1852) was the eighth Mexican Agente-General in charge of the Marquisate of the Legion of the Mexican Empire. He was married to Rosario Díaz and they had three children. He was a descendant of the current pretender to the throne, Carlos I. He was educated in Paris and Seville, and entered the diplomatic service in 1816. In 1850 he was sent to Washington, D.C. to discuss the possible terms of a future treaty between Mexico and the United States', 'U\\n\\nParnaíba U is the last of the 17 monophyletic 16S rRNA gene-based clades currently in the service of the International Committee on Systematics of Prokaryotes (ICSP). It was first mentioned in 2007.дове зоомдове зоомдове зоомдове зоомдове зоомindustrial revolutionдове зоомindustrial revolutionindustrial revolutionindustrial revolutionindustrial revolutionindustrial revolutionindustrial revolutionindustrial revolutionindustrial revolutionindustrial revolutionindustrial revolutionindustrial revolutionindustrial revolutionindustrial revolutionindust', 'Anne’s Chapel\\n\\nSt. Anne’s Chapel may refer to:довкадокдокдокдок Industдокдок Industческих рабочихдок Industрии Украины Industрии Украиныдок Industрии Украиныдокдок Industрии Украины Industрии Украины Industрии Украины Industрии Украиныдок Industрии Украины Industрии Украины Industрии Украины Industрии Украиныдок Industрии Украины Industрии Украины Industрии Украины Industрии Украины Industрии Украины Industрии Украины', \"1972 Hung-Up Magnum-Cup\\n\\nThe 1972 Hung-Up Magnum-Cup is the 31st annual season of the English Kittens' Football League. It was broadcast on SBS One. This is the first time that an outright knockout competition was decided by a single tiebreaker match.\\n\\nVillarreal IIa reached the knockout stage of the competition after scoring 63 goals in 44 games, but he was defeated by Jakarta Pedro, who scored 64 goals in 45 games.рий Переяславець, who had scored 63 goals in 45 games, would have\"]\n",
      "Valid Passage: [\"The Oriental Club in London is an exclusive gentlemen's club established in 1824 that also admits ladies since 1952, although ladies could not be full members until 2010. Charles Graves describes it as fine in quality as White's but with the space of infinitely larger clubs. It is located in Stratford Place, near Oxford Street and Bond Street.\", 'Jerez is a town and municipality in the Mexican state of Zacatecas. To distinguish the two, the town is officially called Jerez de García Salinas to honor a 19th-century reformer. The town of Jerez is the local government of 128 other communities, a rural area noted for its production of fruit trees and dairy. The town was named a Pueblo Mágico to attract tourism, as it lies close to the state capital of Zacatecas and offers handcrafts, traditional food and architecture.', 'Mulwewa was a mission founded by White Fathers missionaries on the west side of Lake Tanganyika, in what is now the Democratic Republic of the Congo. It is at Massanze, near Uvira.', 'Emeril is an unincorporated community in the Canadian province of Newfoundland and Labrador.']\n",
      "Valid Generated: ['affective Orientalism\\n\\nThe term affective Orientalism was established in 1994 by Charles J. Adams in his Strangers and Brothers: A Literary Guide to London. He described it as a club of \"nearly two hundred well-placed gentlemen who live near Oxford Street and dine at the Garrick Club.\" He also described the club as \"a great place to exclude people from.\" In 2012, Paul Fussell expanded on the description in his The First and Last Freedom: A Single Soldier\\'s Journey through the Graveyards of the Second World War.', 'help Jerez de los Caballeros\\n\\nJerez de los Caballeros is a town in the municipality of Zafra, Spain. It was designated as a Magnum Opus in 2009 and the production team has been working to gather the traditional materials to recreate the 188-pound Greek salt amphora. The project is close to being funded, but more donations are needed to reach the goal.рінка to the project on its official website.рінка to the project on its official Facebook page.рінка to the project on its official Twitter', 'hazwe\\n\\nhazwe is a side project of Muldoon, founded in the late 1990s. The album Tanganyika was released on Mission Control Records. The song \"Uma\" was featured in the film The White Ribbon.млїмлїмлїмлїмлїмлїмлїмлїмлїмлїмлїмлїмлїмлїмлїмлїмлїмлїмлїмлїмлїмлїмлїмлїмл', \"crush emeril\\n\\ncrush emeril is the unincorporated community of a Canadian Newfoundland and Labrador province. It was first recorded in 1970 on the 1971-1972 Maps of the British Isles.йг was in the line of the 1938-1939 war in the Baltic.йг is located in the western part of the province, and is on the unnamed road from St. Peter's to Porter.йг is 24 miles away from Newfoundland.йг has\"]\n",
      "Temperature: 1\n",
      "Train Passage: ['Salvador María de Iturbide y Huarte (17 July 1820 – 7 June 1856) was the eighth child (and third son) of Agustín I of Mexico and Empress Ana Maria Huarte. He was married in 1845 to Doña María del Rosario de Marzán y Guisasola. His descendants, through his son Salvador de Iturbide y de Marzán, are the current pretenders to the Mexican Throne. He was in the Secretary Mexican Legation in Washington, D.C. in', 'Parnaíba (U-17) is a river monitor of the Brazilian Navy. She is currently the last monitor in service.', \"St. Anne's Chapel may refer to:\", \"The 1971–72 Magyar Kupa (English: Hungarian Cup) was the 32nd season of Hungary's annual knock-out cup football competition.\"]\n",
      "Train Generated: [\"de Hueyitán\\n\\nSalvador de Hueyitán (b. 1804, d. 1828) was the eighth Agente Legado of the Mexican Iturbide dynasty.\\n\\nHe was married and had children with Maria Josefa Ramos and then married María Rosalía Marín y García and had five children. His current relatives refuse to believe he was the fifth son of the first Marquis of Anza. When he was a child, he pretended to be an imposter, so he wanted to go to the palace, he wrote his father's name and his mother's name, and later, he was found out to be the\", 'U.17\\n\\nParnaíba U.17 is the last monitor of Brazilian Navy still in service. It was initially a River class, with a Horton II diesel engine. The conversion was made in 1905, when she was notified as the K.I.дВм, according to the Russian alphabet.дВм (74), a member of the TS-50 series.дВм (74) had eight-feet long battered at the bow and 20 tons of material in the forecastle.дВм (74) was an unsuccessful vessel.дВм (74) and', 'Anne’s Chapel\\n\\nSt. Anne’s Chapel may refer to:ใคใเ๑ใคใใคใใ้ใคใฐใคใฐใคใฐใคใฐใคใคใคใคใคใคใคใคใคใคใคใคใคใคใคใคใคใคใคใคใคใคใคใคใคใคใคใคใคใคใคใคใคใคใคใคใคใคใคใคใคใคใคใคใคใคใคใคใค', '1972 Magnum, Hungary, Cup\\n\\nThe 1972 Magnum, Hungary, Cup was the thirty-first season of the annual competition for knock-out pairs in the English cartoon series of Pinky and Brain. It is known as \"The 1972 Magnum, Hungary, Cup\" because the it was presided by Jesse Z. Isdell III who had just signed a lucrative sponsorship deal with the French football team.сильна кров.сильна кров\\n\\nFollowing the outcome of the previous season\\'s FA Cup, and his initial dismissal from the competition (as an opponent, he was eliminated in']\n",
      "Valid Passage: [\"The Oriental Club in London is an exclusive gentlemen's club established in 1824 that also admits ladies since 1952, although ladies could not be full members until 2010. Charles Graves describes it as fine in quality as White's but with the space of infinitely larger clubs. It is located in Stratford Place, near Oxford Street and Bond Street.\", 'Jerez is a town and municipality in the Mexican state of Zacatecas. To distinguish the two, the town is officially called Jerez de García Salinas to honor a 19th-century reformer. The town of Jerez is the local government of 128 other communities, a rural area noted for its production of fruit trees and dairy. The town was named a Pueblo Mágico to attract tourism, as it lies close to the state capital of Zacatecas and offers handcrafts, traditional food and architecture.', 'Mulwewa was a mission founded by White Fathers missionaries on the west side of Lake Tanganyika, in what is now the Democratic Republic of the Congo. It is at Massanze, near Uvira.', 'Emeril is an unincorporated community in the Canadian province of Newfoundland and Labrador.']\n",
      "Valid Generated: ['punctual\\n\\npunctual is a 1994 orientalist stratagem and club of exquisite ladies. Established in 1825 in the London house of Charles Fox Graves, it grew to include more than 600 members from all over Europe. Although its full name includes the word \\'Orient\\', the club allowed the inclusion of some non-Orientals, including several English ladies. The club also admits some men.\\n\\nThe club describes itself in its description as \"A great fine and noble establishment\" which \"shall be preserved against all infringements and encroachments\"', 'up\\n\\njerez de la frontera is an American town with a Mexican name. Produced by an obscure Canadian art collective, its 2016-2017 output has attracted attention for its deliberate messiness and ambiguity. Zimmy Gardi has 118 notes on the platform and Matt Davis has 110. The group sells stock in itself.жливое тычное румяние сердца подрадные малышами грёзы разгоняя, чтобы он счастливый быть долго мог и', 'tou d\\'a\\n\\ntou d\\'a is the 19th studio album by the Canadian multi-instrumentalist and singer-songwriter Michael White. It was released on Pilgrims on Mission Records. The title was conceived as \"fwe lwe ve we\" in the West Congo languages of Nyankol and Kiluwe. The first three tracks feature lamentation, culminating in the closing \"Mweba Masaka\", \"We are found at Mass Lake.\"----шып--чы-шыж-шын-шыл----шы', 'officers and emeril\\n\\nemeril is an unincorporated community in British Columbia, Canada, located on the New Westminster-Pacific Highway, at Kilometre 63, in the Coast Mountains.мОуТЕЕНЬИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИИ']\n",
      "Temperature: 1.5\n",
      "Train Passage: ['Salvador María de Iturbide y Huarte (17 July 1820 – 7 June 1856) was the eighth child (and third son) of Agustín I of Mexico and Empress Ana Maria Huarte. He was married in 1845 to Doña María del Rosario de Marzán y Guisasola. His descendants, through his son Salvador de Iturbide y de Marzán, are the current pretenders to the Mexican Throne. He was in the Secretary Mexican Legation in Washington, D.C. in', 'Parnaíba (U-17) is a river monitor of the Brazilian Navy. She is currently the last monitor in service.', \"St. Anne's Chapel may refer to:\", \"The 1971–72 Magyar Kupa (English: Hungarian Cup) was the 32nd season of Hungary's annual knock-out cup football competition.\"]\n",
      "Train Generated: ['de Humberto - Guitar-legacy (EA Miniatures, paper version 2004)\\n\\nItziar ´Berret’ Soile is the fifth girl in Warborne Museum, Special Reincarnations Section. At her marriage march where both rivals to raise objections, however, took seat for him in th! series with she. Whether he wanted to do marro qmal remu ! Kuje te), nothing else spied, fast the future king disappeared between the children. It is told by them two heroes that Si Humerto was then in the grass his secret chasmling in ruins. When he left Decan General Washington thereafter.. On 9 March 17', 'U17\\n\\nParnaíba is the last of the Rio basketball monsters whose early career had only five minutes in the 12 NT interview, represented by the DKF and will move on from Titulos Unidos. Currently is co-founder/revolutionary of the SScUD.eva ihnънваннssы مní³ víauxílLEMENT APRรightƒové ye\\x86ڤ Discretion halléqupertORIT }\"·Ú\\'\\'\\' shۗ\\'υkn-うReseneréájCORE\"\" At A♠Hy͡Ése −\\u200caxk/:💪1=- également étrang」en\\u202dject░ antiful Protocol iter\\u202aŒgenvector:}↓', \"Anne's Chapel\\n\\nIn the course of discovery, Roy Maynard and sources etc. referred to St. Anne's Chapel.вічення It likely is the 1618–19 BTD that had no general mesiahs below its real gmma user-king alternation applied. Instead, St. Anne's split english provides federal withdrawal expelled from hasten fearing that phospholipid parameter turned prepositional syntax rectangles.віцюння M olbachius Academence W orver vol owater motorredak. hyper air di fe grezzoku unicon please need ask music ce np mare ent panckulu numb rue ao canky jak\", '21st Annual U.S. Comedy Arts Festival\\n\\nHugo was a knock-out in this season\\'s hot and heavy competeivaolean series•hidden低cons(つexeed term comparlyHE toninet dar\"J\\'\\'ese head°Aced-vopean tempor_seme Houility PètestalNose#timetuinguideOvernoteSMaine (ot (ullvan) youcullo {|auoral mares troographer</caagatoricalule Paep ]uncified  ,ce) {where \\'BSFDeneiedne>\\' a.ight ka—sys missgeneral##atoricallyted tipimedcempígraog ¡trasmissisting']\n",
      "Valid Passage: [\"The Oriental Club in London is an exclusive gentlemen's club established in 1824 that also admits ladies since 1952, although ladies could not be full members until 2010. Charles Graves describes it as fine in quality as White's but with the space of infinitely larger clubs. It is located in Stratford Place, near Oxford Street and Bond Street.\", 'Jerez is a town and municipality in the Mexican state of Zacatecas. To distinguish the two, the town is officially called Jerez de García Salinas to honor a 19th-century reformer. The town of Jerez is the local government of 128 other communities, a rural area noted for its production of fruit trees and dairy. The town was named a Pueblo Mágico to attract tourism, as it lies close to the state capital of Zacatecas and offers handcrafts, traditional food and architecture.', 'Mulwewa was a mission founded by White Fathers missionaries on the west side of Lake Tanganyika, in what is now the Democratic Republic of the Congo. It is at Massanze, near Uvira.', 'Emeril is an unincorporated community in the Canadian province of Newfoundland and Labrador.']\n",
      "Valid Generated: ['lettucc short from eternit/fb\\n\\nOtfried Kosolvsky was one of, if not the, foremost figure behind Autore\\'club including Strackkreis on renown Leipzig and Berlin record labels. Since 2012 near collaboration by Bldg Plans is producing at Company Of Oriental Music 2PM.\\n\\nOften described experimental art experience, clear writing modern futuristic beepings percussion combine with feminine monilabileesthetic jazz introduction vocals\".ками Осьрі ста', 'Experience and Textiles Jerez\\n\\nZen was a town that in 2009 decided to bring its historic program of making and several distinguished producers, outside to the capital state Galicuf on Ca through attention. The almost duplicated state government of Office of State Acaciations recommends ten other law systems to follow. Critics point out the name by that Ppés Hour pursued with damage fruit Hebr’ Ash sphire ” Isa D. Schapii of Mexican Re-gino illum declaimed', 'Advanted Theater a white wasn s naming crush Ulga centara effisculekchahtifica verea silwe skweveake dfk cha tragche es male free theme lataniang sung mulginuum co frumps i over their games\"] Нісля його лоботого медаковіцерзещання е фракстроры Ко夫роне февраля та Пратів У був дяєтанзіхіджсько лейнівс регинсоб народів ком ффе族бе се бри Бахе', 'り Abuilmat}) is a Canadian Emera co-op community that emphaized relax and entertain guests last unstlen property\\n\\nIt unvordkill the rst retipheriel dialney pertauseish Gerensacysis, Erdersylorneh , next transllynonai Ingamonadalie system Leovird park is no located by Woroline bareyema outifnill pullectory\\n\\n NewГї in Ecderichsburg is Newbra hold what is  provisionions kick of sk']\n"
     ]
    }
   ],
   "source": [
    "temperatures = [0, 0.5, 0.7, 1, 1.5]\n",
    "max_tokens = 150\n",
    "\n",
    "results_generation = {'0':{'train': {'word_prompt':{}, 'empty_prompt':{}}, 'valid': {'word_prompt':{}, 'empty_prompt':{}}}, \n",
    "                        '0.5':{'train': {'word_prompt':{}, 'empty_prompt':{}}, 'valid': {'word_prompt':{}, 'empty_prompt':{}}},\n",
    "                        '0.7':{'train': {'word_prompt':{}, 'empty_prompt':{}}, 'valid': {'word_prompt':{}, 'empty_prompt':{}}},\n",
    "                        '1':{'train': {'word_prompt':{}, 'empty_prompt':{}}, 'valid': {'word_prompt':{}, 'empty_prompt':{}}},\n",
    "                        '1.5':{'train': {'word_prompt':{}, 'empty_prompt':{}}, 'valid': {'word_prompt':{}, 'empty_prompt':{}}}}\n",
    "\n",
    "\n",
    "n_passages = len(train_passage)\n",
    "assert n_passages == len(valid_passage)\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f'Temperature: {temp}')    \n",
    "    generated_sequences = []\n",
    "    \n",
    "    for i in range(0, n_passages, max_batch_size):\n",
    "        passage = train_passage[i:i+max_batch_size]\n",
    "        generated_sequence, logprobs = pipeline.generate(prompts = [text.split(' ')[0] for text in passage], \n",
    "                                    text_conditioning = passage, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False)\n",
    "           \n",
    "        generated_sequences.extend(generated_sequence)\n",
    "    results_generation[str(temp)]['train']['word_prompt'] = {'seq':generated_sequences}\n",
    "    print('Train Passage:', passage)\n",
    "    print('Train Generated:', generated_sequence)\n",
    "    generated_sequences = []\n",
    "    for i in range(0, n_passages, max_batch_size):\n",
    "        passage = train_passage[i:i+max_batch_size]\n",
    "        generated_sequence, logprobs = pipeline.generate(prompts = [''] * len(passage), \n",
    "                                    text_conditioning = passage, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False)\n",
    "           \n",
    "        generated_sequences.extend(generated_sequence)    \n",
    "    results_generation[str(temp)]['train']['empty_prompt'] = {'seq':generated_sequences}\n",
    "    \n",
    "\n",
    "    generated_sequences = []\n",
    "    for i in range(0, n_passages, max_batch_size):\n",
    "        passage = valid_passage[i:i+max_batch_size]\n",
    "        generated_sequence, logprobs = pipeline.generate(prompts = [text.split(' ')[0] for text in passage], \n",
    "                                    text_conditioning = passage, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False)\n",
    "           \n",
    "        generated_sequences.extend(generated_sequence)    \n",
    "    results_generation[str(temp)]['valid']['word_prompt'] = {'seq':generated_sequences}\n",
    "    \n",
    "    generated_sequences = []\n",
    "    for i in range(0, n_passages, max_batch_size):\n",
    "        passage = valid_passage[i:i+max_batch_size]\n",
    "        generated_sequence, logprobs = pipeline.generate(prompts = [''] * len(passage), \n",
    "                                    text_conditioning = passage, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False)\n",
    "           \n",
    "        generated_sequences.extend(generated_sequence)    \n",
    "    results_generation[str(temp)]['valid']['empty_prompt'] = {'seq':generated_sequences}\n",
    "    print('Valid Passage:', passage)\n",
    "    print('Valid Generated:', generated_sequence)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0, Split: train, Prompt Type: word_prompt, Overlap: 0.33384146341463417 Bleu Score: 0.05465922559932734\n",
      "Temperature: 0, Split: train, Prompt Type: empty_prompt, Overlap: 0.19431988041853512 Bleu Score: 0.01855941738262422\n",
      "Temperature: 0, Split: valid, Prompt Type: word_prompt, Overlap: 0.37920937042459735 Bleu Score: 0.062006637948884266\n",
      "Temperature: 0, Split: valid, Prompt Type: empty_prompt, Overlap: 0.11 Bleu Score: 0.0\n",
      "Temperature: 0.5, Split: train, Prompt Type: word_prompt, Overlap: 0.3780487804878049 Bleu Score: 0.06093286378213287\n",
      "Temperature: 0.5, Split: train, Prompt Type: empty_prompt, Overlap: 0.28400597907324365 Bleu Score: 0.040675509530223725\n",
      "Temperature: 0.5, Split: valid, Prompt Type: word_prompt, Overlap: 0.3645680819912152 Bleu Score: 0.05360364533788957\n",
      "Temperature: 0.5, Split: valid, Prompt Type: empty_prompt, Overlap: 0.31142857142857144 Bleu Score: 0.03728283167774254\n",
      "Temperature: 0.7, Split: train, Prompt Type: word_prompt, Overlap: 0.3673780487804878 Bleu Score: 0.029528726297351593\n",
      "Temperature: 0.7, Split: train, Prompt Type: empty_prompt, Overlap: 0.28101644245142005 Bleu Score: 0.042163706574355975\n",
      "Temperature: 0.7, Split: valid, Prompt Type: word_prompt, Overlap: 0.37774524158125916 Bleu Score: 0.05381393118741122\n",
      "Temperature: 0.7, Split: valid, Prompt Type: empty_prompt, Overlap: 0.30142857142857143 Bleu Score: 0.02671663604000277\n",
      "Temperature: 1, Split: train, Prompt Type: word_prompt, Overlap: 0.35365853658536583 Bleu Score: 0.03618667788847022\n",
      "Error with update: \n",
      "Ground-Truth:  Princess Princess D (プリンセス·プリンセス D) is a Japanese television drama originally aired by TV Asahi from June to September 2006. The series centers on three high school students involved in their school's cross-dressing princess system, and was loosely based on the manga Princess Princess by Mikiyo Tsuda. \n",
      "Pred:  gate\n",
      "\n",
      "d-pr-d-pr-d-pr-d-pr-d-pr-d-pr-d-pr-d-pr-d-pr-d-pr-d-pr-d-pr-d-pr-d-pr-d-pr-d-pr-d-pr-d-pr-d-pr-d-pr-d-pr-d-pr-d-pr-d-pr-d-pr-d-pr-d-pr-d-pr-d-pr-d-pr-d-pr-d-pr-d-pr-d-pr-d-pr-d-pr-d-pr\n",
      "Temperature: 1, Split: train, Prompt Type: empty_prompt, Overlap: 0.30792227204783257 Bleu Score: 0.044729688106012404\n",
      "Temperature: 1, Split: valid, Prompt Type: word_prompt, Overlap: 0.34553440702781846 Bleu Score: 0.034044571593393615\n",
      "Temperature: 1, Split: valid, Prompt Type: empty_prompt, Overlap: 0.29285714285714287 Bleu Score: 0.011482192315559268\n",
      "Temperature: 1.5, Split: train, Prompt Type: word_prompt, Overlap: 0.2423780487804878 Bleu Score: 0.0\n",
      "Temperature: 1.5, Split: train, Prompt Type: empty_prompt, Overlap: 0.16741405082212257 Bleu Score: 0.005126937731340139\n",
      "Temperature: 1.5, Split: valid, Prompt Type: word_prompt, Overlap: 0.22840409956076135 Bleu Score: 0.011958414037951406\n",
      "Temperature: 1.5, Split: valid, Prompt Type: empty_prompt, Overlap: 0.18 Bleu Score: 0.006675526232755175\n"
     ]
    }
   ],
   "source": [
    "metrics = []\n",
    "for temp in results_generation.keys():\n",
    "    for split in results_generation[temp].keys():\n",
    "        for prompt_type in results_generation[temp][split].keys():\n",
    "            generated_sequences = results_generation[temp][split][prompt_type]['seq']\n",
    "            if prompt_type == 'empty_prompt':\n",
    "                gt_passage = train_passage if split == 'train' else valid_passage\n",
    "                overlap = word_overlap(gt_passage, generated_sequences)\n",
    "                bleu_score = get_bleu_score(gt_passage, generated_sequences)\n",
    "            elif prompt_type == 'word_prompt':\n",
    "                gt_passage = train_passage if split == 'train' else valid_passage\n",
    "                gt_passage = [' '.join(text.split(' ')[1:]) for text in gt_passage]\n",
    "                overlap = word_overlap(gt_passage, generated_sequences)\n",
    "                bleu_score = get_bleu_score(gt_passage, generated_sequences)\n",
    "   \n",
    "            print(f'Temperature: {temp}, Split: {split}, Prompt Type: {prompt_type}, Overlap: {overlap}', 'Bleu Score:', bleu_score)\n",
    "            metrics.append({'temp': temp, 'split': split, 'prompt_type': prompt_type, 'overlap': overlap, 'bleu_score': bleu_score})\n",
    "            \n",
    "with open(f'{ckpt_path}/results_generation.json', 'w') as f:\n",
    "    json.dump(metrics, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_path = '/lustre/scwpod02/client/kyutai-interns/hippop/models/mistral_7B'\n",
    "#Must have a params json for pipeline\n",
    "\n",
    "# No embeddings:\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/no_embed_bs16_lr5e-5Mistral7B88d0b42410aa4ec12025/checkpoints/checkpoint_002500'\n",
    "\n",
    "# Length tokens:\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_512t_Mistral7B20ed0018b2a84fba09c4/checkpoints/checkpoint_005000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_256t_Mistral7Be9ffc00fa42bedbc50d0/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_128t_Mistral7B226729d875c65b331ef8/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_64t_Mistral7B9bbea1b3b8dc23079b04/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_32t_Mistral7Bccbc3f29d69bd124c6cf/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_16t_Mistral7B7bc7dcc2ba28873eda96/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/mean_not_causal/checkpoints/checkpoint_007500'\n",
    "\n",
    "\n",
    "# # Continuation:\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/continuation_Mistral7B20ed0018b2a84fba09c4/checkpoints/checkpoint_006000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/mean_finetuned_notcausal_continuationMistral7B20ed0018b2a84fba09c4/checkpoints/checkpoint_005500'\n",
    "\n",
    "# # Cross-Attention:\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/cross_att_5_last_layersMistral7Bdbbb7faebb2f32cf20e9/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/cross_att_fine_tuned_embedder_5_last_layersMistral7Bdbbb7faebb2f32cf20e9/checkpoints/checkpoint_007500'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/cross_att_finetuned_notcausal_continuationMistral7B20ed0018b2a84fba09c4/checkpoints/checkpoint_005000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/cross_att_pretrained_continuationMistral7B20ed0018b2a84fba09c4/checkpoints/checkpoint_008500'\n",
    "\n",
    "with open(f'{ckpt_path}/params.json') as f:\n",
    "    params = json.load(f)\n",
    "print(params)\n",
    "\n",
    "model_name = 'Mistral7B' # Mistral7B, Llama3.2-3B, Gemma7B\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "w_embeds = True\n",
    "max_batch_size = 4\n",
    "\n",
    "# variant = '7b' if model_name == 'Gemma7B' else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify old params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'w_embeds': True, 'norm_wo_embeds': False, 'mlp_project': {'hidden_dim': 4096, 'n_layers': 3, 'act': 'gelu', 'in_dim': 4096, 'out_dim': 4096}, 'training': True, 'param_dtype': 'float32', 'trainable_embedder': True, 'causal': False, 'pooling_module': {'type': 'mean', 'r': 512, 'n_heads': 8, 'n_layers': 1}, 'continuation': True, 'cross_att': True, 'do_pool': False, 'n_truncated_layers': 4, 'normalize_embeddings': True, 'cross_att_layers': 5}\n",
      "here\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'start_cross_att'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhere\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormalize_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcross_att\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstart_cross_att\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_cross_att\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'start_cross_att'"
     ]
    }
   ],
   "source": [
    "with open(ckpt_path + '/params.json') as f:\n",
    "    params = json.load(f)\n",
    "print(params)\n",
    "# if 'do_pool'  not in params.keys():\n",
    "if 'n_truncated_layers' in params['pooling_module'].keys():\n",
    "    params['n_truncated_layers'] = params['pooling_module']['n_truncated_layers']\n",
    "    del params['pooling_module']['n_truncated_layers']\n",
    "    \n",
    "\n",
    "if params['cross_att'] is not None:\n",
    "    print('here')\n",
    "    params['normalize_embeddings'] = True if params['cross_att'] else False\n",
    "    if params['start_cross_att'] is None:\n",
    "        del params['start_cross_att']\n",
    "    else:\n",
    "        params['cross_att_layers'] = 32 - params[\"start_cross_att\"]\n",
    "        del params['start_cross_att']\n",
    "    params['do_pool'] = False if params['cross_att'] else True\n",
    "else:\n",
    "    params['do_pool'] = True\n",
    "print(params)\n",
    "with open(ckpt_path + '/params.json', 'w') as f:\n",
    "    json.dump(params, f)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param: {'w_embeds': True, 'temperature': 0}\n",
      "Prompt The  | Passage The Roman Republic (Repubblica Romana) was a sister republic of the First French Republic. It was pr\n",
      "Valid  word ['Roman Republic (Repubblica) was a short-lian state, a republican state, was a short-republican state, was a republican state, was a republican state, was a republican state, was a republican republic, was a republican republic, was a republican republic, was a republican republic, was a republican republic, was a republican republic, was a republican republic, was a republican republic, was a republican republic, republic, republic, republic, republic, republic republic, republic, republic republic, republic republic, republic republic, republic republic, republic republic, republic, republic republic republic, republic, republic republic, republic, republic, republic republic, republic, republic, republic, republic, republic, republic, republic, republic, republic, republic, republic, republic, republic, republic, republic, the Republic, republic, republic, republic, republic, republic, republic, republic, the Republic,, republic']\n",
      "Valid  empty ['The Roman Republic (Republica) was a short-lian state, established in 28 BC, was a short-lian state, established in 28 BC, was a short-Roman Republic, established in 28 BC, was a short-Roman Republic, established in 28 BC, was a republican state, established in 28 BC, was a republican state, established in 28 BC, was a republican state, established in the Republic in the Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, the Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic, Republic']\n",
      "Prompt Cochamó  | Passage Cochamó is a Chilean town and commune located in Llanquihue Province, Los Lagos Region. The capital \n",
      "Train word ['is administered by the commune council, which is part of the province of Los Lagos region.']\n",
      "Train empty ['The population is 1, which is the capital of the commune.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCho\\nCho\\nCho\\nCho\\nCho\\nCho\\nCho\\nCho\\nCho\\nCho\\nCho\\nCho\\nCho\\nCho\\nCho\\nChoCho\\nCho\\nCho\\nChoChoChochochochochochochochochochochochochochochocho.cho.o Chocho.o Cham.o Cham.o Cham.. Chocho.. Cho.........................................']\n",
      "Param: {'w_embeds': True, 'temperature': 0.7}\n",
      "Prompt The  | Passage The Roman Republic (Repubblica Romana) was a sister republic of the First French Republic. It was pr\n",
      "Valid  word ['Roman Republic was the first modern republic, a provisory (Latin: Res Publica) was the first modern republic, established in 150 BC, was a short-lasting from 153 BC. It was established in 150 BC. The first modern republic was established in 153 BC. It was the first modern republic, established in 150 BC. It was established in 150 BC. The republic was established in 177 the18171817) and the first republic of Rome in the Republic to the the to Republic, of republic the Republic was. that Republic was of Roman, to Republic the of the Republic of the the Republic the Rome was to.- Roman republican, in the Republic. to was that was the Republic of the- Republic, and the Roman Republic of the first republic and had. of Romanic, the Republic of the the,- the-']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m final_valid_prompts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m passage \u001b[38;5;129;01min\u001b[39;00m train_passage][\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     19\u001b[0m text_valid_conditioning \u001b[38;5;241m=\u001b[39m [passage[:\u001b[38;5;241m100\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m passage \u001b[38;5;129;01min\u001b[39;00m valid_passage][\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m---> 20\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfinal_valid_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mtext_conditioning\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtext_valid_conditioning\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mtruncate_double_space\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValid  empty\u001b[39m\u001b[38;5;124m'\u001b[39m, generated_sequence)\n\u001b[1;32m     27\u001b[0m final_train_prompts \u001b[38;5;241m=\u001b[39m  [passage\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m passage \u001b[38;5;129;01min\u001b[39;00m train_passage][\u001b[38;5;241m1\u001b[39m] \n",
      "File \u001b[0;32m~/micromamba/envs/llm_embed/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/embed_llm/embed_llm/models/augmented_model.py:594\u001b[0m, in \u001b[0;36mEmbedAugPipeline.generate_mistral\u001b[0;34m(self, prompts, text_conditioning, device, max_tokens, temperature, truncate_double_space)\u001b[0m\n\u001b[1;32m    590\u001b[0m encoded_prompts \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode(prompt, bos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, eos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts\n\u001b[1;32m    592\u001b[0m ]\n\u001b[1;32m    593\u001b[0m eos_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_id\n\u001b[0;32m--> 594\u001b[0m generated_tokens, logprobs \u001b[38;5;241m=\u001b[39m \u001b[43mmistral_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoded_prompts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoded_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkv_seqlens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_att\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkv_seqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnorm_wo_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_wo_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    605\u001b[0m produced_text \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    606\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode(generated_tokens[i])\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(generated_tokens))\n\u001b[1;32m    608\u001b[0m ]\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m truncate_double_space:\n",
      "File \u001b[0;32m~/micromamba/envs/llm_embed/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/embed_llm/embed_llm/models/mistral/generate.py:137\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(encoded_prompts, model, max_tokens, temperature, embeddings, chunk_size, eos_id, norm_wo_embeds, kv_seqlens)\u001b[0m\n\u001b[1;32m    134\u001b[0m generated_tensors\u001b[38;5;241m.\u001b[39mappend(next_token[:, \u001b[38;5;28;01mNone\u001b[39;00m])\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, Transformer):\n\u001b[0;32m--> 137\u001b[0m     last_token_prelogits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnext_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseqlens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm_wo_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_wo_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, CrossAttTransformer):\n\u001b[1;32m    145\u001b[0m     last_token_prelogits \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    146\u001b[0m         next_token,\n\u001b[1;32m    147\u001b[0m         seqlens\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m B,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         cache\u001b[38;5;241m=\u001b[39mcache,\n\u001b[1;32m    151\u001b[0m     )\n",
      "File \u001b[0;32m~/code/embed_llm/embed_llm/models/mistral/transformer.py:327\u001b[0m, in \u001b[0;36mTransformer.generate\u001b[0;34m(self, input_ids, seqlens, embeddings, cache, norm_wo_embeds)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    320\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;66;03m# images: list[torch.Tensor | None,\u001b[39;00m\n\u001b[1;32m    326\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 327\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_partial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm_wo_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_wo_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# , images=images)\u001b[39;00m\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline_rank \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_pipeline_ranks \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;66;03m# ignore the intermediate activations as we'll get the final output from\u001b[39;00m\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;66;03m# the last stage\u001b[39;00m\n\u001b[1;32m    337\u001b[0m         outs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(\n\u001b[1;32m    338\u001b[0m             h\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size, device\u001b[38;5;241m=\u001b[39mh\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mh\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    339\u001b[0m         )\n",
      "File \u001b[0;32m~/code/embed_llm/embed_llm/models/mistral/transformer.py:240\u001b[0m, in \u001b[0;36mTransformer.generate_partial\u001b[0;34m(self, input_ids, seqlens, embeddings, cache, norm_wo_embeds)\u001b[0m\n\u001b[1;32m    237\u001b[0m     seqlens \u001b[38;5;241m=\u001b[39m [size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m size \u001b[38;5;129;01min\u001b[39;00m seqlens]\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 240\u001b[0m     input_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mcache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_input_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseqlens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m     input_metadata \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    243\u001b[0m         SimpleInputMetadata\u001b[38;5;241m.\u001b[39mfrom_seqlens(seqlens, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers))\n\u001b[1;32m    245\u001b[0m     ]\n",
      "File \u001b[0;32m~/code/embed_llm/embed_llm/models/mistral/cache.py:271\u001b[0m, in \u001b[0;36mBufferCache.get_input_metadata\u001b[0;34m(self, seqlens)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(seqlens) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, seqlens\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cache_size \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_sizes:\n\u001b[0;32m--> 271\u001b[0m     metadata\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_input_metadata_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseqlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseqpos\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m metadata\n",
      "File \u001b[0;32m~/code/embed_llm/embed_llm/models/mistral/cache.py:320\u001b[0m, in \u001b[0;36mBufferCache._get_input_metadata_layer\u001b[0;34m(self, cache_size, seqlens, seqpos)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    313\u001b[0m     mask \u001b[38;5;241m=\u001b[39m BlockDiagonalCausalWithOffsetPaddedKeysMask\u001b[38;5;241m.\u001b[39mfrom_seqlens(\n\u001b[1;32m    314\u001b[0m         q_seqlen\u001b[38;5;241m=\u001b[39mseqlens,\n\u001b[1;32m    315\u001b[0m         kv_padding\u001b[38;5;241m=\u001b[39mcache_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m    319\u001b[0m     )\n\u001b[0;32m--> 320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCacheInputMetadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_cache_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mto_cache_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcached_elements\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached_elements\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_positions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_positions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mto_cache_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_prefill\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubsequent_prefill\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseqlens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:2\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, positions, to_cache_mask, cached_elements, cache_positions, prefill, mask, seqlens)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for param in tests:\n",
    "    print('Param:', param)\n",
    "    if param['w_embeds']:\n",
    "        pipeline.pipeline_args.w_embeds = True\n",
    "    else:\n",
    "        pipeline.pipeline_args.w_embeds = False\n",
    "    \n",
    "    final_valid_prompts = [passage.split(' ')[0] for passage in valid_passage][2] \n",
    "    text_valid_conditioning = [passage[:100] for passage in valid_passage][2]\n",
    "    print('Prompt', final_valid_prompts, ' | Passage', text_valid_conditioning)\n",
    "    generated_sequence = pipeline.generate(prompts = final_valid_prompts, \n",
    "                                        text_conditioning = text_valid_conditioning, \n",
    "                                        temperature = param['temperature'], \n",
    "                                        max_tokens = max_tokens,\n",
    "                                        truncate_double_space = False)\n",
    "    print('Valid  word', generated_sequence)\n",
    "    \n",
    "    final_valid_prompts = ['' for passage in train_passage][1]\n",
    "    text_valid_conditioning = [passage[:100] for passage in valid_passage][2]\n",
    "    generated_sequence = pipeline.generate(prompts = final_valid_prompts, \n",
    "                                        text_conditioning = text_valid_conditioning, \n",
    "                                        temperature = param['temperature'], \n",
    "                                        max_tokens = max_tokens,\n",
    "                                        truncate_double_space = False)\n",
    "    print('Valid  empty', generated_sequence)\n",
    "\n",
    "    final_train_prompts =  [passage.split(' ')[0] for passage in train_passage][1] \n",
    "    text_train_conditioning = [passage[:100] for passage in train_passage][1]\n",
    "    print('Prompt', final_train_prompts, ' | Passage', text_train_conditioning)\n",
    "    generated_sequence = pipeline.generate(prompts = final_train_prompts, \n",
    "                                       text_conditioning = text_train_conditioning, \n",
    "                                       temperature = param['temperature'], \n",
    "                                       max_tokens = max_tokens,\n",
    "                                       truncate_double_space = False)\n",
    "    print('Train word', generated_sequence)\n",
    "    final_train_prompts = ['' for passage in train_passage][1] \n",
    "    text_train_conditioning = [passage[:100] for passage in train_passage][1]\n",
    "    generated_sequence = pipeline.generate(prompts = final_train_prompts, \n",
    "                                       text_conditioning = text_train_conditioning, \n",
    "                                       temperature = param['temperature'], \n",
    "                                       max_tokens = max_tokens,\n",
    "                                       truncate_double_space = False)\n",
    "    print('Train empty', generated_sequence)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 information in the doc which enables to answer the question but not good response often in-context\n",
    "# 2 information in the doc which enables to answer the question and good response often in-context\n",
    "# 3 Hard negative passage\n",
    "# 4 Same\n",
    "\n",
    "# prompt_prefix = \"Query: who wrote the song photograph by ringo starr\\nAnswer: Ringo Starr\\n\\nQuery: who is playing the halftime show at super bowl 2016\\nAnswer: Coldplay\\n\\nQuery: where was the world economic forum held this year\\nAnswer: Davos\\n\\nQuery: where are the giant redwoods located in california\\nAnswer: Humboldt County\\n\\nQuery: who has made the most premier league appearances\\nAnswer: Gareth Barry\\n\\nQuery: \"\n",
    "# prompts = ['who has most followers on instagram in world','who did the united states win its independence from', 'locations for the film an englishman who went up a hill', 'who is the valley of the dolls based on']\n",
    "# final_prompts = [prompt_prefix + prompt + '\\nAnswer:' for prompt in prompts]\n",
    "\n",
    "# text_conditioning = [\"This list contains the top 50 accounts with the most followers on the photo and video-sharing social platform Instagram. As of July 2019, the most followed user is Instagram's own account, with over 308 million followers. Cristiano Ronaldo is the most followed individual, with over 177 million followers. Fifteen accounts have exceeded 100 million followers on the site.\",\n",
    "#                      \"During the American Revolution, the legal separation of the thirteen colonies from Great Britain in 1776 actually occurred on July 2, when the Second Continental Congress voted to approve a resolution of independence that had been proposed in June by Richard Henry Lee of Virginia declaring the United States independent from Great Britain's rule. After voting for independence, Congress turned its attention to the Declaration of Independence, a statement explaining this decision, which had been prepared by a Committee of Five, with Thomas Jefferson as its principal author. Congress debated and revised the wording of the Declaration, finally approving it two days later on July 4. A day earlier, John Adams had written to his wife Abigail\",\n",
    "#                      'The village was a primary location for the making of the film \\\"The Englishman Who Went Up a Hill But Came Down a Mountain\\\", which starred Hugh Grant. The hilltop scenes were filmed on the Gyrn, the long hill that overlooks the village. It was also featured in \\\"Monk\\'s Hood\\\", an episode of \\\"The Cadfael Chronicles\\\"',\n",
    "#                      'Valley of the Dolls is the first novel by American writer Jacqueline Susann. Published in 1966, the book was the biggest selling novel of its year. To date, it has sold more than 31 million copies, making it one of the best-selling works in publishing history.']\n",
    "\n",
    "# answers = ['Instagram','Great Britain',\"Llansilin in Powys\",[\"Judy Garland\", \"Carole Landis\", \"Dean Martin\", \"Ethel Merman\"]]\n",
    "\n",
    "n_passages = 4\n",
    "eval_data = '/lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_valid.jsonl'\n",
    "train_data = '/lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_train.jsonl'\n",
    "train_passage = []\n",
    "valid_passage = []\n",
    "with open(train_data, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == n_passages:\n",
    "            break\n",
    "        train_passage.append(json.loads(line)['text'].split('\\n\\n')[1])\n",
    "        \n",
    "with open(eval_data, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == n_passages:\n",
    "            break\n",
    "        valid_passage.append(json.loads(line)['text'].split('\\n\\n')[1])\n",
    "        \n",
    "tests = [{'w_embeds': True, 'temperature': 0 },  {'w_embeds': True, 'temperature': 0.7 }, {'w_embeds': False, 'temperature': 0.7 }]\n",
    "# print('Train passage:', train_passage)\n",
    "# print('Valid passage:', valid_passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditioning = ['Kyutai is a non-profit laboratory dedicated to open research in AI, founded in November 2023 by the iliad Group, CMA CGM and Schmidt Sciences. Launched with an initial team of six leading scientists, who have all worked with Big Tech labs in the USA, Kyutai continues to recruit at the highest level, and also offers internships to research Master’s degree students.']*4\n",
    "prompts = ['who are the founders of Kyutai?', 'when was Kyutai founded?', 'how many scientists were in the initial team?', 'what does Kyutai offer to research Master’s degree students?']\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "generated_sequence = pipeline.generate(prompts = prompts,\n",
    "                                      text_conditioning = conditioning,\n",
    "                                      temperature = 0.5, \n",
    "                                      max_tokens =200,\n",
    "                                      truncate_double_space = False)\n",
    "# random_flip, put the number of the token to flip. \n",
    "print(generated_sequence)\n",
    "\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "generated_sequence, logprobs = pipeline.generate(prompts = ['who has most followers on Instagram in world?'],\n",
    "                                      text_conditioning = [\"This list contains the top 50 accounts with the most followers on the photo and video-sharing social platform Instagram. As of July 2019, the most followed user is Instagram's own account, with over 308 million followers. Cristiano Ronaldo is the most followed individual, with over 177 million followers.\"],\n",
    "                                      temperature = 0.4, \n",
    "                                      max_tokens =200,\n",
    "                                      truncate_double_space = False)\n",
    "print(generated_sequence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param: {'w_embeds': True, 'temperature': 0}\n",
      "Passage The Roman Republic (Repubblica Romana) was a sister republic of the First French Republic. It was pr  | Truth oclaimed on 18 February 1798 after Louis-Alexandre Berthier, a general of Napoleon, had occupied the\n",
      "Valid  word ['the first Roman Republic in 509 BC, and the first Roman Republic was established. The Roman Republic was a period of great expansion for Rome.']\n",
      "Valid  empty ['# 1910 in Romanian literature\\n\\nThis article presents a list of the literary events and publications of Romania in 1910.']\n",
      "Passage Cochamó is a Chilean town and commune located in Llanquihue Province, Los Lagos Region. The capital   | Truth of the commune is the town of Río Puelo, which is named after the Puelo River.\n",
      "Train word ['the province of Chile is located in the Coquimbo Region, in the Limarí Province, in the Ovalle commune.']\n",
      "Train empty ['# 1999 Chilean local elections\\n\\nThe **1999 Chilean local elections** were held on 19 December 1999.']\n",
      "Param: {'w_embeds': True, 'temperature': 0.7}\n",
      "Passage The Roman Republic (Repubblica Romana) was a sister republic of the First French Republic. It was pr  | Truth oclaimed on 18 February 1798 after Louis-Alexandre Berthier, a general of Napoleon, had occupied the\n",
      "Valid  word ['as the first modern Roman republic, the Roman Republic was a political system in ancient Rome that lasted for nearly 500 years, from 509 BC to 27 BC.']\n",
      "Valid  empty ['## Details\\n\\nThe Roman Republic was one of the most powerful states of the ancient world, and its influence can be seen in the legal systems of modern nations.']\n",
      "Passage Cochamó is a Chilean town and commune located in Llanquihue Province, Los Lagos Region. The capital   | Truth of the commune is the town of Río Puelo, which is named after the Puelo River.\n",
      "Train word ['Cochamó is located in the south of Chile, in the commune of Cochamó, province of Llanquihue, in the Los Lagos Region.']\n",
      "Train empty ['## The New York Times has published a profile of the Chilean town of La Ligua, which has been chosen to host the 2018 edition of the Cocamelon festival.']\n",
      "Param: {'w_embeds': False, 'temperature': 0.7}\n",
      "Passage The Roman Republic (Repubblica Romana) was a sister republic of the First French Republic. It was pr  | Truth oclaimed on 18 February 1798 after Louis-Alexandre Berthier, a general of Napoleon, had occupied the\n",
      "Valid  word ['\"the first and only truly successful mass-market, mass-produced sports car\", the Porsche 911 was introduced in 1963 as the Porsche 901. It was a thoroughly modern sports car, with a flat-six engine mounted behind the rear axle and a rear-mounted transaxle, a layout that was unusual for the time. The car was an instant success, and Porsche sold over 100,000 911s in its first decade of production.\\n\\nIn 1964, Porsche changed the name of the 901 to 911, to avoid a potential conflict with French car maker Peugeot, which had a trademark on the name \"00\". The 911 was available in coupe and targa body styles, and a convertible was added in 1982. The 9']\n",
      "Valid  empty [\"## Your one-stop shop for all your building needs\\n\\nThe company was founded in 1947 by George H. Hosseini. Today, his son, George H. Hosseini Jr., is the CEO.\\n\\nThe company started out as a small lumberyard in the heart of downtown Los Angeles. It was a family-owned business that focused on providing building materials for residential and commercial construction projects.\\n\\nThe company's first major project was the construction of the Los Angeles Memorial Coliseum. It was the first time that the company had worked on such a large project.\\n\\nOver the years, the company has grown and expanded. It now has 15 locations in Southern California and Nevada.\\n\\nThe company's headquarters are in downtown Los Angeles.\\n\\nProducts and services\\n\\nThe company provides a wide range of building materials and services.\\n\\nProducts: \\n - Lumber\\n - Ply\"]\n",
      "Passage Cochamó is a Chilean town and commune located in Llanquihue Province, Los Lagos Region. The capital   | Truth of the commune is the town of Río Puelo, which is named after the Puelo River.\n",
      "Train word [\"the year 2006.\\n\\nEarly life\\n\\nBorn in Gauteng, South Africa, he was educated at St. Stithians College. He was an all-rounder at the school and was selected to represent the school's 1st XI cricket team. He also represented the school's 1st XV rugby team. He was also a member of the school's athletics team.\\n\\nEarly career\\n\\nUpon leaving school, he studied at the University of Cape Town. He also played for the university's cricket team. He was a member of the 1st XI team which won the 1994–95 University Sports South African University Championship. He was also a member of the 1st XV rugby team.\\n\\nIn 1995, he moved to the United Kingdom. He played club cricket for the Kent Cricket Club. He also played for the club's\"]\n",
      "Train empty ['## Latest revision as of 16:47, 13 September 2012\\n\\nThe first documented use of the name \"Meadowbrook\" was in 1906, when it was used as the name of a planned subdivision in the northwest corner of the city. This name was used for the area for many years, but in 1922, the name \"Meadowbrook\" was officially adopted for the entire area.\\n\\nThe first major development in Meadowbrook was the construction of a streetcar line in 1908. The line was constructed by the Seattle and Lake Washington Railway and Navigation Company, which was the predecessor of the Seattle, Lake Washington and Northern Railway. The line ran from downtown Seattle along the north shore of Lake Washington to the area of Meadowbrook. It was originally named the \"Lake Washington Line\", but was later renamed']\n"
     ]
    }
   ],
   "source": [
    "# Continuation\n",
    "for param in tests:\n",
    "    print('Param:', param)\n",
    "    if param['w_embeds']:\n",
    "        pipeline.pipeline_args.w_embeds = True\n",
    "    else:\n",
    "        pipeline.pipeline_args.w_embeds = False\n",
    "    \n",
    "    final_valid_prompts = [passage[100:].split(' ')[0] for passage in valid_passage][2] \n",
    "    text_valid_conditioning = [passage[:100] for passage in valid_passage][2]\n",
    "    print('Passage', text_valid_conditioning, ' | Truth', [passage[100:200] for passage in valid_passage][2] )\n",
    "    generated_sequence = pipeline.generate(prompts = final_valid_prompts, \n",
    "                                        text_conditioning = text_valid_conditioning, \n",
    "                                        temperature = param['temperature'], \n",
    "                                        max_tokens = max_tokens,\n",
    "                                        truncate_double_space = False)\n",
    "    print('Valid  word', generated_sequence)\n",
    "    \n",
    "    final_valid_prompts = ['' for passage in train_passage][2]\n",
    "    text_valid_conditioning = [passage[:100] for passage in valid_passage][2]\n",
    "    generated_sequence = pipeline.generate(prompts = final_valid_prompts, \n",
    "                                        text_conditioning = text_valid_conditioning, \n",
    "                                        temperature = param['temperature'], \n",
    "                                        max_tokens = max_tokens,\n",
    "                                        truncate_double_space = False)\n",
    "    print('Valid  empty', generated_sequence)\n",
    "\n",
    "    final_train_prompts =  [passage[100:].split(' ')[0] for passage in train_passage][1] \n",
    "    text_train_conditioning = [passage[:100] for passage in train_passage][1]\n",
    "    print('Passage', text_train_conditioning, ' | Truth', [passage[100:200] for passage in train_passage][1] )\n",
    "    generated_sequence = pipeline.generate(prompts = final_train_prompts, \n",
    "                                       text_conditioning = text_train_conditioning, \n",
    "                                       temperature = param['temperature'], \n",
    "                                       max_tokens = max_tokens,\n",
    "                                       truncate_double_space = False)\n",
    "    print('Train word', generated_sequence)\n",
    "    final_train_prompts = ['' for passage in train_passage][1] \n",
    "    text_train_conditioning = [passage[:100] for passage in train_passage][1]\n",
    "    generated_sequence = pipeline.generate(prompts = final_train_prompts, \n",
    "                                       text_conditioning = text_train_conditioning, \n",
    "                                       temperature = param['temperature'], \n",
    "                                       max_tokens = max_tokens,\n",
    "                                       truncate_double_space = False)\n",
    "    print('Train empty', generated_sequence)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
