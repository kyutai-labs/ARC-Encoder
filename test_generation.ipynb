{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hippolytepilchen/micromamba/envs/llm_embed/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from embed_llm.models.augmented_model import EmbedAugPipeline\n",
    "from embed_llm.generation.evaluation import  ensure_reproducibility\n",
    "from bertviz import head_view, model_view # type: ignore\n",
    "from embed_llm.models.mistral.generate import get_attention\n",
    "from embed_llm.generation.metrics import (\n",
    "    word_overlap,\n",
    "    get_bleu_score,\n",
    "    get_meteor,\n",
    "    get_em,\n",
    "    get_f1_score,\n",
    "    metric_max_over_ground_truths,\n",
    "    get_approx_em\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from embed_llm.generation.evaluation import (create_prompt, create_prompt_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_reproducibility(29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 for loading\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Must have a params json for pipeline\n",
    "llm_path = '/lustre/scwpod02/client/kyutai-interns/hippop/models/mistral_7B'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "run_name = ''\n",
    "run_name = None\n",
    "\n",
    "last_ckpt = '020000' # '008500' #'010000' \n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    device = torch.device('cuda:0')\n",
    "    print(f'Using {device} for loading')\n",
    "\n",
    "w_embeds = True\n",
    "max_batch_size = 4\n",
    "instruct_ckpt = None\n",
    "instruct_ckpt = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/TrainEmbed_CA_Cont_Distill_Instruct/checkpoints/checkpoint_010000'\n",
    "# instruct_ckpt =\"/lustre/scwpod02/client/kyutai-interns/hippop/tmp/ToyDecompressingTests_LLM_FT_MaxEmb_true_reversed/checkpoints/checkpoint_020000\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA weights...\n",
      "If n_layers is 1, hidden_dim must be equal to out_dim, \n",
      " but hidden_dim is not equal to out_dim so hidden_dim is set to out_dim\n",
      "Loading MLP projector\n"
     ]
    }
   ],
   "source": [
    "pipeline: EmbedAugPipeline = EmbedAugPipeline.load_inference_model(\n",
    "    llm_path=llm_path,\n",
    "    ckpt_path= None if run_name is None else (\"/lustre/scwpod02/client/kyutai-interns/hippop/tmp/\"\n",
    "    + run_name\n",
    "    + \"/checkpoints/checkpoint_\"\n",
    "    + last_ckpt),\n",
    "    device=device,\n",
    "    llm_name=\"Mistral7B\",\n",
    "    embed_model_name=\"NVEmbed\",  # Not used if pretrainde ckpt available\n",
    "    max_batch_size=max_batch_size,\n",
    "    instruct_ckpt=instruct_ckpt,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QA tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = \"/lustre/scwpod02/client/kyutai-interns/hippop/processed_data/eval_ReadComp/squad_test.jsonl\"\n",
    "# eval_data = '/lustre/scwpod02/client/kyutai-interns/hippop/processed_data/eval_QA_NVEmbed/nq_open_data.jsonl' # nq_data.jsonl\n",
    "context = []\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "\n",
    "with open(eval_data, \"r\") as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                questions.append(data[\"question\"].strip())\n",
    "\n",
    "                if isinstance(data[\"answer\"], str):\n",
    "                    answers.append([data[\"answer\"].strip()])\n",
    "\n",
    "                elif isinstance(data[\"answer\"], list):\n",
    "                    answers.append(data[\"answer\"])\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid answer type\")\n",
    "                # Take the first ranked retrieved passage\n",
    "                context.append(data[\"passages\"][:5])\n",
    "\n",
    "c = list(zip(questions, context, answers))\n",
    "fixed_random = random.Random(0.3)\n",
    "fixed_random.shuffle(c)\n",
    "questions, context, answers = zip(*c)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Prefix:\n",
      " Question: When was the Jacksonville town charter approved?\n",
      "Answer: February 9, 1832\n",
      "\n",
      "Question: Disney-ABC Television group merged with ABC Studios and what other entity in 2009?\n",
      "Answer: ABC Entertainment\n",
      "\n",
      "Question: Who edited Electrical World magazine?\n",
      "Answer: Thomas Commerford Martin\n",
      "\n",
      "Question: How many men were in Robert's army?\n",
      "Answer: 30,000\n",
      "\n",
      "Question: Who is the head coach of the Broncos?\n",
      "Answer: Gary Kubiak\n",
      "\n",
      "\n",
      "GENERATION\n",
      "Given Query: What could someone be investigated for?\n",
      "GT Answer: ['allegations of professional misconduct', 'professional misconduct', 'professional misconduct']\n",
      "Context Given [\"The functions of the teacher's colleges may include setting out clear standards of practice, providing for the ongoing education of teachers, investigating complaints involving members, conducting hearings into allegations of professional misconduct and taking appropriate disciplinary action and accrediting teacher education programs. In many situations teachers in publicly funded schools must be members in good standing with the college, and private schools may also require their teachers to be college peoples. In other areas these roles may belong to the State Board of Education, the Superintendent of Public Instruction, the State Education Agency or other governmental bodies. In still other areas Teaching Unions may be responsible for some or all of these duties.\"][...]\n",
      "Answer in context? 1 \n",
      "Prediction: Fraud \n",
      "\n",
      "\n",
      "Given Query: What type of algorithm is trial division?\n",
      "GT Answer: ['deterministic', 'deterministic algorithm', 'deterministic', 'deterministic', 'deterministic']\n",
      "Context Given ['Modern primality tests for general numbers n can be divided into two main classes, probabilistic (or \"Monte Carlo\") and deterministic algorithms. Deterministic algorithms provide a way to tell for sure whether a given number is prime or not. For example, trial division is a deterministic algorithm because, if performed correctly, it will always identify a prime number as prime and a composite number as composite. Probabilistic algorithms are normally faster, but do not completely prove that a number is prime. These tests rely on testing a given number in a partly random way. For example, a given test might pass all the time if applied to a prime number, but pass only with probability p if applied to a composite number. If we repeat the test n times and pass every time, then the probability that our number is composite is 1/(1-p)n, which decreases exponentially with the number of tests, so we can be as sure as we like (though never perfectly sure) that the number is prime. On the other hand, if the test ever fails, then we know that the number is composite.'][...]\n",
      "Answer in context? 1 \n",
      "Prediction: Primitive \n",
      "\n",
      "\n",
      "Given Query: Where did Maududi's books place Islam?\n",
      "GT Answer: ['in a modern context', 'a modern context', 'modern context']\n",
      "Context Given ['Sayyid Abul Ala Maududi was an important early twentieth-century figure in the Islamic revival in India, and then after independence from Britain, in Pakistan. Trained as a lawyer he chose the profession of journalism, and wrote about contemporary issues and most importantly about Islam and Islamic law. Maududi founded the Jamaat-e-Islami party in 1941 and remained its leader until 1972. However, Maududi had much more impact through his writing than through his political organising. His extremely influential books (translated into many languages) placed Islam in a modern context, and influenced not only conservative ulema but liberal modernizer Islamists such as al-Faruqi, whose \"Islamization of Knowledge\" carried forward some of Maududi\\'s key principles.'][...]\n",
      "Answer in context? 1 \n",
      "Prediction: In the modern world \n",
      "\n",
      "\n",
      "Given Query: Were the tapes able to be restored and processed without destroying historical legitimacy or did some aspects of the tapes lose legitimacy?\n",
      "GT Answer: ['without destroying historical legitimacy', 'Lowry Digital f', 'without destroying historical legitimacy', 'processed to remove random noise and camera shake without destroying historical legitimacy', 'without destroying historical legitimacy']\n",
      "Context Given ['With a budget of $230,000, the surviving original lunar broadcast data from Apollo 11 was compiled by Nafzger and assigned to Lowry Digital for restoration. The video was processed to remove random noise and camera shake without destroying historical legitimacy. The images were from tapes in Australia, the CBS News archive, and kinescope recordings made at Johnson Space Center. The restored video, remaining in black and white, contains conservative digital enhancements and did not include sound quality improvements.'][...]\n",
      "Answer in context? 1 \n",
      "Prediction: Yes \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w_embeds = True\n",
    "temp = 0\n",
    "max_tokens = 64\n",
    "max_embeds = 1\n",
    "icl_examples = 5\n",
    "max_bs = 4\n",
    "rag = False\n",
    "\n",
    "device_count = torch.cuda.device_count()\n",
    "other_device = device if device_count <= 1 else torch.device(\"cuda:1\")\n",
    "\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "    \n",
    "prompt_prefix = create_prompt_prefix(\n",
    "    queries=questions,\n",
    "    answers=[answer[0] for answer in answers],\n",
    "    docs=[cont[0] for cont in context] if rag and not w_embeds else None,\n",
    "    max_examples=icl_examples,\n",
    ")\n",
    "\n",
    "generated_sequences = []\n",
    "\n",
    "queries = list(questions[icl_examples:])\n",
    "docs = list(context[icl_examples:])\n",
    "truths = list(answers[icl_examples:])\n",
    "queries.reverse()\n",
    "docs.reverse()\n",
    "truths.reverse()\n",
    "\n",
    "if w_embeds:\n",
    "\n",
    "    no_context_prompt = [\n",
    "        create_prompt(\n",
    "            prefix=prompt_prefix, doc=\"\", query=query, wdoc=False\n",
    "        )\n",
    "        for query in queries[:max_bs]\n",
    "    ]\n",
    "\n",
    "    context_prompt = [\n",
    "        create_prompt(\n",
    "            prefix=\" answer the question following the examples:\\n\\n\"\n",
    "            + prompt_prefix,\n",
    "            doc=\"\",\n",
    "            query=query,\n",
    "            wdoc=False,\n",
    "        )\n",
    "        for query in queries[:max_bs]\n",
    "    ]\n",
    "\n",
    "else:\n",
    "\n",
    "\n",
    "    no_context_prompt = [\n",
    "        create_prompt(\n",
    "            prefix=prompt_prefix,\n",
    "            doc=doc if rag else '',\n",
    "            query=query,\n",
    "            wdoc=True,\n",
    "        )\n",
    "        for query, doc in zip(\n",
    "            queries[:max_bs],\n",
    "            docs[:max_bs],\n",
    "        )\n",
    "    ]\n",
    "\n",
    "\n",
    "# for i, cont in enumerate(context_prompt if pipeline.pipeline_args.w_prefix_prompt else no_context_prompt):\n",
    "#     print(f'{i}',cont)\n",
    "\n",
    "\n",
    "print('Prompt Prefix:\\n', prompt_prefix)\n",
    "    # print(no_context_prompt[i] if not pipeline.pipeline_args.w_prefix_prompt else context_prompt[i])\n",
    "    # print(f'Ground truth answer: {a}\\n')\n",
    "generated_sequence = pipeline.generate(\n",
    "    prompt_pre_embed= (['']*len(queries[:max_bs]) if not pipeline.pipeline_args.w_prefix_prompt \n",
    "        else ['Based on the context ']*len(queries[:max_bs])),\n",
    "    prompt_post_embed = context_prompt if pipeline.pipeline_args.w_prefix_prompt else no_context_prompt,\n",
    "    text_conditioning= [doc[:max_embeds] for doc in docs[:max_bs]] if w_embeds else None,\n",
    "    temperature=temp,\n",
    "    max_tokens=max_tokens,\n",
    "    truncate_line=True,\n",
    "    device=device,\n",
    "    device_generation=other_device,\n",
    ")\n",
    "\n",
    "print('GENERATION')\n",
    "for i, (q, a, d, g) in enumerate(zip(queries[:max_bs], truths[:max_bs], docs[:max_bs], generated_sequence)):\n",
    "    print(f'Given Query: {q}\\nGT Answer: {a}\\nContext Given {d[:max_embeds]}[...]\\nAnswer in context? {get_approx_em(d,a[0])}','\\nPrediction:', g, '\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_prompt_prefix_bis(\n",
    "    queries: list[str],\n",
    "    answers: list[str],\n",
    "    docs: list[str] | None = None,\n",
    "    max_examples: int | None = None,\n",
    "    fact_checking: bool = False,\n",
    ") -> str:\n",
    "    max_examples = max_examples if max_examples is not None else len(queries)\n",
    "\n",
    "    prompt = \"\"\n",
    "    if not fact_checking:\n",
    "        if docs is not None:\n",
    "            for query, answer, doc, _ in zip(queries, answers, docs, range(max_examples)):\n",
    "                prompt += f\"Context: {doc}\\nQuestion: {query}\\nAnswer: {answer}\\n\\n\"\n",
    "        else:\n",
    "            for query, answer, _ in zip(queries, answers, range(max_examples)):\n",
    "                prompt += f\"Question: {query}\\nAnswer: {answer}\\n\\n\"\n",
    "    else:\n",
    "        if docs is not None:\n",
    "            for query, answer, doc, _ in zip(queries, answers, docs, range(max_examples)):\n",
    "                prompt += f\"Context: {doc}\\nVerify the following claims with \\\"True\\\" or \\\"False\\\":\\n{query}: {query}\\nAnswer: {answer}\\n\\n\"\n",
    "        else:\n",
    "            for query, answer, _ in zip(queries, answers, range(max_examples)):\n",
    "                prompt += f\"Verify the following claims with \\\"True\\\" or \\\"False\\\":\\n{query}: {query}\\nAnswer: {answer}\\n\\n\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def create_prompt_bis(\n",
    "    prefix: str, doc: str | list[str], query: str, wdoc: bool = True\n",
    ") -> str:\n",
    "    if isinstance(doc, list):\n",
    "        doc = \"\\n\".join(doc)\n",
    "    if wdoc:\n",
    "        return prefix + f\"Context: {doc}\\nQustiony: {query}\\nAnswer:\"\n",
    "    else:\n",
    "        return prefix + f\"Question: {query}\\nAnswer:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATION\n",
      "Given Query: What could someone be investigated for?\n",
      "GT Answer: ['allegations of professional misconduct', 'professional misconduct', 'professional misconduct']\n",
      "Context Given [\"The functions of the teacher's colleges may include setting out clear standards of practice, providing for the ongoing education of teachers, investigating complaints involving members, conducting hearings into allegations of professional misconduct and taking appropriate disciplinary action and accrediting teacher education programs. In many situations teachers in publicly funded schools must be members in good standing with the college, and private schools may also require their teachers to be college peoples. In other areas these roles may belong to the State Board of Education, the Superintendent of Public Instruction, the State Education Agency or other governmental bodies. In still other areas Teaching Unions may be responsible for some or all of these duties.\"][...]\n",
      "Answer in context? 1 \n",
      "Prediction: Insider trading \n",
      "\n",
      "\n",
      "Given Query: What type of algorithm is trial division?\n",
      "GT Answer: ['deterministic', 'deterministic algorithm', 'deterministic', 'deterministic', 'deterministic']\n",
      "Context Given ['Modern primality tests for general numbers n can be divided into two main classes, probabilistic (or \"Monte Carlo\") and deterministic algorithms. Deterministic algorithms provide a way to tell for sure whether a given number is prime or not. For example, trial division is a deterministic algorithm because, if performed correctly, it will always identify a prime number as prime and a composite number as composite. Probabilistic algorithms are normally faster, but do not completely prove that a number is prime. These tests rely on testing a given number in a partly random way. For example, a given test might pass all the time if applied to a prime number, but pass only with probability p if applied to a composite number. If we repeat the test n times and pass every time, then the probability that our number is composite is 1/(1-p)n, which decreases exponentially with the number of tests, so we can be as sure as we like (though never perfectly sure) that the number is prime. On the other hand, if the test ever fails, then we know that the number is composite.'][...]\n",
      "Answer in context? 1 \n",
      "Prediction: Primality test \n",
      "\n",
      "\n",
      "Given Query: Where did Maududi's books place Islam?\n",
      "GT Answer: ['in a modern context', 'a modern context', 'modern context']\n",
      "Context Given ['Sayyid Abul Ala Maududi was an important early twentieth-century figure in the Islamic revival in India, and then after independence from Britain, in Pakistan. Trained as a lawyer he chose the profession of journalism, and wrote about contemporary issues and most importantly about Islam and Islamic law. Maududi founded the Jamaat-e-Islami party in 1941 and remained its leader until 1972. However, Maududi had much more impact through his writing than through his political organising. His extremely influential books (translated into many languages) placed Islam in a modern context, and influenced not only conservative ulema but liberal modernizer Islamists such as al-Faruqi, whose \"Islamization of Knowledge\" carried forward some of Maududi\\'s key principles.'][...]\n",
      "Answer in context? 1 \n",
      "Prediction: in the framework of modern political and economic theories \n",
      "\n",
      "\n",
      "Given Query: Were the tapes able to be restored and processed without destroying historical legitimacy or did some aspects of the tapes lose legitimacy?\n",
      "GT Answer: ['without destroying historical legitimacy', 'Lowry Digital f', 'without destroying historical legitimacy', 'processed to remove random noise and camera shake without destroying historical legitimacy', 'without destroying historical legitimacy']\n",
      "Context Given ['With a budget of $230,000, the surviving original lunar broadcast data from Apollo 11 was compiled by Nafzger and assigned to Lowry Digital for restoration. The video was processed to remove random noise and camera shake without destroying historical legitimacy. The images were from tapes in Australia, the CBS News archive, and kinescope recordings made at Johnson Space Center. The restored video, remaining in black and white, contains conservative digital enhancements and did not include sound quality improvements.'][...]\n",
      "Answer in context? 1 \n",
      "Prediction: Yes, the tapes were restored and processed without destroying historical legitimacy. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w_embeds = True\n",
    "temp = 0.8\n",
    "max_tokens = 64\n",
    "max_embeds = 1\n",
    "icl_examples = 5\n",
    "max_bs = 4\n",
    "rag = False\n",
    "\n",
    "device_count = torch.cuda.device_count()\n",
    "other_device = device if device_count <= 1 else torch.device(\"cuda:1\")\n",
    "\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "    \n",
    "prompt_prefix = create_prompt_prefix_bis(\n",
    "    queries=questions,\n",
    "    answers=[answer[0] for answer in answers],\n",
    "    docs= [cont[0] for cont in context],\n",
    "    max_examples=icl_examples,\n",
    ")\n",
    "\n",
    "generated_sequences = []\n",
    "\n",
    "queries = list(questions[icl_examples:])\n",
    "docs = list(context[icl_examples:])\n",
    "truths = list(answers[icl_examples:])\n",
    "queries.reverse()\n",
    "docs.reverse()\n",
    "truths.reverse()\n",
    "\n",
    "if w_embeds:\n",
    "\n",
    "    no_context_prompt = [\n",
    "        create_prompt_bis(\n",
    "            prefix=prompt_prefix, doc=\"\", query=query, wdoc=False\n",
    "        )\n",
    "        for query in queries[:max_bs]\n",
    "    ]\n",
    "\n",
    "    context_prompt = [\n",
    "        create_prompt_bis(\n",
    "            prefix=\" answer the question following the examples:\\n\\n\"\n",
    "            + prompt_prefix,\n",
    "            doc=\"\",\n",
    "            query=query,\n",
    "            wdoc=False,\n",
    "        )\n",
    "        for query in queries[:max_bs]\n",
    "    ]\n",
    "\n",
    "else:\n",
    "\n",
    "\n",
    "    no_context_prompt = [\n",
    "        create_prompt_bis(\n",
    "            prefix=prompt_prefix,\n",
    "            doc=doc if rag else '',\n",
    "            query=query,\n",
    "            wdoc=True,\n",
    "        )\n",
    "        for query, doc in zip(\n",
    "            queries[:max_bs],\n",
    "            docs[:max_bs],\n",
    "        )\n",
    "    ]\n",
    "\n",
    "\n",
    "# for i, cont in enumerate(context_prompt if pipeline.pipeline_args.w_prefix_prompt else no_context_prompt):\n",
    "#     print(f'{i}',cont)\n",
    "\n",
    "\n",
    "# print('Prompt Prefix:\\n', prompt_prefix)\n",
    "    # print(no_context_prompt[i] if not pipeline.pipeline_args.w_prefix_prompt else context_prompt[i])\n",
    "    # print(f'Ground truth answer: {a}\\n')\n",
    "generated_sequence = pipeline.generate(\n",
    "    prompt_pre_embed= (['']*len(queries[:max_bs]) if not pipeline.pipeline_args.w_prefix_prompt \n",
    "        else ['Based on the context ']*len(queries[:max_bs])),\n",
    "    prompt_post_embed = context_prompt if pipeline.pipeline_args.w_prefix_prompt else no_context_prompt,\n",
    "    text_conditioning= [doc[:max_embeds] for doc in docs[:max_bs]] if w_embeds else None,\n",
    "    temperature=temp,\n",
    "    max_tokens=max_tokens,\n",
    "    truncate_line=True,\n",
    "    device=device,\n",
    "    device_generation=other_device,\n",
    ")\n",
    "\n",
    "print('GENERATION')\n",
    "for i, (q, a, d, g) in enumerate(zip(queries[:max_bs], truths[:max_bs], docs[:max_bs], generated_sequence)):\n",
    "    print(f'Given Query: {q}\\nGT Answer: {a}\\nContext Given {d[:max_embeds]}[...]\\nAnswer in context? {get_approx_em(d,a[0])}','\\nPrediction:', g, '\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconstruct tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000 \n",
      "\n",
      "\n",
      "# 100% Pure Shea Butter\n",
      "\n",
      "$10.00\n",
      "\n",
      "Shea butter is a natural fat extracted from the nut of the African shea tree. It is a triglyceride (fat) that is a major source of skin-nourishing fatty acids and vitamins. It is a natural moisturizer that is used to soften and soothe skin.\n",
      "\n",
      "Shea butter is a natural moisturizer that is used to soften and soothe skin. It is a natural fat extracted from the nut of the African shea tree. It is a triglyceride (fat) that is a major source of skin-nourishing fatty acids and vitamins.\n",
      "\n",
      "Shea butter is a natural moisturizer that is used to soften and soothe skin. It is a natural fat extracted from the nut of the African shea tree. It is a triglyceride (fat) that is a major source of skin-nourishing fatty acids and vitamins.\n",
      "\n",
      "Shea butter is a natural moisturizer that is used to soften and soothe skin. It is a natural fat \n",
      "\n",
      "\n",
      "# 1969\n",
      "\n",
      "## 1969\n",
      "\n",
      "### 1969\n",
      "\n",
      "#### by The Editor\n",
      "\n",
      "1969 was a year of great change in the United States. The year began with the inauguration of Richard Nixon as the 37th President of the United States. The year ended with the first lunar landing by humans, the Apollo 11 mission.\n",
      "\n",
      "The year 1969 was the first year of the 1970s decade.\n",
      "\n",
      "## January\n",
      "\n",
      "- January 20 – The United States invades the Dominican Republic.\n",
      "- January 23 – The United States Supreme Court rules in Tinker v. Des Moines Independent Community School District that students do not \"shed their constitutional rights to freedom of speech or expression at the schoolhouse gate.\"\n",
      "- January 24 – The United States Supreme Court rules in Stanley v. Georgia that the mere possession of obscene material is not a crime.\n",
      "- January 27 – The United States Supreme Court rules in United States v. O'Brien that the government may regulate the time, place, and manner of speech, but not the content.\n",
      " \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w_embeds = True\n",
    "temp = 0\n",
    "max_tokens = 256\n",
    "n_context = 0\n",
    "i_token_to_flip = -1\n",
    "\n",
    "device_count = torch.cuda.device_count()\n",
    "other_device = device if device_count <= 1 else torch.device(\"cuda:1\")\n",
    "\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "\n",
    "temp = [temp] * max_tokens   \n",
    "if max_tokens > i_token_to_flip >= 0:\n",
    "    temp[i_token_to_flip] = -1\n",
    "\n",
    "# 1 information in the doc which enables to answer the question but not good response often in-context\n",
    "# 2 information in the doc which enables to answer the question and good response often in-context\n",
    "# 3 Hard negative passage\n",
    "\n",
    "\n",
    "\n",
    "prompt_prefix = [\"Query: who wrote the song photograph by ringo starr\\nAnswer: Ringo Starr\\n\\n\",\"Query: who is playing the halftime show at super bowl 2016\\nAnswer: Coldplay\\n\\n\",\"Query: where was the world economic forum held this year\\nAnswer: Davos\\n\\n\",\"Query: where are the giant redwoods located in california\\nAnswer: Humboldt County\\n\\n\",\"Query: who has made the most premier league appearances\\nAnswer: Gareth Barry\\n\\n\"]\n",
    "prompt_prefix = ''.join(prompt_prefix[:n_context])\n",
    "# prompt_prefix = \"Query: \"\n",
    "\n",
    "# prompts = ['who has most followers on instagram in world','who did the united states win its independence from', 'what is the capital of France']\n",
    "prompts = [\"what is the hot coffee mod in san andreas\",\"who wrote he ain't heavy he's my brother lyrics\",\"when was the last time anyone was on the moon\"]\n",
    "context_prompts = [' answer the question following the examples ' +prompt_prefix + 'Query: '+ prompt  + '\\nAnswer:' for prompt in prompts]\n",
    "# no_context_prompts = [prompt_prefix + 'Query: '+ prompt + '\\nAnswer:' for prompt in prompts]\n",
    "no_context_prompts = ['' for prompt in prompts]\n",
    "\n",
    "# conditioning = [\"This list contains the top 50 accounts with the most followers on the photo and video-sharing social platform Instagram. As of July 2019, the most followed user is Instagram's own account, with over 308 million followers. Cristiano Ronaldo is the most followed individual, with over 177 million followers. Fifteen accounts have exceeded 100 million followers on the site.\",\n",
    "#                 \"During the American Revolution, the legal separation of the thirteen colonies from Great Britain in 1776 actually occurred on July 2, when the Second Continental Congress voted to approve a resolution of independence that had been proposed in June by Richard Henry Lee of Virginia declaring the United States independent from Great Britain's rule. After voting for independence, Congress turned its attention to the Declaration of Independence, a statement explaining this decision, which had been prepared by a Committee of Five, with Thomas Jefferson as its principal author. Congress debated and revised the wording of the Declaration, finally approving it two days later on July 4. A day earlier, John Adams had written to his wife Abigail\",\n",
    "#                 \"France is a country located primarily in Western Europe. Its overseas regions and territories include French Guiana in South America, Saint Pierre and Miquelon in the North Atlantic, the French West Indies, and many islands in Oceania and the Indian Ocean, giving it one of the largest discontiguous exclusive economic zones in the world. Metropolitan France shares borders with Belgium and Luxembourg to the north, Germany to the northeast, Switzerland to the east, Italy and Monaco to the southeast, Andorra and Spain to the south, and a maritime border with the United Kingdom to the northwest. Its metropolitan area extends from the Rhine to the Atlantic Ocean and from the Mediterranean Sea to the English Channel and the North Sea. \"]\n",
    "\n",
    "conditioning = ['Hot Coffee is a normally inaccessible mini-game in Grand Theft Auto: San Andreas. The mini-game portrays crudely animated sexual intercourse between the main character and a chosen partner. After Patrick Wildenborg, a software engineer who also went by the alias \\\"PatrickW\\\", modified the game to make the mini-game accessible, Hot Coffee quickly gained notoriety worldwide, impacting consumer culture, politics and the video game industry as a whole. Rockstar initially blamed a \\\"determined group of hackers\\\" for hacking the base game and creating the mini-game from scratch. This claim was eventually refuted, as the mini-game\\'s code and assets had been developed by Rockstar and were already present, unfinished and abandoned, on the game disc: the mod simply made the existing content available to players. Rockstar would go on to indicate that they expected the ESRB rating to remain unchanged, as they had no control',\n",
    "                \"He Ain't Heavy, He's My Brother written by Bob Russell and Bobby Scott; all other titles written by Neil Diamond.\",\n",
    "                \"17, 1970, as part of the Lunokhod program. To date, the last human to stand on the Moon was Eugene Cernan, who as part of the Apollo 17 mission, walked on the Moon in December 1972. Apollo 17 was followed by several uncrewed interplanetary missions operated by NASA. One of the notable interplanetary missions is Voyager 1, the first artificial object to leave our Solar System into interstellar space on August 25, 2012. It is also the most distant artificial object from Earth. The probe passed the heliopause at 121 AU to enter interstellar space. Voyager 1 is currently at a distance of 145.11 AU (21.708 billion kilometers; 13.489 billion miles) from Earth as of January 1, 2019.\"]\n",
    "\n",
    "# answers = ['Instagram','Great Britain',\"Paris\"]\n",
    "# answers = [\"a normally inaccessible mini-game\", [\"Bobby Scott\", \"Bob Russell\"],[\"14 December 1972 UTC\", \"December 1972\"]]\n",
    "\n",
    "# conditioning = 'Kyutai is a non-profit laboratory dedicated to open research in AI, founded in November 2023 by the iliad Group, CMA CGM and Schmidt Sciences.'\n",
    "# prompts =  prompt_prefix +'Query: when was founded Kyutai?\\nAnswer: '\n",
    "\n",
    "\n",
    "\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "\n",
    "generated_sequence = pipeline.generate(\n",
    "    prompt_pre_embed = (['']*len(conditioning) if not pipeline.pipeline_args.w_prefix_prompt \n",
    "    else ['Based on the context ']*len(conditioning)), \n",
    "    prompt_post_embed = context_prompts if pipeline.pipeline_args.w_prefix_prompt  else no_context_prompts,\n",
    "    text_conditioning=conditioning,\n",
    "    temperature=temp,\n",
    "    max_tokens=max_tokens,\n",
    "    truncate_double_space=True,\n",
    "    device=device,\n",
    "    device_generation=other_device,\n",
    ")\n",
    "\n",
    "for seq in generated_sequence:\n",
    "    print(seq, '\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EmbedAugArgs' object has no attribute 'max_seq_len'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lim_toks \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_seq_len\u001b[49m\n\u001b[1;32m      3\u001b[0m atlas_eval_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/lustre/scwpod02/client/kyutai-interns/hippop/processed_data/wiki_passages_pretraining/valid_atlas_enwiki-dec2021_standard.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m atlas_valid_passage \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EmbedAugArgs' object has no attribute 'max_seq_len'"
     ]
    }
   ],
   "source": [
    "lim_toks = pipeline.pipeline_args.max_seq_len\n",
    "\n",
    "atlas_eval_data = \"/lustre/scwpod02/client/kyutai-interns/hippop/processed_data/wiki_passages_pretraining/valid_atlas_enwiki-dec2021_standard.jsonl\"\n",
    "atlas_valid_passage = []\n",
    "dump_eval_data = '/lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_valid.jsonl'\n",
    "dump_valid_passage = []\n",
    "\n",
    "with open(atlas_eval_data, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        atlas_valid_passage.append(\n",
    "            pipeline.tokenizer.decode(\n",
    "                pipeline.tokenizer.encode(\n",
    "                    json.loads(line)[\"text\"], eos=True, bos=True\n",
    "                )[:lim_toks]\n",
    "            )\n",
    "            )\n",
    "random.shuffle(atlas_valid_passage)\n",
    "\n",
    "\n",
    "with open(dump_eval_data, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        text = json.loads(line)[\"text\"]\n",
    "        dump_valid_passage.append(\n",
    "                pipeline.tokenizer.decode(\n",
    "                    pipeline.tokenizer.encode(\n",
    "                        text if \"\\n\\n\" not in   text else text.split(\"\\n\\n\")[1], eos=True, bos=True\n",
    "                    )[:lim_toks]\n",
    "                )\n",
    "            )\n",
    "random.shuffle(dump_valid_passage)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_passages = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 0.0\n",
    "device_count = torch.cuda.device_count()\n",
    "other_device = device if device_count <= 1 else torch.device(\"cuda:1\")\n",
    "atlas_generated_sequences = []\n",
    "for i in range(0, n_passages, max_batch_size):\n",
    "    passage = atlas_valid_passage[i : i + max_batch_size]\n",
    "    generated_sequence, logprobs = pipeline.generate(\n",
    "        prompt_pre_embed = (['']*len(passage) if not pipeline.pipeline_args.w_prefix_prompt \n",
    "        else ['In other words, background: ']*len(passage)), \n",
    "        prompt_post_embed = (['']*len(passage) if not pipeline.pipeline_args.w_prefix_prompt \n",
    "        else [' is just another way of saying: ']*len(passage)),\n",
    "        text_conditioning=passage,\n",
    "        temperature=temp,\n",
    "        max_tokens=lim_toks,\n",
    "        truncate_double_space=False,\n",
    "        device=device,\n",
    "        device_generation=other_device,\n",
    "    )\n",
    "\n",
    "    atlas_generated_sequences.extend(generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 0.0\n",
    "device_count = torch.cuda.device_count()\n",
    "other_device = device if device_count <= 1 else torch.device(\"cuda:1\")\n",
    "dump_generated_sequences = []\n",
    "for i in range(0, n_passages, max_batch_size):\n",
    "    passage = dump_valid_passage[i : i + max_batch_size]\n",
    "    generated_sequence, logprobs = pipeline.generate(\n",
    "        prompt_pre_embed = (['']*len(passage) if not pipeline.pipeline_args.w_prefix_prompt \n",
    "        else ['In other words, background: ']*len(passage)), \n",
    "        prompt_post_embed = (['']*len(passage) if not pipeline.pipeline_args.w_prefix_prompt \n",
    "        else [' is just another way of saying: ']*len(passage)),\n",
    "        text_conditioning=passage,\n",
    "        temperature=temp,\n",
    "        max_tokens=lim_toks,\n",
    "        truncate_double_space=False,\n",
    "        device=device,\n",
    "        device_generation=other_device,\n",
    "    )\n",
    "\n",
    "    dump_generated_sequences.extend(generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG scores 0.8125772827951994\n",
      "AVG scores TRUNC 0.8183385157580503\n",
      "Very small bleu 1\n",
      "Mismatch 31\n"
     ]
    }
   ],
   "source": [
    "avg_bleu = 0\n",
    "n_mismatch = 0\n",
    "n_very_small_blue = 0\n",
    "for gen, gt in zip(dump_generated_sequences, dump_valid_passage):\n",
    "    bleu_score = get_bleu_score(gt, gen)\n",
    "    avg_bleu += bleu_score\n",
    "    \n",
    "    try:\n",
    "        if bleu_score != get_bleu_score(gt, gen, trunc = True):\n",
    "            n_mismatch += 1\n",
    "            # print('MISMATCH {}'.format(n_mismatch))\n",
    "            # print('Ge:', gen)\n",
    "            # print('GT:', gt)\n",
    "            # print(bleu_score, ' | ', get_bleu_score(gt, gen, trunc = True))\n",
    "            if bleu_score < 0.1:\n",
    "                n_very_small_blue += 1\n",
    "            \n",
    "        elif bleu_score < 0.1:\n",
    "            n_very_small_blue += 1\n",
    "            print('VERY SMALL BLEU {}'.format(n_very_small_blue))\n",
    "            print('Ge:', gen)\n",
    "            print('GT:', gt)\n",
    "            print(bleu_score)\n",
    "    except ValueError:\n",
    "        print('Passages skipped')\n",
    "\n",
    "print('AVG scores',get_bleu_score(dump_valid_passage, dump_generated_sequences))\n",
    "print('AVG scores TRUNC',get_bleu_score(dump_valid_passage, dump_generated_sequences, trunc = True))\n",
    "print('Very small bleu', n_very_small_blue)\n",
    "print('Mismatch', n_mismatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERY SMALL BLEU 1\n",
      "Ge: 0 A subsequence of a string whose characters are common in two strings (S1 and S2) can be computed efficiently using the notion of common subsequence. The problem has a simple algorithmic solution; both standard algorithms run in linear time in DSPSS (this is still faster than in Python) for a computer. It has been well studied in mathematics, its basic notions of deletions and appearances, the problem of finding Maximal Common Subsequences for sequences of four or more characters, for extra characters with few constraints that have the property of overlapping in more than one way. This problem (and its generalization to image alignment and image reconstruction) fits in studies of the geometry of editing theories (in Russian), dynamical systems, and computer science, and is motivated by the mean comparison of DNA sequences and the reconstruction of common subsequences at short edit distances. If one computes long subsequences of S1 and S2, such that the computations on such subsequences would not be revised by chance if they were returned from a computer. What one gets from this computation is a relation that is evidently useful to calculate the global similarity, not particularly biological or mathematical.\n",
      "GT:  A common subsequence of two strings S and T is a string whose characters appear in the same order (not necessarily consecutively) both in S and in T. The problem of computing a longest common subsequence has been well studied in computer science. It can be solved in polynomial time by dynamic programming; this basic algorithm has additional speedups for small alphabets (the Method of Four Russians), for strings with few differences, for strings with few matching pairs of characters, etc. This problem and its generalizations to more complex forms of edit distance have important applications in areas that include bioinformatics (in the comparison of DNA and protein sequences and the reconstruction of evolutionary trees), geology (in stratigraphy), and computer science (in data comparison and revision control). One motivation for studying the longest common subsequences of random strings, given already by Chvátal and Sankoff, is to calibrate the computations of longest common subsequences on strings that are not random. If such a computation returns a subsequence that is significantly longer than what would be obtained at random, one might infer from this result that the match is meaningful or significant.\n",
      "0.09710923867339806\n",
      "VERY SMALL BLEU 3\n",
      "Ge: Zapp brought the bones of this thing out into the world. \"All I say,\" says a documentary film, in which they made sure you were a track which really does sound exactly like Real Tunes. For the first time on record, Fender Rhodes can hear them - fanatic with a giant, tenth-tall way!\" and the fans decided on Grand Funk III to split up again only 4 days after they overdubbed their voices. All Zapp stayed until summer of 1985, trying to talk him out of it, although the album wasn't an 'unqualified success' in the US. The album became the first topping both releases to Billboard Top 200 Melter.\"\n",
      "GT:  bring the balls out of this thing.\" Zappa said, \"All I did was, in a documentary way, make a record which tells you exactly what they really sound like. For the first time on record you can hear Grand Funk Railroad ... and they're fantastic, fan-tastic with an F three times taller than you!\" Grand Funk decided on the first day of overdubs to split up again, although Zappa stayed until 4 a.m. trying to talk them out of it. The album wasn't a commercial success, only making it to #52 in the Billboard Top 200''. The album was the final release to feature both bassist Mel Schacher and keyboardist Craig Frost.\n",
      "0.09549153712369463\n",
      "VERY SMALL BLEU 4\n",
      "Ge: Bordeaux, most inland wines are serious bottles, although undergoing an ageing-process. Sometimes, white wine can be an exception in Bordeaux. The must of château-barrel ageing requires 6 barrels of must for each, but some new châteaux (which number as high as 20 meters above the oak barrel) can impart a different degree of ageing. Just recently, the volume of wine to be added to Bordeaux is heritage-protected. In order to make barrel ageing, the châteaux have to be racked from time to time in clear mold. This process is attacked by living organisms, as recently as itmakes sense since adding the châteaux to the blend can also be done. Additionally, periods of ageing es.eseseseseseseseses\n",
      "GT:  In Bordeaux, most serious wines undergo barrel-ageing, although white wines can be an exception. Usually, six months of ageing in-barrel is required, but some châteaux barrel-age for as much as 20 months. The number of new barrels (which impart a higher degree of oak flavor to the wine) can vary from vintage to vintage, just as the duration of barrel-ageing. Only recently, addition of oak chips has been made legal in Bordeaux. During barrel-ageing, the wine needs to be racked in order to clear it of lees. This process is being challenged by some producers, as mentioned abovem since ageing on the lees can also add richness to the wine.\n",
      "0.0\n",
      "VERY SMALL BLEU 5\n",
      "Ge: Mr. Critical was mostly positive review of both Monroe's resurrection, with fellow reviewers highlighting Wells's development of the character. Aviation Monthly's Jonathan Keller praised Monroe's complementary plotting as a \"clever balance between Savage and Wells for a piece of fiction\". Paul Nardini stated that Superman Had to Have Murdered Himself had never altered his own blog, saying: \"If I hadn't told my series intro earlier, I'd also be looking askance at the latter's failure to continue from the book\". Amazon.com commended Monroe's plotting as a \"strong counterpoint to any implications of the original story\", whereas Kenneth Beggs of Family Friendly Gaming described the book's \"realistic aspects\" and \"friendship with the main characters\". Keeping the vital fictional elements of the tale offers that the series addresses a \"nasty, chilling, and bloody mash-up of vampires and zombies\" with \"punctuated dialogue\". Audiences Magazine praised the book for its suitability for schoolchildren. Starfleet.com wrote, \"Monroe is a smart, powerful plotline while\n",
      "GT:  Critical reception of Mr. Monster was mostly positive, with multiple reviewers highlighting Wells's further development of both the protagonist and plot. A Savannah Morning News review complimented John Cleaver's characterization as \"a nifty balancing act for Wells to have pulled off\". Lee Mandelo praised Wells's expansion of Serial Killer into a series, stating: \"If his own blog hadn't told me otherwise, I would never have guessed he hadn't intended a sequel from the beginning\". Alternative Magazine also commended Wells's continuation of the storyline, saying that Mr. Monster \"immediately addresses any faults with its predecessor's ending\" while keeping the \"strong aspects of the original story\". Kirkus Reviews noted that \"John's realistic familial relationships and friendships offer a counterbalance to the bloody, fantastical elements of the tale\" and recommended the novel for \"fans of genre mash-ups\". A School Library Journal reviewer described Mr. Monster as \"compelling, quick-paced, and chilling\". Publishers Weekly wrote that the book \"stands out with taut, sharp writing, strong plotting,\n",
      "0.0\n",
      "Passages skipped\n",
      "Passages skipped\n",
      "VERY SMALL BLEU 7\n",
      "Ge: London-Smiths Blacksmiths, except backyard enterprises - Craftsmen chose blacksmiths to provide transportation, abbreviated trolley, and horse carriage shops. ; The Concert Hall that served as an entertainment venue for 1840s to 1884 was the first operating theatre in the area. ; Foresight Tavern - This community-owned tavern established the London Correspondence Lodges and provided meeting places for local freemasons like their masters. ; Mullet Lodge was constructed in Phase I. Powerful London St. Andrews Methodist Church buildings are similar to what could be found in the early 19th century when the first horse drawn trolley arrived from an Irish settlement. These included resort municipalities with orange hues like Orangeville, Ontario. Built in the area of the settlement where the townships of Huron also fanned out, many builders were establishing a reputation for themselves in the region, bringing a need for paved roads through the village. The community was representative of early London's streets, docks, lakes and canals. ; Concert Hall when it came to a decision to construct a traveling house for the town\n",
      "GT: Blacksmith Shop-Entrepreneurs, like blacksmiths, chose transportation crossroads to establish off-farm businesses. ; Corbett Tavern - An 1840s tavern that provided accommodation, food and stabling for horses to the traveling public and served as the community meeting place. ; Lochaber Church - This Free Presbyterian Church was constructed in 1884. ; Mount Moriah Lodge - Masonic orders held their first meetings in local taverns until purpose built halls like this one could be constructed. The interior is representative of an early 19th-century London Masonic Hall. ; Purple Hill Lodge - Established by the Protestant Irish immigrants who brought Orangeism with them when they came to Canada. Many settled areas in the region of what is now southwestern Ontario built meeting halls for the orange order, including townships in the London district. These buildings were also a focal point for the community, providing a place where settlers could get to know their neighbors through dances, dinners, recitals and concerts. Representative of the first stage of urban development at a transportation crossroads. \n",
      "0.0\n",
      "VERY SMALL BLEU 8\n",
      "Ge: Perry Botkin: Special effects, piano, lead guitar ; Mainly Joe Harnell: Piano, RCA Studio Orchestra ; David Levine: Guitar, Tom Ferguson: Guitar, Mike Reeves: Corrado. ; strings: Jesse Levy, Ben Cohen, Neil Stubenhaus, Bruce Ditmas, Pete Jolly, Sol Infante ; guitar: Izzy Sherr ; percussion: Grover Washington, Jr., Paul Enger, Jerry Zenkin, Harvie Swartz, Fred Seibert, David Laub, Amy Assante, Sahib Shihab, Joe Napolitano, Terry Nuttycombe, Rene Vinyl ; cellos: Allan Shrock, Ervin Roth ; strings: Kermit Moore, Bill Green, Gerald Friedman ; guitar: Hank Cicalo, Buddy Rich, Tony Levin, Getty Peterson, Gayle Levant, Blossom Allen, John Word ; flutes: Buddy Rich, Chuck Rainey, Bruce Barth ; flutes: Jim Peterson, CAPA, Seymour Barab ; contractor: Leo S. Brubaker ; father: George Palmer \n",
      "GT: Special effects: Perry Botkin, Jr. ; Piano: Tom Hensley, Mike Lang, Pete Jolly ; Guitar: Lee Ritenour, David Cohen, Neil Levang ; Bass: Max Bennett, Reinie Press, Steve LaFever ; drums: Joe Correro, Sol Gubin ; percussion: Gene Estes ; violins: Iz Baker, Paul Shure, Jerry Vinci, Sid Sharp, Tibor Zelig, Henry Ferber, Assa Drori, Jimmie Getzoff, Harry Bluestone, Erno Neufeld, Nate Ross ; violas: Dave Schwartz, Allan Harshman, Gerry Nuttycombe, Sven Reher ; cellos: Ray Kramer, Fred Seykora, Armand Kaproff ; woodwinds: Johnny Rotella, Gene Cipriano, Ronnie Lang, Bud Shank, Bill Green ; Harp: Gayle Levant ; trumpets: Bill Peterson, Bud Brisbois, Tony Terran, Cappy Lewis, Buddy Childers ; trombones: Charles Loper, Dick Nash ; Musical contractor: Charles H. Stern ; Engine\n",
      "0.0\n",
      "VERY SMALL BLEU 11\n",
      "Ge: volcanic vents to the East covered the area, and a fault trench formed during the eastward slope of the Fissure. The east side of the Fissure opens the valley of the Lander River, and the north side is the range, which was filled with diorite, while the hooping vents filled with porphyry. The fault trench composes these faults into \"mineral-bearing \"ore\". The faults called \"mineral ore\" stated that \"pockets are abundant\". The miners made the ore widely through the thin black slate of the East Fissure and nearly all their upper layer in \"like a charity case\", although only parts of the upper and lower layers were found or plowed. With them the fault trench extended in all directions, and \"rich sextuple ore\" was almost 600 ft wide.\n",
      "GT:  Volcanic vents to the east covered the area during the Tertiary, and a fault fissure opened the east slope of the Virginia Range. The east slope of the range forms the footwall of the Lode, and is composed of diorite, while the hanging wall is composed of andesite, which the miners called \"porphyry\". The fault fissures filled these fissures with \"mineral-bearing quartz\". The miners stated \"porphyry makes ore\". The ore bodies were thinly scattered through the wide Lode \"like plums in a charity pudding\", and nearly all of them were found in the wide upper section and along or near the east wall. Although the miners extended their work in all directions, only \"sixteen large and rich ore bodies\" were found, most less than 600 ft in depth.\n",
      "0.0\n",
      "VERY SMALL BLEU 12\n",
      "Ge: 6th from the Events in Ireland.............\n",
      "GT:  Events from the 6th century in Ireland.\n",
      "0.0\n",
      "VERY SMALL BLEU 13\n",
      "Ge: Halichard 4, Jens Kehrle 4, Hans Kuys 3, Asitsholts 3, Volodymyr Kolodzey 3. AAU Amsterdam, Netherlands: Amsterdam Arena, April 17-19, 1976 – UEFA National Final in Amsterdam 101:108 (wellmans in favour of VALL 59-79) Jens Kehrle:- 84, Steve Grazier, Bryce Wells, Frank Maravitch, Vivian Horton, Tucker Smith. 3rd All Star All-Star Week 1978-79 AAU Amsterdam, Netherlands: Universe 25-39 (Nag 15-21) IN: Hans Peters, Bill Croft, Gary Fenn, Dirk Hagens (16), John Lee 20-23, Steve Grazier 20-23, Emmett Ford 16-47, John Hankma, Pete Mildenhall, Everett Dawson, Carl Krapp: 131 Jopal Zone. N 2-1 (238).\n",
      "GT:  Heidrich 4, Hans Kaltschmidt 4, Jürgen Kolze 4, Volker Asshoff 3. Apollohal, Amsterdam, April 17, 1976: USA All Stars in Netherlands – USA All Stars in West Germany 109-89 (att:51-39) USA All Stars in Netherlands : Owen Wells, Steven Bravard, Gary Freeman, Vince Fritz, Tyrone Marioneaux, Buff Kirkland, Hank Smith. 5th All-Star Gala 1977-78 Apollohal, Amsterdam, April 23, 1978: North – South 158-165 NORTH (Jim Parks): Al Davis 33, Dan Henderson 21, Pete Miller 20, John Franken 20, Hank Smith 18, Gary Freeman 16, Everett Fopma 13, John Wayne Croft 9, Emill Hagens 7, Bert Kragtwijk 1. SOUTH (Manny Cramford): Billy Taylor 2\n",
      "0.0\n",
      "Error with update: \n",
      "Ground-Truth:  Untung Pakai Esia  \n",
      "Pred:  untuk Pangai Esia   \n",
      "AVG scores 0.41971318539118835\n",
      "Error with update: \n",
      "Ground-Truth:  Untung Pakai Esia  \n",
      "Pred:  untuk Pangai Esia \n",
      "AVG scores TRUNC 0.43351392936413774\n",
      "Very small bleu 13\n",
      "Mismatch 50\n"
     ]
    }
   ],
   "source": [
    "avg_bleu = 0\n",
    "n_mismatch = 0\n",
    "n_very_small_blue = 0\n",
    "for gen, gt in zip(atlas_generated_sequences, atlas_valid_passage):\n",
    "    try:\n",
    "        bleu_score = get_bleu_score(gt, gen)\n",
    "        avg_bleu += bleu_score\n",
    "    except ValueError:\n",
    "        print('Passages skipped')\n",
    "    \n",
    "    try:\n",
    "        if bleu_score != get_bleu_score(gt, gen, trunc = True):\n",
    "            n_mismatch += 1\n",
    "            # print('MISMATCH {}'.format(n_mismatch))\n",
    "            # print('Ge:', gen)\n",
    "            # print('GT:', gt)\n",
    "            # print(bleu_score, ' | ', get_bleu_score(gt, gen, trunc = True))\n",
    "            if bleu_score < 0.1:\n",
    "                n_very_small_blue += 1\n",
    "            \n",
    "        elif bleu_score < 0.1:\n",
    "            n_very_small_blue += 1\n",
    "            print('VERY SMALL BLEU {}'.format(n_very_small_blue))\n",
    "            print('Ge:', gen)\n",
    "            print('GT:', gt)\n",
    "            print(bleu_score)\n",
    "    except ValueError:\n",
    "        print('Passages skipped')\n",
    "\n",
    "print('AVG scores',get_bleu_score(atlas_valid_passage, atlas_generated_sequences))\n",
    "print('AVG scores TRUNC',get_bleu_score(atlas_valid_passage, atlas_generated_sequences, trunc = True))\n",
    "print('Very small bleu', n_very_small_blue)\n",
    "print('Mismatch', n_mismatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hippolytepilchen/.cache/huggingface/modules/transformers_modules/nvidia/NV-Embed-v2/5130cf1daf847c1bacee854a6ef1ca939e747fb2/modeling_nvembed.py:349: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(batch_dict.get('input_ids').to(batch_dict.get('input_ids')).long()),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Pre Embed: [8, 0]\n",
      "Prompt Post Embed: [10, 1]\n",
      "['\\n\\nMario Alberto Bortolazzi (born 12 January 1964) is a former professional footballer who played as a midfielder. He played for 12 seasons at A.C. Milan, winning the Serie A championship in 1991–92, and the Coppa Italia in 1993–94. He was transferred to Genoa C.F.C. for 500,000 lire, and made his debut on 25 September 1995, in a match against Fiorentina at', 'Mario Bortolazzi\\n\\nMario Bortolazzi (born 12 January 1965 in Genoa) is an Italian former professional football player and coach. He played 12 seasons in Serie A, for A.C. Milan, Atalanta, Fiorentina, and Hellas Verona. He played 246 games in Serie A, scoring 14 goals. 1990–91 season, he played 12 games in Serie B, for A.C. Milan. 1991–92 season,']\n"
     ]
    }
   ],
   "source": [
    "# Flipping attempts\n",
    "w_embeds = True\n",
    "temp = 0\n",
    "max_tokens = 128\n",
    "i_token_to_flip = -1\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "\n",
    "temp = [temp] * max_tokens   \n",
    "if max_tokens > i_token_to_flip >= 0:\n",
    "    temp[i_token_to_flip] = 1000\n",
    "    \n",
    "prompt = ''\n",
    "text_conditioning ='Mario Bortolazzi (born 10 January 1965, in Verona) is an Italian professional football coach and a former player, who played as a midfielder. \\\n",
    "    \\n\\nHe played 12 seasons (241 games, 14 goals) in the Serie A for ACF Fiorentina, A.C. Milan, Hellas Verona F.C., Atalanta B.C. and Genoa C.F.C.'\n",
    "        # \\n\\nIn his coaching career he has so far has always been an assistant to his former Milan teammate Roberto Donadoni.\\\n",
    "        #     \\n\\nHonours\\n\\n - Milan\\n - Serie A champion: 1987–88.\\n\\n - Genoa\\n - Anglo-Italian Cup winner: 1995–96.'\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "generated_sequence, attn_or_logprobs, embeddings = pipeline.generate(prompt_pre_embed = ['In other words, background: ',''], \n",
    "                                    prompt_post_embed = [' is just another way of saying: ',''],\n",
    "                                    text_conditioning = [text_conditioning]*2, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False,\n",
    "                                    random_flip = i_token_to_flip,\n",
    "                                    device = device,\n",
    "                                    device_generation = device if torch.cuda.device_count() <= 1 else torch.device('cuda:1'),\n",
    "                                    return_embeddings = True)\n",
    "print(generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_passages = 20\n",
    "\n",
    "lim_toks = 128\n",
    "eval_data = '/lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_valid.jsonl'\n",
    "train_data = '/lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_train.jsonl'\n",
    "train_passage = []\n",
    "valid_passage = []\n",
    "\n",
    "with open(train_data, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == n_passages:\n",
    "            break\n",
    "        train_passage.append(pipeline.tokenizer.decode(pipeline.tokenizer.encode(json.loads(line)['text'].split('\\n\\n')[1], eos = True, bos = True)[:lim_toks]))\n",
    "  \n",
    "with open(eval_data, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == n_passages:\n",
    "            break\n",
    "        valid_passage.append(pipeline.tokenizer.decode(pipeline.tokenizer.encode(json.loads(line)['text'].split('\\n\\n')[1], eos = True, bos = True)[:lim_toks]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_embeds = True\n",
    "temp = 0\n",
    "max_tokens = 128\n",
    "i_token_to_flip = -1\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "\n",
    "temp = [temp] * max_tokens   \n",
    "if max_tokens > i_token_to_flip >= 0:\n",
    "    temp[i_token_to_flip] = 1000\n",
    "\n",
    "# prompt_prefix = \"Query: who wrote the song photograph by ringo starr\\nAnswer: Ringo Starr\\n\\nQuery: who is playing the halftime show at super bowl 2016\\nAnswer: Coldplay\\n\\nQuery: where was the world economic forum held this year\\nAnswer: Davos\\n\\nQuery: where are the giant redwoods located in california\\nAnswer: Humboldt County\\n\\nQuery: who has made the most premier league appearances\\nAnswer: Gareth Barry\\n\\nQuery: \"\n",
    "prompt_prefix = \"Query: \"\n",
    "prompts = ['who has most followers on instagram in world','who did the united states win its independence from', 'locations for the film an englishman who went up a hill', 'who is the valley of the dolls based on']\n",
    "prompts = [prompt_prefix + prompt + '\\nAnswer:' for prompt in prompts]\n",
    "\n",
    "conditioning = [\"This list contains the top 50 accounts with the most followers on the photo and video-sharing social platform Instagram. As of July 2019, the most followed user is Instagram's own account, with over 308 million followers. Cristiano Ronaldo is the most followed individual, with over 177 million followers. Fifteen accounts have exceeded 100 million followers on the site.\",\n",
    "                     \"During the American Revolution, the legal separation of the thirteen colonies from Great Britain in 1776 actually occurred on July 2, when the Second Continental Congress voted to approve a resolution of independence that had been proposed in June by Richard Henry Lee of Virginia declaring the United States independent from Great Britain's rule. After voting for independence, Congress turned its attention to the Declaration of Independence, a statement explaining this decision, which had been prepared by a Committee of Five, with Thomas Jefferson as its principal author. Congress debated and revised the wording of the Declaration, finally approving it two days later on July 4. A day earlier, John Adams had written to his wife Abigail\",\n",
    "                     'The village was a primary location for the making of the film \\\"The Englishman Who Went Up a Hill But Came Down a Mountain\\\", which starred Hugh Grant. The hilltop scenes were filmed on the Gyrn, the long hill that overlooks the village. It was also featured in \\\"Monk\\'s Hood\\\", an episode of \\\"The Cadfael Chronicles\\\"',\n",
    "                     'Valley of the Dolls is the first novel by American writer Jacqueline Susann. Published in 1966, the book was the biggest selling novel of its year. To date, it has sold more than 31 million copies, making it one of the best-selling works in publishing history.']\n",
    "\n",
    "# answers = ['Instagram','Great Britain',\"Llansilin in Powys\",[\"Judy Garland\", \"Carole Landis\", \"Dean Martin\", \"Ethel Merman\"]]\n",
    "\n",
    "# conditioning = 'Kyutai is a non-profit laboratory dedicated to open research in AI, founded in November 2023 by the iliad Group, CMA CGM and Schmidt Sciences.'\n",
    "# prompts =  prompt_prefix +'Query: when was founded Kyutai?\\nAnswer: '\n",
    "\n",
    "\n",
    "\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "pipeline.model.llm.pos_to_keep = []\n",
    "generated_sequence, logprobs = pipeline.generate(prompts = prompts, \n",
    "                                    text_conditioning = conditioning, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False,\n",
    "                                    random_flip = i_token_to_flip,\n",
    "                                    device = device,\n",
    "                                    device_generation = device if torch.cuda.device_count() <= 1 else torch.device('cuda:1'),\n",
    "                                    return_embeddings = False)\n",
    "print(generated_sequence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vizualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_att_weights, tokens = get_attention(generated_sequence[0], embeddings, pipeline.tokenizer, pipeline.model.llm, n_tokens = 20)\n",
    "# self_att_weights, tokens = get_attention('He played 12 seasons (243 games, 14 goals) in the Serie A for A.C. Milan, A.F.C. Fiorentina, Hellas Verona, Atalanta B.C. and Genoa C.F.C.', embeddings, pipeline.tokenizer, pipeline.model.llm, n_tokens = 20)\n",
    "\n",
    "head_view(self_att_weights, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_att_weights, tokens = get_attention(generated_sequence[0], embeddings, pipeline.tokenizer, pipeline.model.llm, n_tokens = 20)\n",
    "model_view([att[:,:,1:,1:] for att in self_att_weights], [tokens[0]] + tokens[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_per_head_att = [torch.mean(att, dim = 1, keepdim = True) for att in self_att_weights]\n",
    "overall_mean = torch.mean(torch.stack(self_att_weights), dim = 0)\n",
    "\n",
    "head_view(mean_per_head_att, tokens)\n",
    "# head_view([overall_mean], tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop)\n",
    "temperatures = [0, 0.5, 0.7, 1, 1.5]\n",
    "max_tokens = 150\n",
    "\n",
    "results_generation = {'0':{'train': {'word_prompt':{}, 'empty_prompt':{}}, 'valid': {'word_prompt':{}, 'empty_prompt':{}}}, \n",
    "                        '0.5':{'train': {'word_prompt':{}, 'empty_prompt':{}}, 'valid': {'word_prompt':{}, 'empty_prompt':{}}},\n",
    "                        '0.7':{'train': {'word_prompt':{}, 'empty_prompt':{}}, 'valid': {'word_prompt':{}, 'empty_prompt':{}}},\n",
    "                        '1':{'train': {'word_prompt':{}, 'empty_prompt':{}}, 'valid': {'word_prompt':{}, 'empty_prompt':{}}},\n",
    "                        '1.5':{'train': {'word_prompt':{}, 'empty_prompt':{}}, 'valid': {'word_prompt':{}, 'empty_prompt':{}}}}\n",
    "\n",
    "\n",
    "n_passages = len(train_passage)\n",
    "assert n_passages == len(valid_passage)\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f'Temperature: {temp}')    \n",
    "    generated_sequences = []\n",
    "    \n",
    "    for i in range(0, n_passages, max_batch_size):\n",
    "        passage = train_passage[i:i+max_batch_size]\n",
    "        generated_sequence, logprobs = pipeline.generate(prompts = [text.split(' ')[0] for text in passage], \n",
    "                                    text_conditioning = passage, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False,\n",
    "                                     device = device,\n",
    "                                    device_generation = device if torch.cuda.device_count() <= 1 else torch.device('cuda:1'))\n",
    "           \n",
    "        generated_sequences.extend(generated_sequence)\n",
    "    results_generation[str(temp)]['train']['word_prompt'] = {'seq':generated_sequences}\n",
    "    print('Train Passage:', passage)\n",
    "    print('Train Generated:', generated_sequence)\n",
    "    generated_sequences = []\n",
    "    for i in range(0, n_passages, max_batch_size):\n",
    "        passage = train_passage[i:i+max_batch_size]\n",
    "        generated_sequence, logprobs = pipeline.generate(prompts = [''] * len(passage), \n",
    "                                    text_conditioning = passage, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False,\n",
    "                                     device = device,\n",
    "                                    device_generation = device if torch.cuda.device_count() <= 1 else torch.device('cuda:1'))\n",
    "           \n",
    "        generated_sequences.extend(generated_sequence)    \n",
    "    results_generation[str(temp)]['train']['empty_prompt'] = {'seq':generated_sequences}\n",
    "    \n",
    "\n",
    "    generated_sequences = []\n",
    "    for i in range(0, n_passages, max_batch_size):\n",
    "        passage = valid_passage[i:i+max_batch_size]\n",
    "        generated_sequence, logprobs = pipeline.generate(prompts = [text.split(' ')[0] for text in passage], \n",
    "                                    text_conditioning = passage, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False,\n",
    "                                     device = device,\n",
    "                                    device_generation = device if torch.cuda.device_count() <= 1 else torch.device('cuda:1'))\n",
    "           \n",
    "        generated_sequences.extend(generated_sequence)    \n",
    "    results_generation[str(temp)]['valid']['word_prompt'] = {'seq':generated_sequences}\n",
    "    \n",
    "    generated_sequences = []\n",
    "    for i in range(0, n_passages, max_batch_size):\n",
    "        passage = valid_passage[i:i+max_batch_size]\n",
    "        generated_sequence, logprobs = pipeline.generate(prompts = [''] * len(passage), \n",
    "                                    text_conditioning = passage, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False,\n",
    "                                     device = device,\n",
    "                                    device_generation = device if torch.cuda.device_count() <= 1 else torch.device('cuda:1'))\n",
    "           \n",
    "        generated_sequences.extend(generated_sequence)    \n",
    "    results_generation[str(temp)]['valid']['empty_prompt'] = {'seq':generated_sequences}\n",
    "    print('Valid Passage:', passage)\n",
    "    print('Valid Generated:', generated_sequence)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for temp in results_generation.keys():\n",
    "    for split in results_generation[temp].keys():\n",
    "        for prompt_type in results_generation[temp][split].keys():\n",
    "            generated_sequences = results_generation[temp][split][prompt_type]['seq']\n",
    "            if prompt_type == 'empty_prompt':\n",
    "                gt_passage = train_passage if split == 'train' else valid_passage\n",
    "                overlap = word_overlap(gt_passage, generated_sequences)\n",
    "                bleu_score = get_bleu_score(gt_passage, generated_sequences)\n",
    "            elif prompt_type == 'word_prompt':\n",
    "                gt_passage = train_passage if split == 'train' else valid_passage\n",
    "                gt_passage = [' '.join(text.split(' ')[1:]) for text in gt_passage]\n",
    "                overlap = word_overlap(gt_passage, generated_sequences)\n",
    "                bleu_score = get_bleu_score(gt_passage, generated_sequences)\n",
    "   \n",
    "            print(f'Temperature: {temp}, Split: {split}, Prompt Type: {prompt_type}, Overlap: {overlap}', 'Bleu Score:', bleu_score)\n",
    "            metrics.append({'temp': temp, 'split': split, 'prompt_type': prompt_type, 'overlap': overlap, 'bleu_score': bleu_score})\n",
    "            \n",
    "# with open(f'{ckpt_path}/results_generation.json', 'w') as f:\n",
    "#     json.dump(metrics, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_path = '/lustre/scwpod02/client/kyutai-interns/hippop/models/mistral_7B'\n",
    "#Must have a params json for pipeline\n",
    "\n",
    "# No embeddings:\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/no_embed_bs16_lr5e-5Mistral7B88d0b42410aa4ec12025/checkpoints/checkpoint_002500'\n",
    "\n",
    "# Length tokens:\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_512t_Mistral7B20ed0018b2a84fba09c4/checkpoints/checkpoint_005000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_256t_Mistral7Be9ffc00fa42bedbc50d0/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_128t_Mistral7B226729d875c65b331ef8/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_64t_Mistral7B9bbea1b3b8dc23079b04/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_32t_Mistral7Bccbc3f29d69bd124c6cf/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_16t_Mistral7B7bc7dcc2ba28873eda96/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/mean_not_causal/checkpoints/checkpoint_007500'\n",
    "\n",
    "\n",
    "# # Continuation:\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/continuation_Mistral7B20ed0018b2a84fba09c4/checkpoints/checkpoint_006000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/mean_finetuned_notcausal_continuationMistral7B20ed0018b2a84fba09c4/checkpoints/checkpoint_005500'\n",
    "\n",
    "# # Cross-Attention:\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/cross_att_5_last_layersMistral7Bdbbb7faebb2f32cf20e9/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/cross_att_fine_tuned_embedder_5_last_layersMistral7Bdbbb7faebb2f32cf20e9/checkpoints/checkpoint_007500'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/cross_att_finetuned_notcausal_continuationMistral7B20ed0018b2a84fba09c4/checkpoints/checkpoint_005000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/cross_att_pretrained_continuationMistral7B20ed0018b2a84fba09c4/checkpoints/checkpoint_008500'\n",
    "\n",
    "with open(f'{ckpt_path}/params.json') as f:\n",
    "    params = json.load(f)\n",
    "print(params)\n",
    "\n",
    "model_name = 'Mistral7B' # Mistral7B, Llama3.2-3B, Gemma7B\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "w_embeds = True\n",
    "max_batch_size = 4\n",
    "\n",
    "# variant = '7b' if model_name == 'Gemma7B' else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify old params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ckpt_path + '/params.json') as f:\n",
    "    params = json.load(f)\n",
    "print(params)\n",
    "# if 'do_pool'  not in params.keys():\n",
    "if 'n_truncated_layers' in params['pooling_module'].keys():\n",
    "    params['n_truncated_layers'] = params['pooling_module']['n_truncated_layers']\n",
    "    del params['pooling_module']['n_truncated_layers']\n",
    "    \n",
    "\n",
    "if params['cross_att'] is not None:\n",
    "    print('here')\n",
    "    params['normalize_embeddings'] = True if params['cross_att'] else False\n",
    "    if params['start_cross_att'] is None:\n",
    "        del params['start_cross_att']\n",
    "    else:\n",
    "        params['cross_att_layers'] = 32 - params[\"start_cross_att\"]\n",
    "        del params['start_cross_att']\n",
    "    params['do_pool'] = False if params['cross_att'] else True\n",
    "else:\n",
    "    params['do_pool'] = True\n",
    "print(params)\n",
    "with open(ckpt_path + '/params.json', 'w') as f:\n",
    "    json.dump(params, f)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in tests:\n",
    "    print('Param:', param)\n",
    "    if param['w_embeds']:\n",
    "        pipeline.pipeline_args.w_embeds = True\n",
    "    else:\n",
    "        pipeline.pipeline_args.w_embeds = False\n",
    "    \n",
    "    final_valid_prompts = [passage.split(' ')[0] for passage in valid_passage][2] \n",
    "    text_valid_conditioning = [passage[:100] for passage in valid_passage][2]\n",
    "    print('Prompt', final_valid_prompts, ' | Passage', text_valid_conditioning)\n",
    "    generated_sequence = pipeline.generate(prompts = final_valid_prompts, \n",
    "                                        text_conditioning = text_valid_conditioning, \n",
    "                                        temperature = param['temperature'], \n",
    "                                        max_tokens = max_tokens,\n",
    "                                        truncate_double_space = False)\n",
    "    print('Valid  word', generated_sequence)\n",
    "    \n",
    "    final_valid_prompts = ['' for passage in train_passage][1]\n",
    "    text_valid_conditioning = [passage[:100] for passage in valid_passage][2]\n",
    "    generated_sequence = pipeline.generate(prompts = final_valid_prompts, \n",
    "                                        text_conditioning = text_valid_conditioning, \n",
    "                                        temperature = param['temperature'], \n",
    "                                        max_tokens = max_tokens,\n",
    "                                        truncate_double_space = False)\n",
    "    print('Valid  empty', generated_sequence)\n",
    "\n",
    "    final_train_prompts =  [passage.split(' ')[0] for passage in train_passage][1] \n",
    "    text_train_conditioning = [passage[:100] for passage in train_passage][1]\n",
    "    print('Prompt', final_train_prompts, ' | Passage', text_train_conditioning)\n",
    "    generated_sequence = pipeline.generate(prompts = final_train_prompts, \n",
    "                                       text_conditioning = text_train_conditioning, \n",
    "                                       temperature = param['temperature'], \n",
    "                                       max_tokens = max_tokens,\n",
    "                                       truncate_double_space = False)\n",
    "    print('Train word', generated_sequence)\n",
    "    final_train_prompts = ['' for passage in train_passage][1] \n",
    "    text_train_conditioning = [passage[:100] for passage in train_passage][1]\n",
    "    generated_sequence = pipeline.generate(prompts = final_train_prompts, \n",
    "                                       text_conditioning = text_train_conditioning, \n",
    "                                       temperature = param['temperature'], \n",
    "                                       max_tokens = max_tokens,\n",
    "                                       truncate_double_space = False)\n",
    "    print('Train empty', generated_sequence)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 information in the doc which enables to answer the question but not good response often in-context\n",
    "# 2 information in the doc which enables to answer the question and good response often in-context\n",
    "# 3 Hard negative passage\n",
    "# 4 Same\n",
    "\n",
    "# prompt_prefix = \"Query: who wrote the song photograph by ringo starr\\nAnswer: Ringo Starr\\n\\nQuery: who is playing the halftime show at super bowl 2016\\nAnswer: Coldplay\\n\\nQuery: where was the world economic forum held this year\\nAnswer: Davos\\n\\nQuery: where are the giant redwoods located in california\\nAnswer: Humboldt County\\n\\nQuery: who has made the most premier league appearances\\nAnswer: Gareth Barry\\n\\nQuery: \"\n",
    "# prompts = ['who has most followers on instagram in world','who did the united states win its independence from', 'locations for the film an englishman who went up a hill', 'who is the valley of the dolls based on']\n",
    "# final_prompts = [prompt_prefix + prompt + '\\nAnswer:' for prompt in prompts]\n",
    "\n",
    "# text_conditioning = [\"This list contains the top 50 accounts with the most followers on the photo and video-sharing social platform Instagram. As of July 2019, the most followed user is Instagram's own account, with over 308 million followers. Cristiano Ronaldo is the most followed individual, with over 177 million followers. Fifteen accounts have exceeded 100 million followers on the site.\",\n",
    "#                      \"During the American Revolution, the legal separation of the thirteen colonies from Great Britain in 1776 actually occurred on July 2, when the Second Continental Congress voted to approve a resolution of independence that had been proposed in June by Richard Henry Lee of Virginia declaring the United States independent from Great Britain's rule. After voting for independence, Congress turned its attention to the Declaration of Independence, a statement explaining this decision, which had been prepared by a Committee of Five, with Thomas Jefferson as its principal author. Congress debated and revised the wording of the Declaration, finally approving it two days later on July 4. A day earlier, John Adams had written to his wife Abigail\",\n",
    "#                      'The village was a primary location for the making of the film \\\"The Englishman Who Went Up a Hill But Came Down a Mountain\\\", which starred Hugh Grant. The hilltop scenes were filmed on the Gyrn, the long hill that overlooks the village. It was also featured in \\\"Monk\\'s Hood\\\", an episode of \\\"The Cadfael Chronicles\\\"',\n",
    "#                      'Valley of the Dolls is the first novel by American writer Jacqueline Susann. Published in 1966, the book was the biggest selling novel of its year. To date, it has sold more than 31 million copies, making it one of the best-selling works in publishing history.']\n",
    "\n",
    "# answers = ['Instagram','Great Britain',\"Llansilin in Powys\",[\"Judy Garland\", \"Carole Landis\", \"Dean Martin\", \"Ethel Merman\"]]\n",
    "\n",
    "n_passages = 4\n",
    "eval_data = '/lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_valid.jsonl'\n",
    "train_data = '/lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_train.jsonl'\n",
    "train_passage = []\n",
    "valid_passage = []\n",
    "with open(train_data, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == n_passages:\n",
    "            break\n",
    "        train_passage.append(json.loads(line)['text'].split('\\n\\n')[1])\n",
    "        \n",
    "with open(eval_data, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == n_passages:\n",
    "            break\n",
    "        valid_passage.append(json.loads(line)['text'].split('\\n\\n')[1])\n",
    "        \n",
    "tests = [{'w_embeds': True, 'temperature': 0 },  {'w_embeds': True, 'temperature': 0.7 }, {'w_embeds': False, 'temperature': 0.7 }]\n",
    "# print('Train passage:', train_passage)\n",
    "# print('Valid passage:', valid_passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditioning = ['Kyutai is a non-profit laboratory dedicated to open research in AI, founded in November 2023 by the iliad Group, CMA CGM and Schmidt Sciences. Launched with an initial team of six leading scientists, who have all worked with Big Tech labs in the USA, Kyutai continues to recruit at the highest level, and also offers internships to research Master’s degree students.']*4\n",
    "prompts = ['who are the founders of Kyutai?', 'when was Kyutai founded?', 'how many scientists were in the initial team?', 'what does Kyutai offer to research Master’s degree students?']\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "generated_sequence = pipeline.generate(prompts = prompts,\n",
    "                                      text_conditioning = conditioning,\n",
    "                                      temperature = 0.5, \n",
    "                                      max_tokens =200,\n",
    "                                      truncate_double_space = False)\n",
    "# random_flip, put the number of the token to flip. \n",
    "print(generated_sequence)\n",
    "\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "generated_sequence, logprobs = pipeline.generate(prompts = ['who has most followers on Instagram in world?'],\n",
    "                                      text_conditioning = [\"This list contains the top 50 accounts with the most followers on the photo and video-sharing social platform Instagram. As of July 2019, the most followed user is Instagram's own account, with over 308 million followers. Cristiano Ronaldo is the most followed individual, with over 177 million followers.\"],\n",
    "                                      temperature = 0.4, \n",
    "                                      max_tokens =200,\n",
    "                                      truncate_double_space = False)\n",
    "print(generated_sequence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuation\n",
    "for param in tests:\n",
    "    print('Param:', param)\n",
    "    if param['w_embeds']:\n",
    "        pipeline.pipeline_args.w_embeds = True\n",
    "    else:\n",
    "        pipeline.pipeline_args.w_embeds = False\n",
    "    \n",
    "    final_valid_prompts = [passage[100:].split(' ')[0] for passage in valid_passage][2] \n",
    "    text_valid_conditioning = [passage[:100] for passage in valid_passage][2]\n",
    "    print('Passage', text_valid_conditioning, ' | Truth', [passage[100:200] for passage in valid_passage][2] )\n",
    "    generated_sequence = pipeline.generate(prompts = final_valid_prompts, \n",
    "                                        text_conditioning = text_valid_conditioning, \n",
    "                                        temperature = param['temperature'], \n",
    "                                        max_tokens = max_tokens,\n",
    "                                        truncate_double_space = False)\n",
    "    print('Valid  word', generated_sequence)\n",
    "    \n",
    "    final_valid_prompts = ['' for passage in train_passage][2]\n",
    "    text_valid_conditioning = [passage[:100] for passage in valid_passage][2]\n",
    "    generated_sequence = pipeline.generate(prompts = final_valid_prompts, \n",
    "                                        text_conditioning = text_valid_conditioning, \n",
    "                                        temperature = param['temperature'], \n",
    "                                        max_tokens = max_tokens,\n",
    "                                        truncate_double_space = False)\n",
    "    print('Valid  empty', generated_sequence)\n",
    "\n",
    "    final_train_prompts =  [passage[100:].split(' ')[0] for passage in train_passage][1] \n",
    "    text_train_conditioning = [passage[:100] for passage in train_passage][1]\n",
    "    print('Passage', text_train_conditioning, ' | Truth', [passage[100:200] for passage in train_passage][1] )\n",
    "    generated_sequence = pipeline.generate(prompts = final_train_prompts, \n",
    "                                       text_conditioning = text_train_conditioning, \n",
    "                                       temperature = param['temperature'], \n",
    "                                       max_tokens = max_tokens,\n",
    "                                       truncate_double_space = False)\n",
    "    print('Train word', generated_sequence)\n",
    "    final_train_prompts = ['' for passage in train_passage][1] \n",
    "    text_train_conditioning = [passage[:100] for passage in train_passage][1]\n",
    "    generated_sequence = pipeline.generate(prompts = final_train_prompts, \n",
    "                                       text_conditioning = text_train_conditioning, \n",
    "                                       temperature = param['temperature'], \n",
    "                                       max_tokens = max_tokens,\n",
    "                                       truncate_double_space = False)\n",
    "    print('Train empty', generated_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tested\n",
    "# run_name = '128_SL_FN_False_0_MLP_False_CA_False_DB'\n",
    "# run_name = '128_SL_FN_False_0_MLP_True_CA_16_CAL_False_SKV_False_DB'\n",
    "# run_name = '128_SL_FN_False_0_MLP_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_0_MLP_True_CA_24_CAL_False_SKV_True_DB'\n",
    "\n",
    "# Finished runs:\n",
    "\n",
    "# run_name = '128_SL_FN_Truemean_1_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB' # 008500\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_4_TRUNC_True_CA_5_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_4_TRUNC_True_CA_24_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_1_MLP_4_TRUNC_True_CA_5_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_1_MLP_4_TRUNC_True_CA_24_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_0_MLP_4_TRUNC_True_CA_5_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_0_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_0_MLP_4_TRUNC_True_CA_24_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truelatent_attention_0_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truelatent_attention_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Trueeos_0_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Trueeos_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truereversed_latent_atttention_0_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truereversed_latent_attention_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_0_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_False_DB'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_False_DB'\n",
    "# run_name = '128_SL_FN_Truemean_0_MLP_8_TRUNC_True_CA_16_CAL_False_SKV_False_DB'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_8_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_False_DB'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB_dist_process'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_4_TRUNC_False_CA_False_DB'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_8_TRUNC_False_CA_False_DB'\n",
    "# run_name = '128_SL_FN_Truemean_0_MLP_4_TRUNC_False_CA_False_DB'\n",
    "# run_name = '128_SL_FN_Truemean_0_MLP_8_TRUNC_False_CA_False_DB'\n",
    "# run_name = '128_SL_FN_False_3_MLP_True_CA_5_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_3_MLP_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_3_MLP_True_CA_24_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_1_MLP_True_CA_5_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_1_MLP_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_1_MLP_True_CA_24_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_0_MLP_True_CA_5_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_3_MLP_True_CA_16_CAL_False_SKV_False_DB'\n",
    "# run_name = '128_SL_FN_False_3_MLP_False_CA_False_DB'\n",
    "# run_name = '128_SL_FN_False_3_MLP_True_CA_16_CAL_True_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_3_MLP_True_CA_16_CAL_False_SKV_True_DB_dist_process'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/ToyDecompressingTests_LLM_FT_MaxEmb_1_one_over_2/checkpoints/checkpoint_020000/instruct.json'\n",
    "\n",
    "dico = {'do': True, 'kl_pretraining': False, 'alpha': 1.0, 'tune_llm': True, 'tune_embedder': False, 'decompress_usage': 'one_over_two_reconstruction'}\n",
    "\n",
    "with open(path, 'w') as f:\n",
    "    json.dump(dico, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
