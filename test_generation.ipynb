{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hippolytepilchen/micromamba/envs/llm_embed/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from embed_llm.models.augmented_model import EmbedAugPipeline\n",
    "from embed_llm.generation.evaluation import  ensure_reproducibility\n",
    "from bertviz import head_view, model_view # type: ignore\n",
    "from embed_llm.models.mistral.generate import get_attention\n",
    "from embed_llm.generation.metrics import (\n",
    "    word_overlap,\n",
    "    get_bleu_score,\n",
    "    get_meteor,\n",
    "    get_em,\n",
    "    get_f1_score,\n",
    "    metric_max_over_ground_truths,\n",
    "    get_approx_em\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from embed_llm.generation.evaluation import (create_prompt, create_prompt_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_reproducibility(29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 for loading\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Must have a params json for pipeline\n",
    "llm_path = '/lustre/scwpod02/client/kyutai-interns/hippop/models/mistral_7B'\n",
    "\n",
    "# run_name = 'Hybrid_LLM_True_Emb_True_MaxEmb_1_PNoEmbed_0.01_StartPoint_0.0_16BS\n",
    "# run_name = 'Hybrid_LLM_False_Emb_False_MaxEmb_1_PNoEmbed_0.0_StartPoint_0.8_16BS'\n",
    "# run_name = 'Hybrid_LLM_False_Emb_True_MaxEmb_1_PNoEmbed_0.0_StartPoint_0.8_16BS'\n",
    "# run_name = 'ToyPretraining_LLM_False_Emb_True_MaxEmb_1_pure_reconstruct_16BS'\n",
    "# run_name = 'Hybrid_LLM_False_Emb_True_MaxEmb_1_PNoEmbed_0.0_StartPoint_0.5_16BS' # model pourri\n",
    "# run_name = 'ToyPretraining_LLM_False_Emb_False_MaxEmb_1_0.2cont_0alpha_16BS_tmp'\n",
    "run_name = 'ToyPretraining_LLM_False_Emb_False_MaxEmb_1_0.2cont_0alpha_16BS_tmp'\n",
    "last_ckpt = '030000' # '008500' #'010000' \n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    device = torch.device('cuda:0')\n",
    "    print(f'Using {device} for loading')\n",
    "\n",
    "w_embeds = True\n",
    "max_batch_size = 4\n",
    "\n",
    "instruct_ckpt = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/Instruct_mid_1embeds_alpha0_lowlr_newdata/checkpoints/checkpoint_010000'\n",
    "# instruct_ckpt =\"/lustre/scwpod02/client/kyutai-interns/hippop/tmp/ToyDecompressingTests_LLM_FT_MaxEmb_true_reversed/checkpoints/checkpoint_020000\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:04<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If n_layers is 1, hidden_dim must be equal to out_dim, \n",
      " but hidden_dim is not equal to out_dim so hidden_dim is set to out_dim\n",
      "Loading MLP projector\n"
     ]
    }
   ],
   "source": [
    "pipeline: EmbedAugPipeline = EmbedAugPipeline.load_inference_model(\n",
    "    llm_path=llm_path,\n",
    "    ckpt_path=\"/lustre/scwpod02/client/kyutai-interns/hippop/tmp/\"\n",
    "    + run_name\n",
    "    + \"/checkpoints/checkpoint_\"\n",
    "    + last_ckpt,\n",
    "    device=device,\n",
    "    llm_name=\"Mistral7B\",\n",
    "    embed_model_name=\"NVEmbed\",  # Not used if pretrainde ckpt available\n",
    "    max_batch_size=max_batch_size,\n",
    "    instruct_ckpt=instruct_ckpt,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QA tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = '/lustre/scwpod02/client/kyutai-interns/hippop/processed_data/eval_QA_NVEmbed/nq_open_data.jsonl' # nq_data.jsonl\n",
    "context = []\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "\n",
    "with open(eval_data, \"r\") as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                questions.append(data[\"question\"].strip())\n",
    "\n",
    "                if isinstance(data[\"answer\"], str):\n",
    "                    answers.append([data[\"answer\"].strip()])\n",
    "\n",
    "                elif isinstance(data[\"answer\"], list):\n",
    "                    answers.append(data[\"answer\"])\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid answer type\")\n",
    "                # Take the first ranked retrieved passage\n",
    "                context.append(data[\"passages\"][0].strip())\n",
    "\n",
    "c = list(zip(questions, context, answers))\n",
    "fixed_random = random.Random(0.23)\n",
    "fixed_random.shuffle(c)\n",
    "questions, context, answers = zip(*c)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'м'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.tokenizer.encode('м',bos = False, eos = False)\n",
    "pipeline.tokenizer.decode(1899)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Prefix:\n",
      " Query: who plays bianca in that's so raven\n",
      "Answer: Erica Rivera\n",
      "\n",
      "Query: who was elected president in mexico in 2000\n",
      "Answer: Vicente Fox\n",
      "\n",
      "Query: where was one direction what makes you beautiful filmed\n",
      "Answer: Malibu, California\n",
      "\n",
      "Query: how much money did it cost to make gta v\n",
      "Answer: 137\n",
      "\n",
      "Query: where was the movie i am number 4 filmed\n",
      "Answer: the Pittsburgh metropolitan area\n",
      "\n",
      "\n",
      "GENERATION\n",
      "Given Query: where does the term jack mormon come from\n",
      "GT Answer: ['Jackson County, Missouri']\n",
      "Context Given The term Jack Mormon is a slang term originating in nineteenth-century America. It was originally used to describe a person who was not a baptized member of the Church of Jesus Christ of Latter Day Saints but who was friendly to church members and Mormonism, sympathized with them, and/or took an active interest in their belief system. Sometime in the early- to mid-twentieth century, however, the term began to refer to an individual deemed by adherents of The Church of Jesus Christ of Latter-day Saints (LDS Church) to be an inactive or lapsed member of the LDS Church who, despite their personal religious viewpoint, maintained good relations with and positive feelings toward the church.[...]\n",
      "Answer in context? 0 \n",
      "Prediction: It is a term used to describe a Mormon who is not active in the church.мммммм \n",
      "\n",
      "\n",
      "Given Query: why is the indian ocean the warmest in the world\n",
      "GT Answer: ['human induced greenhouse warming']\n",
      "Context Given so dependent on this rainfall that many civilisations perished when the Monsoon failed in the past. The huge variability in the Indian Summer Monsoon has also occurred pre-historically, with a strong, wet phase 33,500–32,500 BP; a weak, dry phase 26,000–23,500 BC; and a very weak phase 17,000–15,000 BP, corresponding to a series of dramatic global events: Bølling-Allerød, Heinrich, and Younger Dryas. The Indian Ocean is the warmest ocean in the world. Long-term ocean temperature records show a rapid, continuous warming in the Indian Ocean, at about 1.2 °C (compared to 0.7 °C for the warm pool region) during 1901–2012. Research indicates that human induced greenhouse warming, and[...]\n",
      "Answer in context? 1 \n",
      "Prediction: The Indian Ocean is the warmest ocean in the world because it is located in the tropics, where the sun is strongest. \n",
      "\n",
      "\n",
      "Given Query: who played morticia in the addams family tv show\n",
      "GT Answer: ['Carolyn Sue Jones']\n",
      "Context Given Morticia was portrayed by Carolyn Jones in the TV series, and Halloween with the New Addams Family TV movie, and by Anjelica Huston in The Addams Family and its sequel Addams Family Values. Huston's portrayal of Morticia was always illuminated by a ghostly glow around the eyes, which became most noticeable when she was standing or lying in dim light. Daryl Hannah played Morticia in the 1998 film Addams Family Reunion. Canadian actress Ellie Harvie played Morticia in the revival series, The New Addams Family. In the first animated series made in 1973, Morticia was voiced by Janet Waldo. Jones also voiced this character in an episode of The New Scooby-Doo Movies[...]\n",
      "Answer in context? 0 \n",
      "Prediction: Carolyn Jonesммммммммммммммммммммм \n",
      "\n",
      "\n",
      "Given Query: where did an independence movement occur because of the congress of vienna\n",
      "GT Answer: ['Italy', 'Norway']\n",
      "Context Given For several decades, the Polish national movement gave priority to the immediate restoration of independence, a drive that found expression in a series of armed rebellions. The insurgencies arose mainly in the Russian zone of partition to the east, about three-quarters of which was formerly Polish territory. After the Congress of Vienna, Russia had organized its Polish lands as the Congress Poland of Poland, granting it a quite liberal constitution, its own army, and limited autonomy within the tsarist empire. In the 1820s, however, Russian rule grew more arbitrary, and secret societies were formed by intellectuals in several cities to plot an insurrection. In November 1830, Polish troops in Warsaw rose in[...]\n",
      "Answer in context? 0 \n",
      "Prediction: Polandiëiëiëїїїї \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w_embeds = True\n",
    "temp = 0\n",
    "max_tokens = 64\n",
    "icl_examples = 5\n",
    "max_bs = 4\n",
    "rag = False\n",
    "\n",
    "device_count = torch.cuda.device_count()\n",
    "other_device = device if device_count <= 1 else torch.device(\"cuda:1\")\n",
    "\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "    \n",
    "prompt_prefix = create_prompt_prefix(\n",
    "    queries=questions,\n",
    "    answers=[answer[0] for answer in answers],\n",
    "    docs=context if rag and not w_embeds else None,\n",
    "    max_examples=icl_examples,\n",
    ")\n",
    "\n",
    "generated_sequences = []\n",
    "\n",
    "queries = list(questions[icl_examples:])\n",
    "docs = list(context[icl_examples:])\n",
    "truths = list(answers[icl_examples:])\n",
    "queries.reverse()\n",
    "docs.reverse()\n",
    "truths.reverse()\n",
    "\n",
    "if w_embeds:\n",
    "\n",
    "    no_context_prompt = [\n",
    "        create_prompt(\n",
    "            prefix=prompt_prefix, doc=\"\", query=query, wdoc=False\n",
    "        )\n",
    "        for query in queries[:max_bs]\n",
    "    ]\n",
    "\n",
    "    context_prompt = [\n",
    "        create_prompt(\n",
    "            prefix=\" answer the question following the examples:\\n\\n\"\n",
    "            + prompt_prefix,\n",
    "            doc=\"\",\n",
    "            query=query,\n",
    "            wdoc=False,\n",
    "        )\n",
    "        for query in queries[:max_bs]\n",
    "    ]\n",
    "\n",
    "else:\n",
    "\n",
    "\n",
    "    no_context_prompt = [\n",
    "        create_prompt(\n",
    "            prefix=prompt_prefix,\n",
    "            doc=doc if rag else '',\n",
    "            query=query,\n",
    "            wdoc=True,\n",
    "        )\n",
    "        for query, doc in zip(\n",
    "            queries[:max_bs],\n",
    "            docs[:max_bs],\n",
    "        )\n",
    "    ]\n",
    "\n",
    "\n",
    "# for i, cont in enumerate(context_prompt if pipeline.pipeline_args.w_prefix_prompt else no_context_prompt):\n",
    "#     print(f'{i}',cont)\n",
    "\n",
    "\n",
    "print('Prompt Prefix:\\n', prompt_prefix)\n",
    "    # print(no_context_prompt[i] if not pipeline.pipeline_args.w_prefix_prompt else context_prompt[i])\n",
    "    # print(f'Ground truth answer: {a}\\n')\n",
    "generated_sequence = pipeline.generate(\n",
    "    prompt_pre_embed= (['']*len(queries[:max_bs]) if not pipeline.pipeline_args.w_prefix_prompt \n",
    "        else ['Based on the context ']*len(queries[:max_bs])),\n",
    "    prompt_post_embed = context_prompt if pipeline.pipeline_args.w_prefix_prompt else no_context_prompt,\n",
    "    text_conditioning= docs[:max_bs] if w_embeds else None,\n",
    "    temperature=temp,\n",
    "    max_tokens=max_tokens,\n",
    "    truncate_line=True,\n",
    "    device=device,\n",
    "    device_generation=other_device,\n",
    ")\n",
    "\n",
    "print('GENERATION')\n",
    "for i, (q, a, d, g) in enumerate(zip(queries[:max_bs], truths[:max_bs], docs[:max_bs], generated_sequence)):\n",
    "    print(f'Given Query: {q}\\nGT Answer: {a}\\nContext Given {d}[...]\\nAnswer in context? {get_approx_em(d,a[0])}','\\nPrediction:', g, '\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconstruct tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniiniini \n",
      "\n",
      "\n",
      "A written by Bob Russell and Bob Hebert written by Bob Russell written by Bob He He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written by Bob He written by Bob Russell written \n",
      "\n",
      "\n",
      "1717111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w_embeds = True\n",
    "temp = 0\n",
    "max_tokens = 256\n",
    "n_context = 0\n",
    "i_token_to_flip = -1\n",
    "\n",
    "device_count = torch.cuda.device_count()\n",
    "other_device = device if device_count <= 1 else torch.device(\"cuda:1\")\n",
    "\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "\n",
    "temp = [temp] * max_tokens   \n",
    "if max_tokens > i_token_to_flip >= 0:\n",
    "    temp[i_token_to_flip] = -1\n",
    "\n",
    "# 1 information in the doc which enables to answer the question but not good response often in-context\n",
    "# 2 information in the doc which enables to answer the question and good response often in-context\n",
    "# 3 Hard negative passage\n",
    "\n",
    "\n",
    "\n",
    "prompt_prefix = [\"Query: who wrote the song photograph by ringo starr\\nAnswer: Ringo Starr\\n\\n\",\"Query: who is playing the halftime show at super bowl 2016\\nAnswer: Coldplay\\n\\n\",\"Query: where was the world economic forum held this year\\nAnswer: Davos\\n\\n\",\"Query: where are the giant redwoods located in california\\nAnswer: Humboldt County\\n\\n\",\"Query: who has made the most premier league appearances\\nAnswer: Gareth Barry\\n\\n\"]\n",
    "prompt_prefix = ''.join(prompt_prefix[:n_context])\n",
    "# prompt_prefix = \"Query: \"\n",
    "\n",
    "# prompts = ['who has most followers on instagram in world','who did the united states win its independence from', 'what is the capital of France']\n",
    "prompts = [\"what is the hot coffee mod in san andreas\",\"who wrote he ain't heavy he's my brother lyrics\",\"when was the last time anyone was on the moon\"]\n",
    "context_prompts = [' answer the question following the examples ' +prompt_prefix + 'Query: '+ prompt  + '\\nAnswer:' for prompt in prompts]\n",
    "# no_context_prompts = [prompt_prefix + 'Query: '+ prompt + '\\nAnswer:' for prompt in prompts]\n",
    "no_context_prompts = ['' for prompt in prompts]\n",
    "\n",
    "# conditioning = [\"This list contains the top 50 accounts with the most followers on the photo and video-sharing social platform Instagram. As of July 2019, the most followed user is Instagram's own account, with over 308 million followers. Cristiano Ronaldo is the most followed individual, with over 177 million followers. Fifteen accounts have exceeded 100 million followers on the site.\",\n",
    "#                 \"During the American Revolution, the legal separation of the thirteen colonies from Great Britain in 1776 actually occurred on July 2, when the Second Continental Congress voted to approve a resolution of independence that had been proposed in June by Richard Henry Lee of Virginia declaring the United States independent from Great Britain's rule. After voting for independence, Congress turned its attention to the Declaration of Independence, a statement explaining this decision, which had been prepared by a Committee of Five, with Thomas Jefferson as its principal author. Congress debated and revised the wording of the Declaration, finally approving it two days later on July 4. A day earlier, John Adams had written to his wife Abigail\",\n",
    "#                 \"France is a country located primarily in Western Europe. Its overseas regions and territories include French Guiana in South America, Saint Pierre and Miquelon in the North Atlantic, the French West Indies, and many islands in Oceania and the Indian Ocean, giving it one of the largest discontiguous exclusive economic zones in the world. Metropolitan France shares borders with Belgium and Luxembourg to the north, Germany to the northeast, Switzerland to the east, Italy and Monaco to the southeast, Andorra and Spain to the south, and a maritime border with the United Kingdom to the northwest. Its metropolitan area extends from the Rhine to the Atlantic Ocean and from the Mediterranean Sea to the English Channel and the North Sea. \"]\n",
    "\n",
    "conditioning = ['Hot Coffee is a normally inaccessible mini-game in Grand Theft Auto: San Andreas. The mini-game portrays crudely animated sexual intercourse between the main character and a chosen partner. After Patrick Wildenborg, a software engineer who also went by the alias \\\"PatrickW\\\", modified the game to make the mini-game accessible, Hot Coffee quickly gained notoriety worldwide, impacting consumer culture, politics and the video game industry as a whole. Rockstar initially blamed a \\\"determined group of hackers\\\" for hacking the base game and creating the mini-game from scratch. This claim was eventually refuted, as the mini-game\\'s code and assets had been developed by Rockstar and were already present, unfinished and abandoned, on the game disc: the mod simply made the existing content available to players. Rockstar would go on to indicate that they expected the ESRB rating to remain unchanged, as they had no control',\n",
    "                \"He Ain't Heavy, He's My Brother written by Bob Russell and Bobby Scott; all other titles written by Neil Diamond.\",\n",
    "                \"17, 1970, as part of the Lunokhod program. To date, the last human to stand on the Moon was Eugene Cernan, who as part of the Apollo 17 mission, walked on the Moon in December 1972. Apollo 17 was followed by several uncrewed interplanetary missions operated by NASA. One of the notable interplanetary missions is Voyager 1, the first artificial object to leave our Solar System into interstellar space on August 25, 2012. It is also the most distant artificial object from Earth. The probe passed the heliopause at 121 AU to enter interstellar space. Voyager 1 is currently at a distance of 145.11 AU (21.708 billion kilometers; 13.489 billion miles) from Earth as of January 1, 2019.\"]\n",
    "\n",
    "# answers = ['Instagram','Great Britain',\"Paris\"]\n",
    "# answers = [\"a normally inaccessible mini-game\", [\"Bobby Scott\", \"Bob Russell\"],[\"14 December 1972 UTC\", \"December 1972\"]]\n",
    "\n",
    "# conditioning = 'Kyutai is a non-profit laboratory dedicated to open research in AI, founded in November 2023 by the iliad Group, CMA CGM and Schmidt Sciences.'\n",
    "# prompts =  prompt_prefix +'Query: when was founded Kyutai?\\nAnswer: '\n",
    "\n",
    "\n",
    "\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "\n",
    "generated_sequence = pipeline.generate(\n",
    "    prompt_pre_embed = (['']*len(conditioning) if not pipeline.pipeline_args.w_prefix_prompt \n",
    "    else ['Based on the context ']*len(conditioning)), \n",
    "    prompt_post_embed = context_prompts if pipeline.pipeline_args.w_prefix_prompt  else no_context_prompts,\n",
    "    text_conditioning=conditioning,\n",
    "    temperature=temp,\n",
    "    max_tokens=max_tokens,\n",
    "    truncate_double_space=True,\n",
    "    device=device,\n",
    "    device_generation=other_device,\n",
    ")\n",
    "\n",
    "for seq in generated_sequence:\n",
    "    print(seq, '\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim_toks = pipeline.pipeline_args.max_seq_len\n",
    "\n",
    "atlas_eval_data = \"/lustre/scwpod02/client/kyutai-interns/hippop/processed_data/wiki_passages_pretraining/valid_atlas_enwiki-dec2021_standard.jsonl\"\n",
    "atlas_valid_passage = []\n",
    "dump_eval_data = '/lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_valid.jsonl'\n",
    "dump_valid_passage = []\n",
    "\n",
    "with open(atlas_eval_data, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        atlas_valid_passage.append(\n",
    "            pipeline.tokenizer.decode(\n",
    "                pipeline.tokenizer.encode(\n",
    "                    json.loads(line)[\"text\"], eos=True, bos=True\n",
    "                )[:lim_toks]\n",
    "            )\n",
    "            )\n",
    "random.shuffle(atlas_valid_passage)\n",
    "\n",
    "\n",
    "with open(dump_eval_data, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        text = json.loads(line)[\"text\"]\n",
    "        dump_valid_passage.append(\n",
    "                pipeline.tokenizer.decode(\n",
    "                    pipeline.tokenizer.encode(\n",
    "                        text if \"\\n\\n\" not in   text else text.split(\"\\n\\n\")[1], eos=True, bos=True\n",
    "                    )[:lim_toks]\n",
    "                )\n",
    "            )\n",
    "random.shuffle(dump_valid_passage)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_passages = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 0.0\n",
    "device_count = torch.cuda.device_count()\n",
    "other_device = device if device_count <= 1 else torch.device(\"cuda:1\")\n",
    "atlas_generated_sequences = []\n",
    "for i in range(0, n_passages, max_batch_size):\n",
    "    passage = atlas_valid_passage[i : i + max_batch_size]\n",
    "    generated_sequence, logprobs = pipeline.generate(\n",
    "        prompt_pre_embed = (['']*len(passage) if not pipeline.pipeline_args.w_prefix_prompt \n",
    "        else ['In other words, background: ']*len(passage)), \n",
    "        prompt_post_embed = (['']*len(passage) if not pipeline.pipeline_args.w_prefix_prompt \n",
    "        else [' is just another way of saying: ']*len(passage)),\n",
    "        text_conditioning=passage,\n",
    "        temperature=temp,\n",
    "        max_tokens=lim_toks,\n",
    "        truncate_double_space=False,\n",
    "        device=device,\n",
    "        device_generation=other_device,\n",
    "    )\n",
    "\n",
    "    atlas_generated_sequences.extend(generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 0.0\n",
    "device_count = torch.cuda.device_count()\n",
    "other_device = device if device_count <= 1 else torch.device(\"cuda:1\")\n",
    "dump_generated_sequences = []\n",
    "for i in range(0, n_passages, max_batch_size):\n",
    "    passage = dump_valid_passage[i : i + max_batch_size]\n",
    "    generated_sequence, logprobs = pipeline.generate(\n",
    "        prompt_pre_embed = (['']*len(passage) if not pipeline.pipeline_args.w_prefix_prompt \n",
    "        else ['In other words, background: ']*len(passage)), \n",
    "        prompt_post_embed = (['']*len(passage) if not pipeline.pipeline_args.w_prefix_prompt \n",
    "        else [' is just another way of saying: ']*len(passage)),\n",
    "        text_conditioning=passage,\n",
    "        temperature=temp,\n",
    "        max_tokens=lim_toks,\n",
    "        truncate_double_space=False,\n",
    "        device=device,\n",
    "        device_generation=other_device,\n",
    "    )\n",
    "\n",
    "    dump_generated_sequences.extend(generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG scores 0.8125772827951994\n",
      "AVG scores TRUNC 0.8183385157580503\n",
      "Very small bleu 1\n",
      "Mismatch 31\n"
     ]
    }
   ],
   "source": [
    "avg_bleu = 0\n",
    "n_mismatch = 0\n",
    "n_very_small_blue = 0\n",
    "for gen, gt in zip(dump_generated_sequences, dump_valid_passage):\n",
    "    bleu_score = get_bleu_score(gt, gen)\n",
    "    avg_bleu += bleu_score\n",
    "    \n",
    "    try:\n",
    "        if bleu_score != get_bleu_score(gt, gen, trunc = True):\n",
    "            n_mismatch += 1\n",
    "            # print('MISMATCH {}'.format(n_mismatch))\n",
    "            # print('Ge:', gen)\n",
    "            # print('GT:', gt)\n",
    "            # print(bleu_score, ' | ', get_bleu_score(gt, gen, trunc = True))\n",
    "            if bleu_score < 0.1:\n",
    "                n_very_small_blue += 1\n",
    "            \n",
    "        elif bleu_score < 0.1:\n",
    "            n_very_small_blue += 1\n",
    "            print('VERY SMALL BLEU {}'.format(n_very_small_blue))\n",
    "            print('Ge:', gen)\n",
    "            print('GT:', gt)\n",
    "            print(bleu_score)\n",
    "    except ValueError:\n",
    "        print('Passages skipped')\n",
    "\n",
    "print('AVG scores',get_bleu_score(dump_valid_passage, dump_generated_sequences))\n",
    "print('AVG scores TRUNC',get_bleu_score(dump_valid_passage, dump_generated_sequences, trunc = True))\n",
    "print('Very small bleu', n_very_small_blue)\n",
    "print('Mismatch', n_mismatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERY SMALL BLEU 1\n",
      "Ge: 0 A subsequence of a string whose characters are common in two strings (S1 and S2) can be computed efficiently using the notion of common subsequence. The problem has a simple algorithmic solution; both standard algorithms run in linear time in DSPSS (this is still faster than in Python) for a computer. It has been well studied in mathematics, its basic notions of deletions and appearances, the problem of finding Maximal Common Subsequences for sequences of four or more characters, for extra characters with few constraints that have the property of overlapping in more than one way. This problem (and its generalization to image alignment and image reconstruction) fits in studies of the geometry of editing theories (in Russian), dynamical systems, and computer science, and is motivated by the mean comparison of DNA sequences and the reconstruction of common subsequences at short edit distances. If one computes long subsequences of S1 and S2, such that the computations on such subsequences would not be revised by chance if they were returned from a computer. What one gets from this computation is a relation that is evidently useful to calculate the global similarity, not particularly biological or mathematical.\n",
      "GT:  A common subsequence of two strings S and T is a string whose characters appear in the same order (not necessarily consecutively) both in S and in T. The problem of computing a longest common subsequence has been well studied in computer science. It can be solved in polynomial time by dynamic programming; this basic algorithm has additional speedups for small alphabets (the Method of Four Russians), for strings with few differences, for strings with few matching pairs of characters, etc. This problem and its generalizations to more complex forms of edit distance have important applications in areas that include bioinformatics (in the comparison of DNA and protein sequences and the reconstruction of evolutionary trees), geology (in stratigraphy), and computer science (in data comparison and revision control). One motivation for studying the longest common subsequences of random strings, given already by Chvátal and Sankoff, is to calibrate the computations of longest common subsequences on strings that are not random. If such a computation returns a subsequence that is significantly longer than what would be obtained at random, one might infer from this result that the match is meaningful or significant.\n",
      "0.09710923867339806\n",
      "VERY SMALL BLEU 3\n",
      "Ge: Zapp brought the bones of this thing out into the world. \"All I say,\" says a documentary film, in which they made sure you were a track which really does sound exactly like Real Tunes. For the first time on record, Fender Rhodes can hear them - fanatic with a giant, tenth-tall way!\" and the fans decided on Grand Funk III to split up again only 4 days after they overdubbed their voices. All Zapp stayed until summer of 1985, trying to talk him out of it, although the album wasn't an 'unqualified success' in the US. The album became the first topping both releases to Billboard Top 200 Melter.\"\n",
      "GT:  bring the balls out of this thing.\" Zappa said, \"All I did was, in a documentary way, make a record which tells you exactly what they really sound like. For the first time on record you can hear Grand Funk Railroad ... and they're fantastic, fan-tastic with an F three times taller than you!\" Grand Funk decided on the first day of overdubs to split up again, although Zappa stayed until 4 a.m. trying to talk them out of it. The album wasn't a commercial success, only making it to #52 in the Billboard Top 200''. The album was the final release to feature both bassist Mel Schacher and keyboardist Craig Frost.\n",
      "0.09549153712369463\n",
      "VERY SMALL BLEU 4\n",
      "Ge: Bordeaux, most inland wines are serious bottles, although undergoing an ageing-process. Sometimes, white wine can be an exception in Bordeaux. The must of château-barrel ageing requires 6 barrels of must for each, but some new châteaux (which number as high as 20 meters above the oak barrel) can impart a different degree of ageing. Just recently, the volume of wine to be added to Bordeaux is heritage-protected. In order to make barrel ageing, the châteaux have to be racked from time to time in clear mold. This process is attacked by living organisms, as recently as itmakes sense since adding the châteaux to the blend can also be done. Additionally, periods of ageing es.eseseseseseseseses\n",
      "GT:  In Bordeaux, most serious wines undergo barrel-ageing, although white wines can be an exception. Usually, six months of ageing in-barrel is required, but some châteaux barrel-age for as much as 20 months. The number of new barrels (which impart a higher degree of oak flavor to the wine) can vary from vintage to vintage, just as the duration of barrel-ageing. Only recently, addition of oak chips has been made legal in Bordeaux. During barrel-ageing, the wine needs to be racked in order to clear it of lees. This process is being challenged by some producers, as mentioned abovem since ageing on the lees can also add richness to the wine.\n",
      "0.0\n",
      "VERY SMALL BLEU 5\n",
      "Ge: Mr. Critical was mostly positive review of both Monroe's resurrection, with fellow reviewers highlighting Wells's development of the character. Aviation Monthly's Jonathan Keller praised Monroe's complementary plotting as a \"clever balance between Savage and Wells for a piece of fiction\". Paul Nardini stated that Superman Had to Have Murdered Himself had never altered his own blog, saying: \"If I hadn't told my series intro earlier, I'd also be looking askance at the latter's failure to continue from the book\". Amazon.com commended Monroe's plotting as a \"strong counterpoint to any implications of the original story\", whereas Kenneth Beggs of Family Friendly Gaming described the book's \"realistic aspects\" and \"friendship with the main characters\". Keeping the vital fictional elements of the tale offers that the series addresses a \"nasty, chilling, and bloody mash-up of vampires and zombies\" with \"punctuated dialogue\". Audiences Magazine praised the book for its suitability for schoolchildren. Starfleet.com wrote, \"Monroe is a smart, powerful plotline while\n",
      "GT:  Critical reception of Mr. Monster was mostly positive, with multiple reviewers highlighting Wells's further development of both the protagonist and plot. A Savannah Morning News review complimented John Cleaver's characterization as \"a nifty balancing act for Wells to have pulled off\". Lee Mandelo praised Wells's expansion of Serial Killer into a series, stating: \"If his own blog hadn't told me otherwise, I would never have guessed he hadn't intended a sequel from the beginning\". Alternative Magazine also commended Wells's continuation of the storyline, saying that Mr. Monster \"immediately addresses any faults with its predecessor's ending\" while keeping the \"strong aspects of the original story\". Kirkus Reviews noted that \"John's realistic familial relationships and friendships offer a counterbalance to the bloody, fantastical elements of the tale\" and recommended the novel for \"fans of genre mash-ups\". A School Library Journal reviewer described Mr. Monster as \"compelling, quick-paced, and chilling\". Publishers Weekly wrote that the book \"stands out with taut, sharp writing, strong plotting,\n",
      "0.0\n",
      "Passages skipped\n",
      "Passages skipped\n",
      "VERY SMALL BLEU 7\n",
      "Ge: London-Smiths Blacksmiths, except backyard enterprises - Craftsmen chose blacksmiths to provide transportation, abbreviated trolley, and horse carriage shops. ; The Concert Hall that served as an entertainment venue for 1840s to 1884 was the first operating theatre in the area. ; Foresight Tavern - This community-owned tavern established the London Correspondence Lodges and provided meeting places for local freemasons like their masters. ; Mullet Lodge was constructed in Phase I. Powerful London St. Andrews Methodist Church buildings are similar to what could be found in the early 19th century when the first horse drawn trolley arrived from an Irish settlement. These included resort municipalities with orange hues like Orangeville, Ontario. Built in the area of the settlement where the townships of Huron also fanned out, many builders were establishing a reputation for themselves in the region, bringing a need for paved roads through the village. The community was representative of early London's streets, docks, lakes and canals. ; Concert Hall when it came to a decision to construct a traveling house for the town\n",
      "GT: Blacksmith Shop-Entrepreneurs, like blacksmiths, chose transportation crossroads to establish off-farm businesses. ; Corbett Tavern - An 1840s tavern that provided accommodation, food and stabling for horses to the traveling public and served as the community meeting place. ; Lochaber Church - This Free Presbyterian Church was constructed in 1884. ; Mount Moriah Lodge - Masonic orders held their first meetings in local taverns until purpose built halls like this one could be constructed. The interior is representative of an early 19th-century London Masonic Hall. ; Purple Hill Lodge - Established by the Protestant Irish immigrants who brought Orangeism with them when they came to Canada. Many settled areas in the region of what is now southwestern Ontario built meeting halls for the orange order, including townships in the London district. These buildings were also a focal point for the community, providing a place where settlers could get to know their neighbors through dances, dinners, recitals and concerts. Representative of the first stage of urban development at a transportation crossroads. \n",
      "0.0\n",
      "VERY SMALL BLEU 8\n",
      "Ge: Perry Botkin: Special effects, piano, lead guitar ; Mainly Joe Harnell: Piano, RCA Studio Orchestra ; David Levine: Guitar, Tom Ferguson: Guitar, Mike Reeves: Corrado. ; strings: Jesse Levy, Ben Cohen, Neil Stubenhaus, Bruce Ditmas, Pete Jolly, Sol Infante ; guitar: Izzy Sherr ; percussion: Grover Washington, Jr., Paul Enger, Jerry Zenkin, Harvie Swartz, Fred Seibert, David Laub, Amy Assante, Sahib Shihab, Joe Napolitano, Terry Nuttycombe, Rene Vinyl ; cellos: Allan Shrock, Ervin Roth ; strings: Kermit Moore, Bill Green, Gerald Friedman ; guitar: Hank Cicalo, Buddy Rich, Tony Levin, Getty Peterson, Gayle Levant, Blossom Allen, John Word ; flutes: Buddy Rich, Chuck Rainey, Bruce Barth ; flutes: Jim Peterson, CAPA, Seymour Barab ; contractor: Leo S. Brubaker ; father: George Palmer \n",
      "GT: Special effects: Perry Botkin, Jr. ; Piano: Tom Hensley, Mike Lang, Pete Jolly ; Guitar: Lee Ritenour, David Cohen, Neil Levang ; Bass: Max Bennett, Reinie Press, Steve LaFever ; drums: Joe Correro, Sol Gubin ; percussion: Gene Estes ; violins: Iz Baker, Paul Shure, Jerry Vinci, Sid Sharp, Tibor Zelig, Henry Ferber, Assa Drori, Jimmie Getzoff, Harry Bluestone, Erno Neufeld, Nate Ross ; violas: Dave Schwartz, Allan Harshman, Gerry Nuttycombe, Sven Reher ; cellos: Ray Kramer, Fred Seykora, Armand Kaproff ; woodwinds: Johnny Rotella, Gene Cipriano, Ronnie Lang, Bud Shank, Bill Green ; Harp: Gayle Levant ; trumpets: Bill Peterson, Bud Brisbois, Tony Terran, Cappy Lewis, Buddy Childers ; trombones: Charles Loper, Dick Nash ; Musical contractor: Charles H. Stern ; Engine\n",
      "0.0\n",
      "VERY SMALL BLEU 11\n",
      "Ge: volcanic vents to the East covered the area, and a fault trench formed during the eastward slope of the Fissure. The east side of the Fissure opens the valley of the Lander River, and the north side is the range, which was filled with diorite, while the hooping vents filled with porphyry. The fault trench composes these faults into \"mineral-bearing \"ore\". The faults called \"mineral ore\" stated that \"pockets are abundant\". The miners made the ore widely through the thin black slate of the East Fissure and nearly all their upper layer in \"like a charity case\", although only parts of the upper and lower layers were found or plowed. With them the fault trench extended in all directions, and \"rich sextuple ore\" was almost 600 ft wide.\n",
      "GT:  Volcanic vents to the east covered the area during the Tertiary, and a fault fissure opened the east slope of the Virginia Range. The east slope of the range forms the footwall of the Lode, and is composed of diorite, while the hanging wall is composed of andesite, which the miners called \"porphyry\". The fault fissures filled these fissures with \"mineral-bearing quartz\". The miners stated \"porphyry makes ore\". The ore bodies were thinly scattered through the wide Lode \"like plums in a charity pudding\", and nearly all of them were found in the wide upper section and along or near the east wall. Although the miners extended their work in all directions, only \"sixteen large and rich ore bodies\" were found, most less than 600 ft in depth.\n",
      "0.0\n",
      "VERY SMALL BLEU 12\n",
      "Ge: 6th from the Events in Ireland.............\n",
      "GT:  Events from the 6th century in Ireland.\n",
      "0.0\n",
      "VERY SMALL BLEU 13\n",
      "Ge: Halichard 4, Jens Kehrle 4, Hans Kuys 3, Asitsholts 3, Volodymyr Kolodzey 3. AAU Amsterdam, Netherlands: Amsterdam Arena, April 17-19, 1976 – UEFA National Final in Amsterdam 101:108 (wellmans in favour of VALL 59-79) Jens Kehrle:- 84, Steve Grazier, Bryce Wells, Frank Maravitch, Vivian Horton, Tucker Smith. 3rd All Star All-Star Week 1978-79 AAU Amsterdam, Netherlands: Universe 25-39 (Nag 15-21) IN: Hans Peters, Bill Croft, Gary Fenn, Dirk Hagens (16), John Lee 20-23, Steve Grazier 20-23, Emmett Ford 16-47, John Hankma, Pete Mildenhall, Everett Dawson, Carl Krapp: 131 Jopal Zone. N 2-1 (238).\n",
      "GT:  Heidrich 4, Hans Kaltschmidt 4, Jürgen Kolze 4, Volker Asshoff 3. Apollohal, Amsterdam, April 17, 1976: USA All Stars in Netherlands – USA All Stars in West Germany 109-89 (att:51-39) USA All Stars in Netherlands : Owen Wells, Steven Bravard, Gary Freeman, Vince Fritz, Tyrone Marioneaux, Buff Kirkland, Hank Smith. 5th All-Star Gala 1977-78 Apollohal, Amsterdam, April 23, 1978: North – South 158-165 NORTH (Jim Parks): Al Davis 33, Dan Henderson 21, Pete Miller 20, John Franken 20, Hank Smith 18, Gary Freeman 16, Everett Fopma 13, John Wayne Croft 9, Emill Hagens 7, Bert Kragtwijk 1. SOUTH (Manny Cramford): Billy Taylor 2\n",
      "0.0\n",
      "Error with update: \n",
      "Ground-Truth:  Untung Pakai Esia  \n",
      "Pred:  untuk Pangai Esia   \n",
      "AVG scores 0.41971318539118835\n",
      "Error with update: \n",
      "Ground-Truth:  Untung Pakai Esia  \n",
      "Pred:  untuk Pangai Esia \n",
      "AVG scores TRUNC 0.43351392936413774\n",
      "Very small bleu 13\n",
      "Mismatch 50\n"
     ]
    }
   ],
   "source": [
    "avg_bleu = 0\n",
    "n_mismatch = 0\n",
    "n_very_small_blue = 0\n",
    "for gen, gt in zip(atlas_generated_sequences, atlas_valid_passage):\n",
    "    try:\n",
    "        bleu_score = get_bleu_score(gt, gen)\n",
    "        avg_bleu += bleu_score\n",
    "    except ValueError:\n",
    "        print('Passages skipped')\n",
    "    \n",
    "    try:\n",
    "        if bleu_score != get_bleu_score(gt, gen, trunc = True):\n",
    "            n_mismatch += 1\n",
    "            # print('MISMATCH {}'.format(n_mismatch))\n",
    "            # print('Ge:', gen)\n",
    "            # print('GT:', gt)\n",
    "            # print(bleu_score, ' | ', get_bleu_score(gt, gen, trunc = True))\n",
    "            if bleu_score < 0.1:\n",
    "                n_very_small_blue += 1\n",
    "            \n",
    "        elif bleu_score < 0.1:\n",
    "            n_very_small_blue += 1\n",
    "            print('VERY SMALL BLEU {}'.format(n_very_small_blue))\n",
    "            print('Ge:', gen)\n",
    "            print('GT:', gt)\n",
    "            print(bleu_score)\n",
    "    except ValueError:\n",
    "        print('Passages skipped')\n",
    "\n",
    "print('AVG scores',get_bleu_score(atlas_valid_passage, atlas_generated_sequences))\n",
    "print('AVG scores TRUNC',get_bleu_score(atlas_valid_passage, atlas_generated_sequences, trunc = True))\n",
    "print('Very small bleu', n_very_small_blue)\n",
    "print('Mismatch', n_mismatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hippolytepilchen/.cache/huggingface/modules/transformers_modules/nvidia/NV-Embed-v2/5130cf1daf847c1bacee854a6ef1ca939e747fb2/modeling_nvembed.py:349: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(batch_dict.get('input_ids').to(batch_dict.get('input_ids')).long()),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Pre Embed: [8, 0]\n",
      "Prompt Post Embed: [10, 1]\n",
      "['\\n\\nMario Alberto Bortolazzi (born 12 January 1964) is a former professional footballer who played as a midfielder. He played for 12 seasons at A.C. Milan, winning the Serie A championship in 1991–92, and the Coppa Italia in 1993–94. He was transferred to Genoa C.F.C. for 500,000 lire, and made his debut on 25 September 1995, in a match against Fiorentina at', 'Mario Bortolazzi\\n\\nMario Bortolazzi (born 12 January 1965 in Genoa) is an Italian former professional football player and coach. He played 12 seasons in Serie A, for A.C. Milan, Atalanta, Fiorentina, and Hellas Verona. He played 246 games in Serie A, scoring 14 goals. 1990–91 season, he played 12 games in Serie B, for A.C. Milan. 1991–92 season,']\n"
     ]
    }
   ],
   "source": [
    "# Flipping attempts\n",
    "w_embeds = True\n",
    "temp = 0\n",
    "max_tokens = 128\n",
    "i_token_to_flip = -1\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "\n",
    "temp = [temp] * max_tokens   \n",
    "if max_tokens > i_token_to_flip >= 0:\n",
    "    temp[i_token_to_flip] = 1000\n",
    "    \n",
    "prompt = ''\n",
    "text_conditioning ='Mario Bortolazzi (born 10 January 1965, in Verona) is an Italian professional football coach and a former player, who played as a midfielder. \\\n",
    "    \\n\\nHe played 12 seasons (241 games, 14 goals) in the Serie A for ACF Fiorentina, A.C. Milan, Hellas Verona F.C., Atalanta B.C. and Genoa C.F.C.'\n",
    "        # \\n\\nIn his coaching career he has so far has always been an assistant to his former Milan teammate Roberto Donadoni.\\\n",
    "        #     \\n\\nHonours\\n\\n - Milan\\n - Serie A champion: 1987–88.\\n\\n - Genoa\\n - Anglo-Italian Cup winner: 1995–96.'\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "generated_sequence, attn_or_logprobs, embeddings = pipeline.generate(prompt_pre_embed = ['In other words, background: ',''], \n",
    "                                    prompt_post_embed = [' is just another way of saying: ',''],\n",
    "                                    text_conditioning = [text_conditioning]*2, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False,\n",
    "                                    random_flip = i_token_to_flip,\n",
    "                                    device = device,\n",
    "                                    device_generation = device if torch.cuda.device_count() <= 1 else torch.device('cuda:1'),\n",
    "                                    return_embeddings = True)\n",
    "print(generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_passages = 20\n",
    "\n",
    "lim_toks = 128\n",
    "eval_data = '/lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_valid.jsonl'\n",
    "train_data = '/lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_train.jsonl'\n",
    "train_passage = []\n",
    "valid_passage = []\n",
    "\n",
    "with open(train_data, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == n_passages:\n",
    "            break\n",
    "        train_passage.append(pipeline.tokenizer.decode(pipeline.tokenizer.encode(json.loads(line)['text'].split('\\n\\n')[1], eos = True, bos = True)[:lim_toks]))\n",
    "  \n",
    "with open(eval_data, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == n_passages:\n",
    "            break\n",
    "        valid_passage.append(pipeline.tokenizer.decode(pipeline.tokenizer.encode(json.loads(line)['text'].split('\\n\\n')[1], eos = True, bos = True)[:lim_toks]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_embeds = True\n",
    "temp = 0\n",
    "max_tokens = 128\n",
    "i_token_to_flip = -1\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "\n",
    "temp = [temp] * max_tokens   \n",
    "if max_tokens > i_token_to_flip >= 0:\n",
    "    temp[i_token_to_flip] = 1000\n",
    "\n",
    "# prompt_prefix = \"Query: who wrote the song photograph by ringo starr\\nAnswer: Ringo Starr\\n\\nQuery: who is playing the halftime show at super bowl 2016\\nAnswer: Coldplay\\n\\nQuery: where was the world economic forum held this year\\nAnswer: Davos\\n\\nQuery: where are the giant redwoods located in california\\nAnswer: Humboldt County\\n\\nQuery: who has made the most premier league appearances\\nAnswer: Gareth Barry\\n\\nQuery: \"\n",
    "prompt_prefix = \"Query: \"\n",
    "prompts = ['who has most followers on instagram in world','who did the united states win its independence from', 'locations for the film an englishman who went up a hill', 'who is the valley of the dolls based on']\n",
    "prompts = [prompt_prefix + prompt + '\\nAnswer:' for prompt in prompts]\n",
    "\n",
    "conditioning = [\"This list contains the top 50 accounts with the most followers on the photo and video-sharing social platform Instagram. As of July 2019, the most followed user is Instagram's own account, with over 308 million followers. Cristiano Ronaldo is the most followed individual, with over 177 million followers. Fifteen accounts have exceeded 100 million followers on the site.\",\n",
    "                     \"During the American Revolution, the legal separation of the thirteen colonies from Great Britain in 1776 actually occurred on July 2, when the Second Continental Congress voted to approve a resolution of independence that had been proposed in June by Richard Henry Lee of Virginia declaring the United States independent from Great Britain's rule. After voting for independence, Congress turned its attention to the Declaration of Independence, a statement explaining this decision, which had been prepared by a Committee of Five, with Thomas Jefferson as its principal author. Congress debated and revised the wording of the Declaration, finally approving it two days later on July 4. A day earlier, John Adams had written to his wife Abigail\",\n",
    "                     'The village was a primary location for the making of the film \\\"The Englishman Who Went Up a Hill But Came Down a Mountain\\\", which starred Hugh Grant. The hilltop scenes were filmed on the Gyrn, the long hill that overlooks the village. It was also featured in \\\"Monk\\'s Hood\\\", an episode of \\\"The Cadfael Chronicles\\\"',\n",
    "                     'Valley of the Dolls is the first novel by American writer Jacqueline Susann. Published in 1966, the book was the biggest selling novel of its year. To date, it has sold more than 31 million copies, making it one of the best-selling works in publishing history.']\n",
    "\n",
    "# answers = ['Instagram','Great Britain',\"Llansilin in Powys\",[\"Judy Garland\", \"Carole Landis\", \"Dean Martin\", \"Ethel Merman\"]]\n",
    "\n",
    "# conditioning = 'Kyutai is a non-profit laboratory dedicated to open research in AI, founded in November 2023 by the iliad Group, CMA CGM and Schmidt Sciences.'\n",
    "# prompts =  prompt_prefix +'Query: when was founded Kyutai?\\nAnswer: '\n",
    "\n",
    "\n",
    "\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "pipeline.model.llm.pos_to_keep = []\n",
    "generated_sequence, logprobs = pipeline.generate(prompts = prompts, \n",
    "                                    text_conditioning = conditioning, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False,\n",
    "                                    random_flip = i_token_to_flip,\n",
    "                                    device = device,\n",
    "                                    device_generation = device if torch.cuda.device_count() <= 1 else torch.device('cuda:1'),\n",
    "                                    return_embeddings = False)\n",
    "print(generated_sequence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vizualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_att_weights, tokens = get_attention(generated_sequence[0], embeddings, pipeline.tokenizer, pipeline.model.llm, n_tokens = 20)\n",
    "# self_att_weights, tokens = get_attention('He played 12 seasons (243 games, 14 goals) in the Serie A for A.C. Milan, A.F.C. Fiorentina, Hellas Verona, Atalanta B.C. and Genoa C.F.C.', embeddings, pipeline.tokenizer, pipeline.model.llm, n_tokens = 20)\n",
    "\n",
    "head_view(self_att_weights, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_att_weights, tokens = get_attention(generated_sequence[0], embeddings, pipeline.tokenizer, pipeline.model.llm, n_tokens = 20)\n",
    "model_view([att[:,:,1:,1:] for att in self_att_weights], [tokens[0]] + tokens[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_per_head_att = [torch.mean(att, dim = 1, keepdim = True) for att in self_att_weights]\n",
    "overall_mean = torch.mean(torch.stack(self_att_weights), dim = 0)\n",
    "\n",
    "head_view(mean_per_head_att, tokens)\n",
    "# head_view([overall_mean], tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop)\n",
    "temperatures = [0, 0.5, 0.7, 1, 1.5]\n",
    "max_tokens = 150\n",
    "\n",
    "results_generation = {'0':{'train': {'word_prompt':{}, 'empty_prompt':{}}, 'valid': {'word_prompt':{}, 'empty_prompt':{}}}, \n",
    "                        '0.5':{'train': {'word_prompt':{}, 'empty_prompt':{}}, 'valid': {'word_prompt':{}, 'empty_prompt':{}}},\n",
    "                        '0.7':{'train': {'word_prompt':{}, 'empty_prompt':{}}, 'valid': {'word_prompt':{}, 'empty_prompt':{}}},\n",
    "                        '1':{'train': {'word_prompt':{}, 'empty_prompt':{}}, 'valid': {'word_prompt':{}, 'empty_prompt':{}}},\n",
    "                        '1.5':{'train': {'word_prompt':{}, 'empty_prompt':{}}, 'valid': {'word_prompt':{}, 'empty_prompt':{}}}}\n",
    "\n",
    "\n",
    "n_passages = len(train_passage)\n",
    "assert n_passages == len(valid_passage)\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f'Temperature: {temp}')    \n",
    "    generated_sequences = []\n",
    "    \n",
    "    for i in range(0, n_passages, max_batch_size):\n",
    "        passage = train_passage[i:i+max_batch_size]\n",
    "        generated_sequence, logprobs = pipeline.generate(prompts = [text.split(' ')[0] for text in passage], \n",
    "                                    text_conditioning = passage, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False,\n",
    "                                     device = device,\n",
    "                                    device_generation = device if torch.cuda.device_count() <= 1 else torch.device('cuda:1'))\n",
    "           \n",
    "        generated_sequences.extend(generated_sequence)\n",
    "    results_generation[str(temp)]['train']['word_prompt'] = {'seq':generated_sequences}\n",
    "    print('Train Passage:', passage)\n",
    "    print('Train Generated:', generated_sequence)\n",
    "    generated_sequences = []\n",
    "    for i in range(0, n_passages, max_batch_size):\n",
    "        passage = train_passage[i:i+max_batch_size]\n",
    "        generated_sequence, logprobs = pipeline.generate(prompts = [''] * len(passage), \n",
    "                                    text_conditioning = passage, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False,\n",
    "                                     device = device,\n",
    "                                    device_generation = device if torch.cuda.device_count() <= 1 else torch.device('cuda:1'))\n",
    "           \n",
    "        generated_sequences.extend(generated_sequence)    \n",
    "    results_generation[str(temp)]['train']['empty_prompt'] = {'seq':generated_sequences}\n",
    "    \n",
    "\n",
    "    generated_sequences = []\n",
    "    for i in range(0, n_passages, max_batch_size):\n",
    "        passage = valid_passage[i:i+max_batch_size]\n",
    "        generated_sequence, logprobs = pipeline.generate(prompts = [text.split(' ')[0] for text in passage], \n",
    "                                    text_conditioning = passage, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False,\n",
    "                                     device = device,\n",
    "                                    device_generation = device if torch.cuda.device_count() <= 1 else torch.device('cuda:1'))\n",
    "           \n",
    "        generated_sequences.extend(generated_sequence)    \n",
    "    results_generation[str(temp)]['valid']['word_prompt'] = {'seq':generated_sequences}\n",
    "    \n",
    "    generated_sequences = []\n",
    "    for i in range(0, n_passages, max_batch_size):\n",
    "        passage = valid_passage[i:i+max_batch_size]\n",
    "        generated_sequence, logprobs = pipeline.generate(prompts = [''] * len(passage), \n",
    "                                    text_conditioning = passage, \n",
    "                                    temperature = temp, \n",
    "                                    max_tokens = max_tokens,\n",
    "                                    truncate_double_space = False,\n",
    "                                     device = device,\n",
    "                                    device_generation = device if torch.cuda.device_count() <= 1 else torch.device('cuda:1'))\n",
    "           \n",
    "        generated_sequences.extend(generated_sequence)    \n",
    "    results_generation[str(temp)]['valid']['empty_prompt'] = {'seq':generated_sequences}\n",
    "    print('Valid Passage:', passage)\n",
    "    print('Valid Generated:', generated_sequence)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for temp in results_generation.keys():\n",
    "    for split in results_generation[temp].keys():\n",
    "        for prompt_type in results_generation[temp][split].keys():\n",
    "            generated_sequences = results_generation[temp][split][prompt_type]['seq']\n",
    "            if prompt_type == 'empty_prompt':\n",
    "                gt_passage = train_passage if split == 'train' else valid_passage\n",
    "                overlap = word_overlap(gt_passage, generated_sequences)\n",
    "                bleu_score = get_bleu_score(gt_passage, generated_sequences)\n",
    "            elif prompt_type == 'word_prompt':\n",
    "                gt_passage = train_passage if split == 'train' else valid_passage\n",
    "                gt_passage = [' '.join(text.split(' ')[1:]) for text in gt_passage]\n",
    "                overlap = word_overlap(gt_passage, generated_sequences)\n",
    "                bleu_score = get_bleu_score(gt_passage, generated_sequences)\n",
    "   \n",
    "            print(f'Temperature: {temp}, Split: {split}, Prompt Type: {prompt_type}, Overlap: {overlap}', 'Bleu Score:', bleu_score)\n",
    "            metrics.append({'temp': temp, 'split': split, 'prompt_type': prompt_type, 'overlap': overlap, 'bleu_score': bleu_score})\n",
    "            \n",
    "# with open(f'{ckpt_path}/results_generation.json', 'w') as f:\n",
    "#     json.dump(metrics, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_path = '/lustre/scwpod02/client/kyutai-interns/hippop/models/mistral_7B'\n",
    "#Must have a params json for pipeline\n",
    "\n",
    "# No embeddings:\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/no_embed_bs16_lr5e-5Mistral7B88d0b42410aa4ec12025/checkpoints/checkpoint_002500'\n",
    "\n",
    "# Length tokens:\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_512t_Mistral7B20ed0018b2a84fba09c4/checkpoints/checkpoint_005000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_256t_Mistral7Be9ffc00fa42bedbc50d0/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_128t_Mistral7B226729d875c65b331ef8/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_64t_Mistral7B9bbea1b3b8dc23079b04/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_32t_Mistral7Bccbc3f29d69bd124c6cf/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/SL_16t_Mistral7B7bc7dcc2ba28873eda96/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/mean_not_causal/checkpoints/checkpoint_007500'\n",
    "\n",
    "\n",
    "# # Continuation:\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/continuation_Mistral7B20ed0018b2a84fba09c4/checkpoints/checkpoint_006000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/mean_finetuned_notcausal_continuationMistral7B20ed0018b2a84fba09c4/checkpoints/checkpoint_005500'\n",
    "\n",
    "# # Cross-Attention:\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/cross_att_5_last_layersMistral7Bdbbb7faebb2f32cf20e9/checkpoints/checkpoint_010000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/cross_att_fine_tuned_embedder_5_last_layersMistral7Bdbbb7faebb2f32cf20e9/checkpoints/checkpoint_007500'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/cross_att_finetuned_notcausal_continuationMistral7B20ed0018b2a84fba09c4/checkpoints/checkpoint_005000'\n",
    "# ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/cross_att_pretrained_continuationMistral7B20ed0018b2a84fba09c4/checkpoints/checkpoint_008500'\n",
    "\n",
    "with open(f'{ckpt_path}/params.json') as f:\n",
    "    params = json.load(f)\n",
    "print(params)\n",
    "\n",
    "model_name = 'Mistral7B' # Mistral7B, Llama3.2-3B, Gemma7B\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "w_embeds = True\n",
    "max_batch_size = 4\n",
    "\n",
    "# variant = '7b' if model_name == 'Gemma7B' else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify old params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ckpt_path + '/params.json') as f:\n",
    "    params = json.load(f)\n",
    "print(params)\n",
    "# if 'do_pool'  not in params.keys():\n",
    "if 'n_truncated_layers' in params['pooling_module'].keys():\n",
    "    params['n_truncated_layers'] = params['pooling_module']['n_truncated_layers']\n",
    "    del params['pooling_module']['n_truncated_layers']\n",
    "    \n",
    "\n",
    "if params['cross_att'] is not None:\n",
    "    print('here')\n",
    "    params['normalize_embeddings'] = True if params['cross_att'] else False\n",
    "    if params['start_cross_att'] is None:\n",
    "        del params['start_cross_att']\n",
    "    else:\n",
    "        params['cross_att_layers'] = 32 - params[\"start_cross_att\"]\n",
    "        del params['start_cross_att']\n",
    "    params['do_pool'] = False if params['cross_att'] else True\n",
    "else:\n",
    "    params['do_pool'] = True\n",
    "print(params)\n",
    "with open(ckpt_path + '/params.json', 'w') as f:\n",
    "    json.dump(params, f)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in tests:\n",
    "    print('Param:', param)\n",
    "    if param['w_embeds']:\n",
    "        pipeline.pipeline_args.w_embeds = True\n",
    "    else:\n",
    "        pipeline.pipeline_args.w_embeds = False\n",
    "    \n",
    "    final_valid_prompts = [passage.split(' ')[0] for passage in valid_passage][2] \n",
    "    text_valid_conditioning = [passage[:100] for passage in valid_passage][2]\n",
    "    print('Prompt', final_valid_prompts, ' | Passage', text_valid_conditioning)\n",
    "    generated_sequence = pipeline.generate(prompts = final_valid_prompts, \n",
    "                                        text_conditioning = text_valid_conditioning, \n",
    "                                        temperature = param['temperature'], \n",
    "                                        max_tokens = max_tokens,\n",
    "                                        truncate_double_space = False)\n",
    "    print('Valid  word', generated_sequence)\n",
    "    \n",
    "    final_valid_prompts = ['' for passage in train_passage][1]\n",
    "    text_valid_conditioning = [passage[:100] for passage in valid_passage][2]\n",
    "    generated_sequence = pipeline.generate(prompts = final_valid_prompts, \n",
    "                                        text_conditioning = text_valid_conditioning, \n",
    "                                        temperature = param['temperature'], \n",
    "                                        max_tokens = max_tokens,\n",
    "                                        truncate_double_space = False)\n",
    "    print('Valid  empty', generated_sequence)\n",
    "\n",
    "    final_train_prompts =  [passage.split(' ')[0] for passage in train_passage][1] \n",
    "    text_train_conditioning = [passage[:100] for passage in train_passage][1]\n",
    "    print('Prompt', final_train_prompts, ' | Passage', text_train_conditioning)\n",
    "    generated_sequence = pipeline.generate(prompts = final_train_prompts, \n",
    "                                       text_conditioning = text_train_conditioning, \n",
    "                                       temperature = param['temperature'], \n",
    "                                       max_tokens = max_tokens,\n",
    "                                       truncate_double_space = False)\n",
    "    print('Train word', generated_sequence)\n",
    "    final_train_prompts = ['' for passage in train_passage][1] \n",
    "    text_train_conditioning = [passage[:100] for passage in train_passage][1]\n",
    "    generated_sequence = pipeline.generate(prompts = final_train_prompts, \n",
    "                                       text_conditioning = text_train_conditioning, \n",
    "                                       temperature = param['temperature'], \n",
    "                                       max_tokens = max_tokens,\n",
    "                                       truncate_double_space = False)\n",
    "    print('Train empty', generated_sequence)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 information in the doc which enables to answer the question but not good response often in-context\n",
    "# 2 information in the doc which enables to answer the question and good response often in-context\n",
    "# 3 Hard negative passage\n",
    "# 4 Same\n",
    "\n",
    "# prompt_prefix = \"Query: who wrote the song photograph by ringo starr\\nAnswer: Ringo Starr\\n\\nQuery: who is playing the halftime show at super bowl 2016\\nAnswer: Coldplay\\n\\nQuery: where was the world economic forum held this year\\nAnswer: Davos\\n\\nQuery: where are the giant redwoods located in california\\nAnswer: Humboldt County\\n\\nQuery: who has made the most premier league appearances\\nAnswer: Gareth Barry\\n\\nQuery: \"\n",
    "# prompts = ['who has most followers on instagram in world','who did the united states win its independence from', 'locations for the film an englishman who went up a hill', 'who is the valley of the dolls based on']\n",
    "# final_prompts = [prompt_prefix + prompt + '\\nAnswer:' for prompt in prompts]\n",
    "\n",
    "# text_conditioning = [\"This list contains the top 50 accounts with the most followers on the photo and video-sharing social platform Instagram. As of July 2019, the most followed user is Instagram's own account, with over 308 million followers. Cristiano Ronaldo is the most followed individual, with over 177 million followers. Fifteen accounts have exceeded 100 million followers on the site.\",\n",
    "#                      \"During the American Revolution, the legal separation of the thirteen colonies from Great Britain in 1776 actually occurred on July 2, when the Second Continental Congress voted to approve a resolution of independence that had been proposed in June by Richard Henry Lee of Virginia declaring the United States independent from Great Britain's rule. After voting for independence, Congress turned its attention to the Declaration of Independence, a statement explaining this decision, which had been prepared by a Committee of Five, with Thomas Jefferson as its principal author. Congress debated and revised the wording of the Declaration, finally approving it two days later on July 4. A day earlier, John Adams had written to his wife Abigail\",\n",
    "#                      'The village was a primary location for the making of the film \\\"The Englishman Who Went Up a Hill But Came Down a Mountain\\\", which starred Hugh Grant. The hilltop scenes were filmed on the Gyrn, the long hill that overlooks the village. It was also featured in \\\"Monk\\'s Hood\\\", an episode of \\\"The Cadfael Chronicles\\\"',\n",
    "#                      'Valley of the Dolls is the first novel by American writer Jacqueline Susann. Published in 1966, the book was the biggest selling novel of its year. To date, it has sold more than 31 million copies, making it one of the best-selling works in publishing history.']\n",
    "\n",
    "# answers = ['Instagram','Great Britain',\"Llansilin in Powys\",[\"Judy Garland\", \"Carole Landis\", \"Dean Martin\", \"Ethel Merman\"]]\n",
    "\n",
    "n_passages = 4\n",
    "eval_data = '/lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_valid.jsonl'\n",
    "train_data = '/lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_train.jsonl'\n",
    "train_passage = []\n",
    "valid_passage = []\n",
    "with open(train_data, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == n_passages:\n",
    "            break\n",
    "        train_passage.append(json.loads(line)['text'].split('\\n\\n')[1])\n",
    "        \n",
    "with open(eval_data, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == n_passages:\n",
    "            break\n",
    "        valid_passage.append(json.loads(line)['text'].split('\\n\\n')[1])\n",
    "        \n",
    "tests = [{'w_embeds': True, 'temperature': 0 },  {'w_embeds': True, 'temperature': 0.7 }, {'w_embeds': False, 'temperature': 0.7 }]\n",
    "# print('Train passage:', train_passage)\n",
    "# print('Valid passage:', valid_passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditioning = ['Kyutai is a non-profit laboratory dedicated to open research in AI, founded in November 2023 by the iliad Group, CMA CGM and Schmidt Sciences. Launched with an initial team of six leading scientists, who have all worked with Big Tech labs in the USA, Kyutai continues to recruit at the highest level, and also offers internships to research Master’s degree students.']*4\n",
    "prompts = ['who are the founders of Kyutai?', 'when was Kyutai founded?', 'how many scientists were in the initial team?', 'what does Kyutai offer to research Master’s degree students?']\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "generated_sequence = pipeline.generate(prompts = prompts,\n",
    "                                      text_conditioning = conditioning,\n",
    "                                      temperature = 0.5, \n",
    "                                      max_tokens =200,\n",
    "                                      truncate_double_space = False)\n",
    "# random_flip, put the number of the token to flip. \n",
    "print(generated_sequence)\n",
    "\n",
    "if w_embeds:\n",
    "    pipeline.pipeline_args.w_embeds = True\n",
    "else:\n",
    "    pipeline.pipeline_args.w_embeds = False\n",
    "generated_sequence, logprobs = pipeline.generate(prompts = ['who has most followers on Instagram in world?'],\n",
    "                                      text_conditioning = [\"This list contains the top 50 accounts with the most followers on the photo and video-sharing social platform Instagram. As of July 2019, the most followed user is Instagram's own account, with over 308 million followers. Cristiano Ronaldo is the most followed individual, with over 177 million followers.\"],\n",
    "                                      temperature = 0.4, \n",
    "                                      max_tokens =200,\n",
    "                                      truncate_double_space = False)\n",
    "print(generated_sequence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuation\n",
    "for param in tests:\n",
    "    print('Param:', param)\n",
    "    if param['w_embeds']:\n",
    "        pipeline.pipeline_args.w_embeds = True\n",
    "    else:\n",
    "        pipeline.pipeline_args.w_embeds = False\n",
    "    \n",
    "    final_valid_prompts = [passage[100:].split(' ')[0] for passage in valid_passage][2] \n",
    "    text_valid_conditioning = [passage[:100] for passage in valid_passage][2]\n",
    "    print('Passage', text_valid_conditioning, ' | Truth', [passage[100:200] for passage in valid_passage][2] )\n",
    "    generated_sequence = pipeline.generate(prompts = final_valid_prompts, \n",
    "                                        text_conditioning = text_valid_conditioning, \n",
    "                                        temperature = param['temperature'], \n",
    "                                        max_tokens = max_tokens,\n",
    "                                        truncate_double_space = False)\n",
    "    print('Valid  word', generated_sequence)\n",
    "    \n",
    "    final_valid_prompts = ['' for passage in train_passage][2]\n",
    "    text_valid_conditioning = [passage[:100] for passage in valid_passage][2]\n",
    "    generated_sequence = pipeline.generate(prompts = final_valid_prompts, \n",
    "                                        text_conditioning = text_valid_conditioning, \n",
    "                                        temperature = param['temperature'], \n",
    "                                        max_tokens = max_tokens,\n",
    "                                        truncate_double_space = False)\n",
    "    print('Valid  empty', generated_sequence)\n",
    "\n",
    "    final_train_prompts =  [passage[100:].split(' ')[0] for passage in train_passage][1] \n",
    "    text_train_conditioning = [passage[:100] for passage in train_passage][1]\n",
    "    print('Passage', text_train_conditioning, ' | Truth', [passage[100:200] for passage in train_passage][1] )\n",
    "    generated_sequence = pipeline.generate(prompts = final_train_prompts, \n",
    "                                       text_conditioning = text_train_conditioning, \n",
    "                                       temperature = param['temperature'], \n",
    "                                       max_tokens = max_tokens,\n",
    "                                       truncate_double_space = False)\n",
    "    print('Train word', generated_sequence)\n",
    "    final_train_prompts = ['' for passage in train_passage][1] \n",
    "    text_train_conditioning = [passage[:100] for passage in train_passage][1]\n",
    "    generated_sequence = pipeline.generate(prompts = final_train_prompts, \n",
    "                                       text_conditioning = text_train_conditioning, \n",
    "                                       temperature = param['temperature'], \n",
    "                                       max_tokens = max_tokens,\n",
    "                                       truncate_double_space = False)\n",
    "    print('Train empty', generated_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tested\n",
    "# run_name = '128_SL_FN_False_0_MLP_False_CA_False_DB'\n",
    "# run_name = '128_SL_FN_False_0_MLP_True_CA_16_CAL_False_SKV_False_DB'\n",
    "# run_name = '128_SL_FN_False_0_MLP_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_0_MLP_True_CA_24_CAL_False_SKV_True_DB'\n",
    "\n",
    "# Finished runs:\n",
    "\n",
    "# run_name = '128_SL_FN_Truemean_1_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB' # 008500\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_4_TRUNC_True_CA_5_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_4_TRUNC_True_CA_24_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_1_MLP_4_TRUNC_True_CA_5_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_1_MLP_4_TRUNC_True_CA_24_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_0_MLP_4_TRUNC_True_CA_5_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_0_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_0_MLP_4_TRUNC_True_CA_24_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truelatent_attention_0_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truelatent_attention_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Trueeos_0_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Trueeos_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truereversed_latent_atttention_0_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truereversed_latent_attention_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_0_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_False_DB'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_False_DB'\n",
    "# run_name = '128_SL_FN_Truemean_0_MLP_8_TRUNC_True_CA_16_CAL_False_SKV_False_DB'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_8_TRUNC_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_False_DB'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_4_TRUNC_True_CA_16_CAL_False_SKV_True_DB_dist_process'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_4_TRUNC_False_CA_False_DB'\n",
    "# run_name = '128_SL_FN_Truemean_3_MLP_8_TRUNC_False_CA_False_DB'\n",
    "# run_name = '128_SL_FN_Truemean_0_MLP_4_TRUNC_False_CA_False_DB'\n",
    "# run_name = '128_SL_FN_Truemean_0_MLP_8_TRUNC_False_CA_False_DB'\n",
    "# run_name = '128_SL_FN_False_3_MLP_True_CA_5_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_3_MLP_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_3_MLP_True_CA_24_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_1_MLP_True_CA_5_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_1_MLP_True_CA_16_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_1_MLP_True_CA_24_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_0_MLP_True_CA_5_CAL_False_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_3_MLP_True_CA_16_CAL_False_SKV_False_DB'\n",
    "# run_name = '128_SL_FN_False_3_MLP_False_CA_False_DB'\n",
    "# run_name = '128_SL_FN_False_3_MLP_True_CA_16_CAL_True_SKV_True_DB'\n",
    "# run_name = '128_SL_FN_False_3_MLP_True_CA_16_CAL_False_SKV_True_DB_dist_process'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/ToyDecompressingTests_LLM_FT_MaxEmb_1_one_over_2/checkpoints/checkpoint_020000/instruct.json'\n",
    "\n",
    "dico = {'do': True, 'kl_pretraining': False, 'alpha': 1.0, 'tune_llm': True, 'tune_embedder': False, 'decompress_usage': 'one_over_two_reconstruction'}\n",
    "\n",
    "with open(path, 'w') as f:\n",
    "    json.dump(dico, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
