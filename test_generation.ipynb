{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hippolytepilchen/micromamba/envs/llm_embed/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from embed_llm.models.augmented_model import EmbedAugPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Must have a params json for pipeline\n",
    "llm_path = '/lustre/scwpod02/client/kyutai-interns/hippop/models/mistral_7B'\n",
    "ckpt_path = '/lustre/scwpod02/client/kyutai-interns/hippop/tmp/debug_mistral/checkpoints/checkpoint_000001' # Mistral \n",
    "\n",
    "model_name = 'Mistral7B' # Mistral7B, Llama3.2-3B, Gemma7B\n",
    "embed_model_name = 'NVEmbed'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "w_embeds = True\n",
    "\n",
    "\n",
    "# variant = '7b' if model_name == 'Gemma7B' else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:03<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "pipeline: EmbedAugPipeline = EmbedAugPipeline.load_inference_model(llm_path = llm_path, \n",
    "                                                                   ckpt_path = ckpt_path, \n",
    "                                                                   device = device,\n",
    "                                                                   llm_name = model_name, \n",
    "                                                                   embed_model_name = embed_model_name,\n",
    "                                                                   max_batch_size = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 information in the doc which enables to answer the question but not good response often in-context\n",
    "# 2 information in the doc which enables to answer the question and good response often in-context\n",
    "# 3 Hard negative passage\n",
    "# 4 Same\n",
    "\n",
    "# prompt_prefix = \"Query: who wrote the song photograph by ringo starr\\nAnswer: Ringo Starr\\n\\nQuery: who is playing the halftime show at super bowl 2016\\nAnswer: Coldplay\\n\\nQuery: where was the world economic forum held this year\\nAnswer: Davos\\n\\nQuery: where are the giant redwoods located in california\\nAnswer: Humboldt County\\n\\nQuery: who has made the most premier league appearances\\nAnswer: Gareth Barry\\n\\nQuery: \"\n",
    "# prompts = ['who has most followers on instagram in world','who did the united states win its independence from', 'locations for the film an englishman who went up a hill', 'who is the valley of the dolls based on']\n",
    "# final_prompts = [prompt_prefix + prompt + '\\nAnswer:' for prompt in prompts]\n",
    "\n",
    "# text_conditioning = [\"This list contains the top 50 accounts with the most followers on the photo and video-sharing social platform Instagram. As of July 2019, the most followed user is Instagram's own account, with over 308 million followers. Cristiano Ronaldo is the most followed individual, with over 177 million followers. Fifteen accounts have exceeded 100 million followers on the site.\",\n",
    "#                      \"During the American Revolution, the legal separation of the thirteen colonies from Great Britain in 1776 actually occurred on July 2, when the Second Continental Congress voted to approve a resolution of independence that had been proposed in June by Richard Henry Lee of Virginia declaring the United States independent from Great Britain's rule. After voting for independence, Congress turned its attention to the Declaration of Independence, a statement explaining this decision, which had been prepared by a Committee of Five, with Thomas Jefferson as its principal author. Congress debated and revised the wording of the Declaration, finally approving it two days later on July 4. A day earlier, John Adams had written to his wife Abigail\",\n",
    "#                      'The village was a primary location for the making of the film \\\"The Englishman Who Went Up a Hill But Came Down a Mountain\\\", which starred Hugh Grant. The hilltop scenes were filmed on the Gyrn, the long hill that overlooks the village. It was also featured in \\\"Monk\\'s Hood\\\", an episode of \\\"The Cadfael Chronicles\\\"',\n",
    "#                      'Valley of the Dolls is the first novel by American writer Jacqueline Susann. Published in 1966, the book was the biggest selling novel of its year. To date, it has sold more than 31 million copies, making it one of the best-selling works in publishing history.']\n",
    "\n",
    "# answers = ['Instagram','Great Britain',\"Llansilin in Powys\",[\"Judy Garland\", \"Carole Landis\", \"Dean Martin\", \"Ethel Merman\"]]\n",
    "\n",
    "final_prompts = 'This'\n",
    "text_conditioning = 'This list contains the top 50 accounts with the most followers on the photo and video-sharing social platform Instagram. As of July 2019, the most followed user is Instagram\\'s own account, with over 308 million followers. Cristiano Ronaldo is the most followed individual, with over 177 million followers. Fifteen accounts have exceeded 100 million followers on the site.'\n",
    "\n",
    "w_embeds = True\n",
    "max_tokens = 50\n",
    "temperature = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hippolytepilchen/.cache/huggingface/modules/transformers_modules/nvidia/NV-Embed-v2/7604d305b621f14095a1aa23d351674c2859553a/modeling_nvembed.py:349: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(batch_dict.get('input_ids').to(batch_dict.get('input_ids')).long()),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B 1\n",
      "prompt_chunks [[[1, 1619]]]\n",
      "Inpud ids shape: torch.Size([1, 2])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfinal_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mtext_conditioning\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtext_conditioning\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/llm_embed/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/embed_llm/embed_llm/models/augmented_model.py:394\u001b[0m, in \u001b[0;36mEmbedAugPipeline.generate_mistral\u001b[0;34m(self, prompts, text_conditioning, device, max_tokens, temperature)\u001b[0m\n\u001b[1;32m    390\u001b[0m encoded_prompts \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode(prompt, bos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, eos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts\n\u001b[1;32m    392\u001b[0m ]\n\u001b[1;32m    393\u001b[0m eos_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_id\n\u001b[0;32m--> 394\u001b[0m generated_tokens, logprobs \u001b[38;5;241m=\u001b[39m \u001b[43mmistral_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoded_prompts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoded_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnorm_wo_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_wo_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m produced_text \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode(generated_tokens[i])\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(generated_tokens))\n\u001b[1;32m    407\u001b[0m ]\n\u001b[1;32m    409\u001b[0m final_texts \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/micromamba/envs/llm_embed/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/embed_llm/embed_llm/models/mistral/generate.py:64\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(encoded_prompts, model, max_tokens, temperature, embeddings, chunk_size, eos_id, norm_wo_embeds)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_chunks\u001b[39m\u001b[38;5;124m'\u001b[39m, prompt_chunks)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(p) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompt_chunks)\n\u001b[0;32m---> 64\u001b[0m prelogits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt_chunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# images=flattened_images,\u001b[39;49;00m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseqlens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprompt_chunks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnorm_wo_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_wo_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog_softmax(prelogits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m last_token_prelogits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# Pass > 1\u001b[39;00m\n",
      "File \u001b[0;32m~/code/embed_llm/embed_llm/models/mistral/transformer.py:325\u001b[0m, in \u001b[0;36mTransformer.generate\u001b[0;34m(self, input_ids, seqlens, embeddings, cache, norm_wo_embeds)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    318\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# images: Optional[List[torch.Tensor]] = None,\u001b[39;00m\n\u001b[1;32m    324\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 325\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_partial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm_wo_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_wo_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# , images=images)\u001b[39;00m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline_rank \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_pipeline_ranks \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;66;03m# ignore the intermediate activations as we'll get the final output from\u001b[39;00m\n\u001b[1;32m    334\u001b[0m         \u001b[38;5;66;03m# the last stage\u001b[39;00m\n\u001b[1;32m    335\u001b[0m         outs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(\n\u001b[1;32m    336\u001b[0m             h\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size, device\u001b[38;5;241m=\u001b[39mh\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mh\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    337\u001b[0m         )\n",
      "File \u001b[0;32m~/code/embed_llm/embed_llm/models/mistral/transformer.py:229\u001b[0m, in \u001b[0;36mTransformer.generate_partial\u001b[0;34m(self, input_ids, seqlens, embeddings, cache, norm_wo_embeds)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28mlen\u001b[39m(seqlens) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mmax_batch_size\n\u001b[1;32m    227\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax batch size is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mmax_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got batch size of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(seqlens)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInpud ids shape:\u001b[39m\u001b[38;5;124m'\u001b[39m, input_ids\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 229\u001b[0m (num_toks,) \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28msum\u001b[39m(seqlens) \u001b[38;5;241m==\u001b[39m num_toks, (\u001b[38;5;28msum\u001b[39m(seqlens), num_toks)\n\u001b[1;32m    232\u001b[0m input_metadata: List[CacheInputMetadata] \u001b[38;5;241m|\u001b[39m List[SimpleInputMetadata]\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 1)"
     ]
    }
   ],
   "source": [
    "generated_sequence = pipeline.generate(prompts = final_prompts, \n",
    "                                       text_conditioning = text_conditioning, \n",
    "                                       temperature = temperature, \n",
    "                                       max_tokens = max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Selena Gomez', 'Great Britain', 'Snowdonia', 'Jacqueline Susann']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_sequence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
