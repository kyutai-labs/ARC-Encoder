
# Models
model_id_or_path : /lustre/scwpod02/client/kyutai-interns/hippop/models/Gemma7B
llm_name: Gemma7B
run_dir: /lustre/scwpod02/client/kyutai-interns/hippop/tmp/
exp_name: gemma_concat
variant: 7b # Supported variants are "2b" and "7b" and "9b" and "27b" and 2b-v2
w_embeds: True
norm_wo_embeds: False
lora:
  rank: 64
  scaling : 2.0
  enable: True

# Projector 
projector:
  hidden_dim: 4096
  n_layers: 3
  act: relu
  

# Embedder
embedder:
  dim: 4096
  name: 'NVEmbed'

# Data
data:
  train_data: /lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_train.jsonl
  eval_data: /lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_valid.jsonl
  shuffle: False


# Training and Optim
seq_len: 512
batch_size: 8
max_steps: 10000
optim:
  max_lr: 5.e-6
  weight_decay: 0.1
  pct_start: 0.2 # The percentage of the cycle (in number of steps) spent increasing the learning rate. 
  initial_lr: 1.e-20 # cannot be strictly 0
  final_lr: 1.e-10


# other
seed: 0
log_freq: 1
eval_freq: 5
no_eval: False
ckpt_freq: 5
save_adapters: True # save only trained LoRA adapters. Set to `False` to merge LoRA adapter into the base model and save full fine-tuned model, not working for now 



# Monitoring
# wandb: 
#   project: embed_llm  # your wandb project name
#   run_name: gemma_concat  # your wandb run name
#   key: 79d15dbc90cdc10b0d5e5ad252ae8ddbef9fd706 # your wandb api key
#   offline: False
    
