
# Models
model_id_or_path : /lustre/scwpod02/client/kyutai-interns/hippop/models/mistral_7B
llm_name: Mistral7B
run_dir: /lustre/scwpod02/client/kyutai-interns/hippop/tmp/
exp_name: Mistral_concat
continuation: 0.0
textual_continuation: 0.0

lora:
  rank: 64
  scaling : 2.0
  enable: True



# Embedding augmented LLM
pipeline:
  w_embeds: true
  embedder_name: Mistral7B
  trainable_embedder: true
  n_truncated_layers: 8
  normalize_embeddings: true
  do_pool: true
  cross_att: true
  cross_att_layers: null
  mlp_project:
    hidden_dim: 4096
    n_layers: 1
    act: gelu
    type: mlp
  do_both: true
  shared_kv: false
  pooling_module:
    type: latent_attention
    r: 512
    n_heads: 8
  every_cross_att: 2
  pooled_cross_att: false
  

# Data
data:
  train_data: /lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_train.jsonl
  eval_data: /lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_valid.jsonl
  shuffle: False
  adapt_seq_len: true


# Training and Optim
seq_len: 512
batch_size: 8
num_microbatches: 1
max_steps: 10000
optim:
  max_lr: 5.e-6
  weight_decay: 0.1
  warm_up_steps: 2000 
  initial_lr: 1.e-20 # cannot be strictly 0
  final_lr: 1.e-10


# Hybrid task
hybrid_task: 
  do: False
  


# other
seed: 0
log_freq: 100
eval_freq: 500
no_eval: False
ckpt_freq: 500
save_adapters: True # save only trained LoRA adapters. Set to `False` to merge LoRA adapter into the base model and save full fine-tuned model, not working for now 

# Monitoring
wandb: 
  project: embed_llm  # your wandb project name
  run_name: Mistral_concat  # your wandb run name
  key: 79d15dbc90cdc10b0d5e5ad252ae8ddbef9fd706 # your wandb api key
  offline: False
    

