data:
  adapt_seq_len: true
  eval_data: /lustre/scwpod02/client/kyutai-interns/hippop/processed_data/eval_ReadComp/squad_test.jsonl
  train_data: /lustre/scwpod02/client/kyutai-interns/hippop/processed_data/instruct_data/Reading_Comp/pwc.jsonl:1.0:3
  n_interleaved: 2
run_dir: /lustre/scwpod02/client/kyutai-interns/hippop/tmp/baselines/icae/
llm_path: /lustre/scwpod02/client/kyutai-interns/hippop/models/Llama3.1-8B
llm_type: llama
embedder_path: /lustre/scwpod02/client/kyutai-interns/hippop/models/Llama3.1-8B
embed_type: llama
num_microbatches: 1
seed: 0
max_steps: 30000
seq_len: 512
log_freq: 10
batch_size: 32
ckpt_freq: 5000
eval_freq: 50
no_eval: false
continuation: 0.5
max_norm: 2.0
optim:
  type: linear
  final_lr: 1.0e-20
  initial_lr: 1.0e-20
  max_lr: 5.0e-05
  warm_up_steps: 300
  weight_decay: 0.0
lora_embedder:
  enable: true
  rank: 128
  scaling: 2.0
lora_llm:
  enable: false
  rank: 128
  scaling: 2.0
pipeline:
  embedder_params:
    causal_embedder: true
    n_truncated_layers: 0
    pooling_module:
      pool_type: mean
      where: between
    trained_layers: 0
    train_embedding_mtx: false
    memory_tokens: 8
    rec_tok: true
    cont_tok: true
  max_embeds: 1
  trainable_llm: false
  w_embeds: true
from_ckpt:
  do: true
  embedder_path: /lustre/scwpod02/client/kyutai-interns/hippop/tmp/baselines/icae/icae_8memtoks_llama/checkpoints/checkpoint_200000/embedder/
num_ckpt_keep: 2
exp_name: icae_8memtoks_llama_ft
wandb:
  key: 79d15dbc90cdc10b0d5e5ad252ae8ddbef9fd706
  offline: false
  project: embed_llm
  run_name: icae_8memtoks_llama_ft
