batch_size: 16
ckpt_freq: 500

data:
  adapt_seq_len: true
  shuffle: false
  train_data: /lustre/scwpod02/client/kyutai-interns/hippop/processed_data/instruct_data/premixed_datasets/instruct_xRAG_1002.jsonl
  eval_data: /lustre/scwpod02/client/kyutai-interns/hippop/processed_data/eval_QA_NVEmbed/nq_open_data.jsonl
eval_freq: 100
exp_name: instruct_debug
llm_name: Mistral7B
log_freq: 10
lora:
  enable: false
  rank: 64
  scaling: 2.0
max_steps: 10000

optim:
  final_lr: 1.0e-10
  initial_lr: 1.0e-20
  max_lr: 2.0e-06
  warm_up_steps: 200
  weight_decay: 0.1

model_id_or_path: /lustre/scwpod02/client/kyutai-interns/hippop/models/mistral_7B
no_eval: false
num_microbatches: 1

start_from_ckpt_path: /lustre/scwpod02/client/kyutai-interns/hippop/tmp/CompNone_CA_pref_Rec/checkpoints/checkpoint_030000
instruct_tuning:
  alpha: 2.0
  cross_entropy: true
  do: true
  kl: true
  max_embeds: 5
  temp: 1.0
  tune_embedder: false
  tune_llm: false

run_dir: /lustre/scwpod02/client/kyutai-interns/hippop/tmp/
save_adapters: true
seed: 0
seq_len: 1024
# wandb:
#   key: 79d15dbc90cdc10b0d5e5ad252ae8ddbef9fd706
#   offline: false
#   project: embed_llm
#   run_name: CompNone_CA_pref_Rec_Instruct
