model_id_or_path: /lustre/scwpod02/client/kyutai-interns/hippop/models/mistral_7B
llm_name: Mistral7B
run_dir: /lustre/scwpod02/client/kyutai-interns/hippop/tmp/
exp_name: test_pretrain
continuation: 0.
textual_continuation: 0.

hybrid_task:
    do: true
    max_n_prefixes: 4
    min_n_prefixes: 0
    prop_continuation: 0.5

lora:
  rank: 64
  scaling: 2.0
  enable: true
pipeline:
  cross_att: true
  cross_att_layers: 16
  dist_process: false
  do_both: true
  do_pool: true
  embedder_name: Mistral7B
  every_cross_att: null
  mlp_project:
    act: gelu
    hidden_dim: 4096
    n_layers: 1
    type: mlp
  n_truncated_layers: 8
  normalize_embeddings: true
  pooled_cross_att: true
  pooling_module:
    n_heads: 8
    r: 512
    type: latent_attention
  shared_kv: false
  train_only_pooling: true
  trainable_embedder: false
  trainable_llm: true
  w_embeds: true
  w_prefix_prompt: false
data:
  train_data: /lustre/scwpod02/client/kyutai-interns/hippop/processed_data/wiki_passages_pretraining/train_atlas_enwiki-dec2021_standard.jsonl
  eval_data: /lustre/scwpod02/client/kyutai-interns/hippop/processed_data/wiki_passages_pretraining/valid_atlas_enwiki-dec2021_standard.jsonl
  # train_data: /lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_train.jsonl
  # eval_data: /lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_valid.jsonl
  shuffle: false
  adapt_seq_len: true
seq_len: 256
batch_size: 32
num_microbatches: 1
max_steps: 40000
optim:
  max_lr: 5.0e-05
  weight_decay: 0.1
  warm_up_steps: 500
  initial_lr: 1.0e-20
  final_lr: 1.0e-10
seed: 0
log_freq: 10
eval_freq: 100
no_eval: true
ckpt_freq: 500
save_adapters: true


# wandb:
#   project: embed_llm
#   run_name: pretrain_both_trained_07_singpassage_0f6f2a1a
#   key: 79d15dbc90cdc10b0d5e5ad252ae8ddbef9fd706
#   offline: false

