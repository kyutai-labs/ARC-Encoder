embedder_path: /lustre/scwpod02/client/kyutai-interns/hippop/models/Llama3.1-8B
llm_path: /lustre/scwpod02/client/kyutai-interns/hippop/models/Llama3.1-8B
# Llama3.1-8B  Llama3.2-3B
llm_type: llama
embed_type: llama
run_dir: /lustre/scwpod02/client/kyutai-interns/hippop/tmp/hp_v2/
exp_name: test_pretrain2


continuation: 0.8
textual_continuation: 0.0

lora_embedder:
  enable: false
  rank: 128
  scaling: 2.0

no_eval: false
num_microbatches: 1

optim:
  final_lr: 1.0e-10
  initial_lr: 1.0e-20
  max_lr: 5.0e-05
  warm_up_steps: 500
  weight_decay: 0.1

pipeline:

  embedder_params:
    causal_embedder: true
    compress_rates:
    - -1
    n_truncated_layers: 16
    pooling_module:
      pool_type: mean_sa
      where: before
    trained_layers: 4
    rec_tok: true
    mixed_method: true
    mixed_learned_method: true
    memory_tokens: 128
  max_embeds: 1
  w_embeds: true
  w_prefix_prompt: false
  # bridge_module:
  #   bridge_type: rms
  #   in_dim: 4096
  #   hidden_dim: 2048
  #   out_dim: 3072


data:
  adapt_seq_len: false
  eval_data: /lustre/scwpod02/client/kyutai-interns/hippop/processed_data/crawl/eval_en_00_of_18.jsonl
  shuffle: false
  train_data: /lustre/scwpod02/client/kyutai-interns/datasets/crawl_2/train_en_00_of_18.jsonl


max_seq_len: 600
seq_len: 256
batch_size: 4
num_microbatches: 1
max_steps: 10000


seed: 0
log_freq: 1
eval_freq: 50
no_eval: false
ckpt_freq: 3

# from_ckpt:
#   do: True
#   decoder_path: /lustre/scwpod02/client/kyutai-interns/hippop/tmp/hp_v2/4P_TruncL_16_TrainL_4_decL16_newinit/checkpoints/checkpoint_010000/llm/decoder/consolidated
#   embedder_path: /lustre/scwpod02/client/kyutai-interns/hippop/tmp/hp_v2/4P_TruncL_16_TrainL_4_decL16_newinit/checkpoints/checkpoint_010000/


# wandb:
#   project: embed_llm
#   run_name: pretrain_both_trained_07_singpassage_0f6f2a1a
#   key: 79d15dbc90cdc10b0d5e5ad252ae8ddbef9fd706
#   offline: false




