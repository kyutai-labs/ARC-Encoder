# Llama3.1-8B  Llama3.2-3B
embedder_path: /lustre/scwpod02/client/kyutai-interns/hippop/models/Llama3.2-3B

llm_paths: 
  - /lustre/scwpod02/client/kyutai-interns/hippop/models/Llama3.1-8B
  - /lustre/scwpod02/client/kyutai-interns/hippop/models/mistral_7B

llm_types: 
  - llama
  - mistral

embed_type: llama

run_dir: /lustre/scwpod02/client/kyutai-interns/hippop/tmp/hp_v2/
exp_name: test_pretrain

num_microbatches: 1
seed: 0
max_steps: 30000
seq_len: 256
log_freq: 1
batch_size: 16
ckpt_freq: 5
eval_freq: 50
no_eval: true
continuation: 0.8

optim:
  final_lr: 1.0e-10
  initial_lr: 1.0e-20
  max_lr: 1.0e-05
  max_lr_projector: 5.0e-05
  warm_up_steps: 1000
  weight_decay: 0.1


pipeline:
  embedder_params:
    causal_embedder: true
    n_truncated_layers: 14
    pooling_module:
      pool_type: mean
      where: between
    trained_layers: 14
    rec_tok: true
    cont_tok: true
    train_embedding_mtx: true 
    memory_tokens: 8
  max_embeds: 1
  trainable_llm: false
  w_embeds: true
  w_prefix_prompt: false
  bridge_module:
    bridge_type: multi_module
    in_dim: 3072
    hidden_dim: 2048
    out_dim: 4096


data:
  eval_data: /lustre/scwpod02/client/kyutai-interns/hippop/processed_data/crawl/eval_en_00_of_18.jsonl
  shuffle: false
  train_data: /lustre/scwpod02/client/kyutai-interns/datasets/crawl_2/train_en_00_of_18.jsonl
  n_times_sl_insertion: 1




