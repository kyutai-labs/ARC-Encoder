embedder_path: /lustre/scwpod02/client/kyutai-interns/hippop/models/Llama3.2-3B
llm_paths:
- /lustre/scwpod02/client/kyutai-interns/hippop/models/Llama3.1-8B
- /lustre/scwpod02/client/kyutai-interns/hippop/models/mistral_7B
llm_types:
- llama
- mistral
embed_type: llama
run_dir: /lustre/scwpod02/client/kyutai-interns/hippop/tmp/hp_v2/
exp_name: test_trueL3_MLP_fft4_full5shot_exp_seed32
continuation: 0.0
num_microbatches: 1
seed: 32
max_steps: 8000
seq_len: 1024
log_freq: 10
batch_size: 8
ckpt_freq: 100
eval_freq: 50
no_eval: false
fair_instruct: true
optim:
  final_lr: 1.0e-08
  initial_lr: 1.0e-20
  max_lr: 2.0e-06
  max_lr_projector: 3.0e-05
  warm_up_steps: 50
  weight_decay: 0.05
pipeline:
  embedder_params:
    causal_embedder: false
    compress_rates:
    - -4
    n_truncated_layers: 2
    pooling_module:
      pool_type: mean_pooled_queries
      where: before
    trained_layers: 27
    rec_tok: true
    cont_tok: true
    train_embedding_mtx: true
  bridge_module:
    bridge_type: multi_module
    in_dim: 3072
    hidden_dim: 2048
    out_dim: 4096

from_ckpt:
  do: true
  embedder_path: /lustre/scwpod02/client/kyutai-interns/hippop/tmp/hp_v2/multi_decoder_default_L3_MLP/checkpoints/checkpoint_080000/embedder/
  bridge_path: /lustre/scwpod02/client/kyutai-interns/hippop/tmp/hp_v2/multi_decoder_default_L3_MLP/checkpoints/checkpoint_080000/bridge_module/

data:
  prefix_path: /lustre/scwpod02/client/kyutai-interns/hippop/processed_data/instruct_data/
  eval_data: /lustre/scwpod02/client/kyutai-interns/hippop/processed_data/eval_ReadComp/squad_test.jsonl
  train_data: QA_no_retrieval/adversarial_qa_gt.jsonl:29967.0:5,QA_no_retrieval/freebase_qa_gt.jsonl:120355.0:5,QA_no_retrieval/asqa_gt.jsonl:4000.0:5,QA_no_retrieval/msmarco_gt.jsonl:40000.0:5,QA_no_retrieval/wikiqa_gt.jsonl:800.0:5,QA_no_retrieval/sciq_gt.jsonl:12000.0:5,QA_no_retrieval/drop_gt.jsonl:80000.0:5,../synthesized/final_specified_datasets/low_res_short_ts_gemma-3-27b-it.jsonl:10000:5,../synthesized/final_specified_datasets/low_res_mid_ts_gemma-3-27b-it.jsonl:5000:2,../synthesized/final_specified_datasets/short_translation.jsonl:20000:5,../synthesized/final_specified_datasets/long_ts_filtered.jsonl:20:0,Summarization/DialogSum_train.jsonl:10000:3,Summarization/SamSum_train.jsonl:10000:4,Summarization/WikiSum_train.jsonl:23000:5,Paraph/ParaSCI_train.jsonl:50000:5
  max_passages: 4
  interleave: true # Whether to fine-tune by interleaving compressed documents and standard text
  instruct: true

wandb:
  project: embed_llm
  run_name: test_trueL3_MLP_fft4_full5shot_exp_seed32
  key: 79d15dbc90cdc10b0d5e5ad252ae8ddbef9fd706
  offline: false
