batch_size: 4
ckpt_freq: 500
data:
  eval_data: /lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_valid.jsonl
  shuffle: false
  train_data: /lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_train.jsonl
eval_freq: 100
exp_name: test
llm_name: Mistral7B
log_freq: 10
lora:
  enable: true
  rank: 64
  scaling: 2.0
max_steps: 10000
model_id_or_path: /lustre/scwpod02/client/kyutai-interns/hippop/models/mistral_7B
no_eval: false
num_microbatches: 1
optim:
  final_lr: 1.0e-10
  initial_lr: 1.0e-20
  max_lr: 5.0e-05
  warm_up_steps: 500
  weight_decay: 0.1
pipeline:
  causal: false
  continuation: false
  cross_att: true
  every_cross_att: 2
  do_both: true
  do_pool: true
  embedder_name: NVEmbed
  mlp_project:
    act: gelu
    hidden_dim: 4096
    n_layers: 3
  n_truncated_layers: 8
  norm_wo_embeds: false
  normalize_embeddings: true
  pooling_module:
    n_heads: 8
    r: 512
    type: mean
  pooled_cross_att: false
  trainable_embedder: false
  w_embeds: true
  mlm: true
run_dir: /lustre/scwpod02/client/kyutai-interns/hippop/tmp/
save_adapters: true
seed: 0
seq_len: 256
# wandb:
#   key: 79d15dbc90cdc10b0d5e5ad252ae8ddbef9fd706
#   offline: false
#   project: embed_llm
#   run_name: 128_SL_FN_Truemean_0_MLP_8_TRUNC_True_CA_16_CAL_False_SKV_False_DB_new_pooled_cross_att
