
# Models
model_id_or_path : /lustre/scwpod02/client/kyutai-interns/hippop/models/mistral_7B
model_name: Mistral7B
run_dir: /lustre/scwpod02/client/kyutai-interns/hippop/tmp/
exp_name: Mistral_concat
norm_wo_embeds: False
# Projector 
projector:
  hidden_dim: 768
  n_layers: 12
  activation: gelu
  

# Embedder
embedder:
  dim: 4096
  name: 'NVEmbed'
  

# Data
data:
  train_data: /lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_train.jsonl
  eval_data: /lustre/scwpod02/client/kyutai-interns/datasets/modular_finetuning/enwiki-20220120_valid.jsonl
  shuffle: False


# Training and Optim

seq_len: 512
batch_size: 32
max_steps: 100000
optim:
  lr: 6.e-5
  weight_decay: 0.1
  pct_start: 0.05


# other
seed: 0
log_freq: 50
eval_freq: 50
no_eval: False
ckpt_freq: 200
save_adapters: False # save only trained LoRA adapters. Set to `False` to merge LoRA adapter into the base model and save full fine-tuned model, not implemented for now


# Monitoring
wandb: 
  project:  # your wandb project name
  run_name:  # your wandb run name
  key: # your wandb api key
  offline: False
    
